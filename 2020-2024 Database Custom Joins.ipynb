{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfa6e9d-16c2-44ad-8e40-d4836ac05b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instructions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ec68f3-30bc-4392-ba14-67985a6b2b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The main objective of this pipeline is to create a single analytical table where each row is a flight, enriched with:\n",
    "Core scheduling and routing information (carrier, origin/destination, distance, calendar fields).\n",
    "Binary delay labels and supporting delay metrics (`DEP_DEL15`, `ARR_DEL15`, `DEP_DELAY`, `ARR_DELAY`).\n",
    "“As-of” origin weather features sampled from NOAA hourly data before departure (temperature, visibility, wind, precipitation, etc.).\n",
    "Geographic helper fields (lat/lon for airports and stations, station–airport distances).\n",
    "The result is a model-ready dataset that can be used for classification (on-time vs delayed) regression on delay minutes, while carefully avoiding information leakage from post-departure variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48c4cc1-5cae-4a15-866b-49a71faeffa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature selection and leakage control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d0ca64-d445-41f9-9042-6feb6ef1fa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We define a clear feature-selection and leakage-control policy before any heavy joins are performed. First, it distinguishes outcomes from inputs: delay outcomes are treated strictly as labels or evaluation targets, while everything that could realistically be known before departure is treated as candidate predictors. On the input side, the dataset emphasizes variables that are fixed in advance or known operationally before takeoff: calendar information that captures seasonal and weekly patterns; planned schedule details such as planned departure and arrival times and planned flight duration; and structural flight characteristics like carrier identity, route, and distance. These are complemented with “as-of” origin weather features obtained from historical observations up to a fixed cutoff time before departure, ensuring that only past or present meteorological information is used. In contrast, any field that reflects what actually happened after the aircraft began its departure sequence—such as realized delays, taxi-out times, airborne times, or ex-post cause codes—is explicitly excluded from the modeling feature set to avoid data leakage. Those variables are retained only in separate diagnostic groups for analysis and model evaluation, not as predictors. This separation between pre-event and post-event information ensures that the final feature set mirrors the information a real-time system would have at prediction time, guarding against over-optimistic performance estimates and improving the robustness of the resulting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c999b3-44df-421a-a496-e8b986ebd048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Feature**           | **Type**           | **Description**                                         | **Notes / Use in Model**                     |\n",
    "| --------------------- | ------------------ | ------------------------------------------------------- | -------------------------------------------- |\n",
    "| `DEP_DEL15`           | Target (Binary)    | 1 if departure delay ≥15 minutes, else 0                | Main prediction target                       |\n",
    "| `DEP_DELAY`           | Numeric            | Departure delay in minutes (negative = early departure) | Used for validation and correlation          |\n",
    "| `CRS_DEP_HOUR`        | Numeric (0–23)     | Scheduled departure hour extracted from `CRS_DEP_TIME`  | Captures daily delay pattern                 |\n",
    "| `DAY_OF_WEEK`         | Categorical (1–7)  | Day of the week (1 = Monday, 7 = Sunday)                | Reflects weekday vs. weekend trends          |\n",
    "| `MONTH`               | Categorical (1–12) | Month of the year                                       | Captures seasonal variation                  |\n",
    "| `OP_UNIQUE_CARRIER`   | Categorical        | Airline carrier code (e.g., AA, DL, UA)                 | Delay rates vary by airline                  |\n",
    "| `ORIGIN`              | Categorical        | Origin airport IATA code                                | Captures airport-level delay patterns        |\n",
    "| `DEST`                | Categorical        | Destination airport IATA code                           | May add contextual variation                 |\n",
    "| `DISTANCE`            | Numeric            | Flight distance in miles                                | Longer flights less affected by short delays |\n",
    "| `TAXI_OUT`            | Numeric            | Taxi-out time in minutes                                | Indicator of airport congestion              |\n",
    "| `CRS_ELAPSED_TIME`    | Numeric            | Scheduled flight duration in minutes                    | Useful for normalization                     |\n",
    "| `HourlyPrecipitation` | Numeric            | Precipitation at departure station (inches/hour)        | Proxy for adverse weather                    |\n",
    "| `HourlyVisibility`    | Numeric            | Visibility at departure station (miles)                 | Lower values may increase delay risk         |\n",
    "| `HourlyWindSpeed`     | Numeric            | Wind speed at departure station (mph)                   | Captures storm or runway condition impact    |\n",
    "| `CANCELLED`           | Binary             | 1 if flight was canceled                                | May need to exclude or treat separately      |\n",
    "| `DIVERTED`            | Binary             | 1 if flight diverted to another airport                 | Usually excluded for modeling                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5e455f-c04e-49ea-81e8-54786be78b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Column                | Description                                                  | Source | Notes                                       |\n",
    "| :-------------------- | :----------------------------------------------------------- | :----- | :------------------------------------------ |\n",
    "| QUARTER               | Calendar quarter of the year (1–4)                           | Flight | some seasons might experience more delays than others: seasonality                      |\n",
    "| MONTH                 | Month of flight date (1–12)                                  | Flight | Use for monthly trends                      |\n",
    "| DAY_OF_MONTH          | Day of the month (1–31)                                      | Flight | Temporal feature                            |\n",
    "| DAY_OF_WEEK           | Day of the week (1=Mon, 7=Sun)                               | Flight | Delays vary by weekday                      |\n",
    "| FL_DATE               | Flight date                                                  | Flight | Combine with time fields for timestamp      |\n",
    "| OP_UNIQUE_CARRIER     | Unique airline carrier code (e.g., AA, DL)                   | Flight | Key categorical variable                    |\n",
    "| OP_CARRIER_AIRLINE_ID | Airline numeric ID from BTS                                  | Flight | Alternate carrier ID                        |\n",
    "| OP_CARRIER            | Carrier abbreviation                                         | Flight | Duplicate of OP_UNIQUE_CARRIER              |\n",
    "| TAIL_NUM              | Aircraft tail number                                         | Flight | Often missing or reused                     |\n",
    "| OP_CARRIER_FL_NUM     | Flight number                                                | Flight | Combine with carrier for unique flight ID   |\n",
    "| ORIGIN_AIRPORT_ID     | Unique numeric ID for origin airport                         | Flight | Key for joins                               |\n",
    "| ORIGIN_AIRPORT_SEQ_ID | Unique ID per airport sequence                               | Flight | Not needed for modeling                     |\n",
    "| ORIGIN_CITY_MARKET_ID | City market ID                                               | Flight | Identifies metro area                       |\n",
    "| ORIGIN                | Origin airport code (IATA)                                   | Flight | Major key feature                           |\n",
    "| ORIGIN_CITY_NAME      | Full city name of origin                                     | Flight | Redundant with ORIGIN                       |\n",
    "| ORIGIN_STATE_ABR      | Origin state abbreviation                                    | Flight | Useful for mapping                          |\n",
    "| ORIGIN_STATE_FIPS     | State FIPS code                                              | Flight | Redundant geographic ID                     |\n",
    "| ORIGIN_STATE_NM       | Full state name                                              | Flight | Informational only                          |\n",
    "| ORIGIN_WAC            | World Area Code for origin                                   | Flight | May be dropped                              |\n",
    "| DEST_AIRPORT_ID       | Unique numeric ID for destination airport                    | Flight | Key for joins                               |\n",
    "| DEST_AIRPORT_SEQ_ID   | Destination sequence ID                                      | Flight | Often redundant                             |\n",
    "| DEST_CITY_MARKET_ID   | Destination city market ID                                   | Flight | Identifies metro area                       |\n",
    "| DEST                  | Destination airport code (IATA)                              | Flight | Key feature                                 |\n",
    "| DEST_CITY_NAME        | Full city name of destination                                | Flight | Informational                               |\n",
    "| DEST_STATE_ABR        | Destination state abbreviation                               | Flight | Useful for mapping                          |\n",
    "| DEST_STATE_FIPS       | Destination state FIPS code                                  | Flight | Redundant                                   |\n",
    "| DEST_STATE_NM         | Full destination state name                                  | Flight | Informational only                          |\n",
    "| DEST_WAC              | World Area Code for destination                              | Flight | May be dropped                              |\n",
    "| CRS_DEP_TIME          | Scheduled departure time (HHMM local)                        | Flight | Convert to hour for modeling                |\n",
    "| DEP_TIME              | Actual departure time (HHMM local)                           | Flight | Post-departure → leakage                    |\n",
    "| DEP_DELAY             | Departure delay in minutes                                   | Flight | Leakage (after event)                       |\n",
    "| DEP_DELAY_NEW         | Departure delay, no negatives                                | Flight | Leakage (after event)                       |\n",
    "| DEP_DEL15             | 1 if departure delay ≥15 min                                 | Flight | Post-departure indicator                    |\n",
    "| DEP_DELAY_GROUP       | Categorical group of departure delay                         | Flight | Leakage variable                            |\n",
    "| DEP_TIME_BLK          | Scheduled departure block (time interval)                    | Flight | Keep for modeling                           |\n",
    "| TAXI_OUT              | Taxi-out time in minutes                                     | Flight | Leakage; occurs after departure             |\n",
    "| WHEELS_OFF            | Time wheels left ground (HHMM)                               | Flight | Leakage                                     |\n",
    "| WHEELS_ON             | Time wheels touched down (HHMM)                              | Flight | Leakage                                     |\n",
    "| TAXI_IN               | Taxi-in time (minutes)                                       | Flight | Leakage                                     |\n",
    "| CRS_ARR_TIME          | Scheduled arrival time (HHMM)                                | Flight | Keep; pre-scheduled info                    |\n",
    "| ARR_TIME              | Actual arrival time (HHMM)                                   | Flight | Leakage                                     |\n",
    "| ARR_DELAY             | Arrival delay (minutes)                                      | Flight | Target-related; drop                        |\n",
    "| ARR_DELAY_NEW         | Non-negative arrival delay                                   | Flight | Redundant                                   |\n",
    "| ARR_DEL15             | (1 if arrival delay ≥15 min)             | Flight | Binary label                                |\n",
    "| ARR_DELAY_GROUP       | Grouped arrival delay                                        | Flight | Redundant with ARR_DEL15                    |\n",
    "| ARR_TIME_BLK          | Scheduled arrival block                                      | Flight | Pre-scheduled; usable                       |\n",
    "| CANCELLED             | 1 if flight was cancelled                                    | Flight | Keep for classification                     |\n",
    "| CANCELLATION_CODE     | Code for reason of cancellation (A=Carrier, B=Weather, etc.) | Flight | Important categorical for cancelled flights |\n",
    "| DIVERTED              | 1 if flight diverted to another airport                      | Flight | Keep; rare event                            |\n",
    "| CRS_ELAPSED_TIME      | Scheduled elapsed flight time (min)                          | Flight | Useful duration variable                    |\n",
    "| ACTUAL_ELAPSED_TIME   | Actual total flight time (min)                               | Flight | Leakage                                     |\n",
    "| AIR_TIME              | In-air flight time (min)                                     | Flight | Leakage                                     |\n",
    "| FLIGHTS               | Number of flights (usually 1)                                | Flight | Constant; drop                              |\n",
    "| DISTANCE              | Great circle distance (miles)                                | Flight | Key continuous variable                     |\n",
    "| DISTANCE_GROUP        | Distance category (1=short haul, etc.)                       | Flight | Categorical; keep                           |\n",
    "| CARRIER_DELAY         | Delay due to airline (min)                                   | Flight | Post-event; leakage                         |\n",
    "| WEATHER_DELAY         | Delay due to weather (min)                                   | Flight | Leakage                                     |\n",
    "| NAS_DELAY             | Delay due to air traffic control (min)                       | Flight | Leakage                                     |\n",
    "| SECURITY_DELAY        | Delay due to security (min)                                  | Flight | Leakage                                     |\n",
    "| LATE_AIRCRAFT_DELAY   | Delay due to late incoming aircraft (min)                    | Flight | Leakage                                     |\n",
    "| FIRST_DEP_TIME        | First departure attempt (for multi-leg flights)              | Flight | Leakage                                     |\n",
    "| TOTAL_ADD_GTIME       | Total gate time added                                        | Flight | Leakage                                     |\n",
    "| LONGEST_ADD_GTIME     | Longest gate time added                                      | Flight | Leakage                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f303540d-c9e3-4a10-a43d-6274dc9d5baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Category                           | Columns                                                                                                                                                                                               | Action                       | Notes                                                                             |\n",
    "| :--------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------- | :-------------------------------------------------------------------------------- |\n",
    "| **Target**                         | `ARR_DEL15`                                                                                                                                                                                           | Keep                         | Binary classification label (1 = delay ≥15 min)                                   |\n",
    "| **Temporal Features**              | `MONTH`, `DAY_OF_WEEK`, `CRS_DEP_TIME`, `CRS_ARR_TIME`, `DEP_TIME_BLK`, `ARR_TIME_BLK`                                                                                                                | Keep (or engineer)           | Convert times to hour bins or features; captures time-of-day and seasonal effects |\n",
    "| **Flight Ops / Routing**           | `OP_UNIQUE_CARRIER`, `ORIGIN`, `DEST`, `DISTANCE`, `DISTANCE_GROUP`, `CANCELLED`, `DIVERTED`                                                                                                          | Keep                         | Core predictive variables; encode categorical features                            |\n",
    "| **Weather (Hourly)**               | `HourlyPrecipitation`, `HourlyVisibility`, `HourlyWindSpeed`                                                                                                                                          | Keep                         | Key real-time weather indicators                                                  |\n",
    "| **Weather (Hourly – Derived)**     | `HourlyPrecipitation_D`, `HourlyVisibility_D`, `HourlyWindSpeed_D`                                                                                                                                    | Keep                         | Change/delta features showing recent shifts                                       |\n",
    "| **Weather (Daily)**                | `DailyPrecipitation`, `DailyMaximumDryBulbTemperature`, `DailyMinimumDryBulbTemperature`, `DailyPeakWindSpeed`                                                                                        | Review                       | Optional aggregates for extended weather context                                  |\n",
    "| **Geographic / Station Info**      | `LATITUDE`, `LONGITUDE`, `ELEVATION`, `origin_station_dis`                                                                                                                                            | Review                       | Keep if you plan spatial or distance-based analysis                               |\n",
    "| **Carrier ID / Meta**              | `OP_CARRIER_AIRLINE_ID`, `OP_CARRIER`                                                                                                                                                                 | Drop                         | Redundant with `OP_UNIQUE_CARRIER`                                                |\n",
    "| **Delay & Post-Arrival Variables** | `DEP_DELAY`, `ARR_DELAY`, `AIR_TIME`, `ACTUAL_ELAPSED_TIME`, `WHEELS_OFF`, `WHEELS_ON`, `TAXI_OUT`, `TAXI_IN`, `CARRIER_DELAY`, `WEATHER_DELAY`, `NAS_DELAY`, `SECURITY_DELAY`, `LATE_AIRCRAFT_DELAY` | Drop (leakage)               | All occur after or depend on delay outcome                                        |\n",
    "| **Monthly / Climate Features**     | All `Monthly...` fields (`MonthlyAverageRH`, `MonthlyGreatestPrecip`, etc.)                                                                                                                           | Drop                         | More than 90% null and not relevant for single-flight prediction                  |\n",
    "| **High-Null / Empty Monthly & Short-Duration** | `MonthlyAverageRH`, `MonthlyDaysWithGT001Precip`, `MonthlyDaysWithGT010Precip`, `MonthlyDaysWithGT32Temp`, `MonthlyDaysWithGT90Temp`, `MonthlyDaysWithLT0Temp`, `MonthlyDaysWithLT32Temp`, `MonthlyDepartureFromNormalAverageTemperature`, `MonthlyDepartureFromNormalCoolingDegreeDays`, `MonthlyDepartureFromNormalHeatingDegreeDays`, `MonthlyDepartureFromNormalMaximumTemperature`, `MonthlyDepartureFromNormalMinimumTemperature`, `MonthlyDepartureFromNormalPrecipitation`, `MonthlyDewpointTemperature`, `MonthlyGreatestPrecip`, `MonthlyGreatestPrecipDate`, `MonthlyGreatestSnowDepth`, `MonthlyGreatestSnowDepthDate`, `MonthlyGreatestSnowfall`, `MonthlyGreatestSnowfallDate`, `MonthlyMaxSeaLevelPressureValue`, `MonthlyMaxSeaLevelPressureValueDate`, `MonthlyMaxSeaLevelPressureValueTime`, `MonthlyMaximumTemperature`, `MonthlyMeanTemperature`, `MonthlyMinSeaLevelPressureValue`, `MonthlyMinSeaLevelPressureValueDate`, `MonthlyMinSeaLevelPressureValueTime`, `MonthlyMinimumTemperature`, `MonthlySeaLevelPressure`, `MonthlyStationPressure`, `MonthlyTotalLiquidPrecipitation`, `MonthlyTotalSnowfall`, `MonthlyWetBulb`, `AWND`, `CDSD`, `CLDD`, `DSNW`, `HDSD`, `HTDD`, `NormalsCoolingDegreeDay`, `NormalsHeatingDegreeDay`, `ShortDurationEndDate005`, `ShortDurationEndDate010`, `ShortDurationEndDate015`, `ShortDurationEndDate020`, `ShortDurationEndDate030`, `ShortDurationEndDate045`, `ShortDurationEndDate060`, `ShortDurationEndDate080`, `ShortDurationEndDate100`, `ShortDurationEndDate120`, `ShortDurationEndDate150`, `ShortDurationEndDate180`, `ShortDurationPrecipitationValue005`, `ShortDurationPrecipitationValue010`, `ShortDurationPrecipitationValue015`, `ShortDurationPrecipitationValue020`, `ShortDurationPrecipitationValue030`, `ShortDurationPrecipitationValue045`, `ShortDurationPrecipitationValue060`, `ShortDurationPrecipitationValue080`, `ShortDurationPrecipitationValue100`, `ShortDurationPrecipitationValue120`, `ShortDurationPrecipitationValue150`, `ShortDurationPrecipitationValue180` | Drop   | All null in OTPW;  safe to drop to simplify schema |\n",
    "| **Backup / Metadata Fields**       | All `Backup...`, `ShortDuration...`, `_row_desc`, `REM`                                                                                                                                               | Drop                         | Join metadata and reference only                                                  |\n",
    "| **Text Fields**                    | `NAME`, `REPORT_TYPE`, `DailyWeather`, `HourlySkyConditions`, `HourlyPresentWeatherType`                                                                                                              | Optional (drop for baseline) | Free text; may require NLP or feature extraction later                            |\n",
    "| **Station Linking**                | `STATION`, `DATE`, `SOURCE`                                                                                                                                                                           | Keep for validation only     | Needed for join checks; not a model input                                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a47548-4298-4f1f-9d4a-7b1ca7fe4d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### iv. Airport–Weather Integration Plan (Data Joins, Issues & Rationale)\n",
    "\n",
    "Our exploratory review of the four source tables (flights, airport codes, weather, and station metadata) showed that the **biggest blockers are not in the flights themselves but in the lookup tables we need to join to**. The flights table already carries clean IATA airport codes (`ORIGIN`, `DEST`) and consistent date fields (`FL_DATE`, `YEAR`, `MONTH`, …), so it is a good factual backbone. However, our main airport codes file does **not** include time zones and sometimes only stores geolocation as a single `coordinates` string (`\"lon, lat\"`). At the same time, the external GitHub airport list **does** provide `timezone`, and often better lat/lon, but it doesn’t perfectly align 1:1 with our current codes file. This makes a direct “flights → airports → weather” join fragile, because we would be mixing two partially-overlapping airport catalogs. To fix this, we will first build **one master airport dimension** by joining the GitHub timezones and geolocation into the codes we already use in flights, and we will coalesce coordinates so that every IATA that appears as an origin/destination ends up with: **(a)** a timezone, **(b)** a lat/lon pair, and **(c)** a human-readable name.\n",
    "\n",
    "The second major issue is at the **weather** side: NOAA data is hourly and station-based (`STATION`, `DATE`, 100+ weather features), not airport-based. That means there is no native key to connect “ATL, SFO, ORD…” directly to a weather row. Also, stations and airports don’t share the exact same IDs: stations use a different identifier (and sometimes we need to normalize with the `stations.csv` file). On top of that, **weather is in UTC** while our flights are in **local airport time**, so if we don’t add airport timezones first, we can’t reliably pick “the weather hour that corresponds to this departure.” To solve this, we will: (1) compute airport → nearest-(1..3)-station pairs using the unified lat/lon we just created, (2) store that in a small bridge table (`airport_weather_station`), and (3) when we enrich flights, we will convert flight times to UTC using the airport’s timezone and then pick the matching hourly weather from the correct station. This approach gives us a repeatable pattern we can scale from the 3-month sample up to 2015–2021.\n",
    "\n",
    "**Key data problems we identified:**\n",
    "- Our original airport codes file **lacks time zones**, so flight local times cannot be aligned to UTC weather.\n",
    "- Airport geolocation is sometimes packed as a single text field (`coordinates`), so we must **parse and standardize lat/lon**.\n",
    "- Weather rows are **station-based, not airport-based**, so we must create an extra **airport → station** bridge.\n",
    "- Stations may come from **two slightly different sources** (`weather.csv` vs `stations.csv`), so we need **ID normalization**.\n",
    "- Not all flights’ origin/dest codes are guaranteed to appear in the GitHub airport list, so we will **fallback to the original codes file** to avoid losing rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1504b449-a360-441d-aa67-4385e03e153c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### v. Entity–Relationship Blueprint for Flights ↔ Airports ↔ Weather\n",
    "\n",
    "This diagram summarizes the core entities we will use to enrich flight records with meteorological data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ed5ada-5845-4882-90db-90f79b933a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<div class=\"mermaid\">\n",
       "erDiagram\n",
       "    %% LEGEND\n",
       "    %% PK_... = primary key (or business key)\n",
       "    %% FK_... = foreign key to another table\n",
       "\n",
       "    FLIGHTS {\n",
       "        string PK_flight_row\n",
       "        date FL_DATE\n",
       "        string FK_origin_iata_code\n",
       "        string FK_dest_iata_code\n",
       "        string OP_UNIQUE_CARRIER\n",
       "        int OP_CARRIER_FL_NUM\n",
       "        int CRS_DEP_TIME\n",
       "        int CRS_ARR_TIME\n",
       "        int YEAR\n",
       "        int MONTH\n",
       "        int DAY_OF_MONTH\n",
       "    }\n",
       "\n",
       "    MASTER_AIRPORTS {\n",
       "        string PK_iata_code\n",
       "        string ident\n",
       "        string name\n",
       "        string municipality\n",
       "        string iso_country\n",
       "        string iso_region\n",
       "        string airport_timezone\n",
       "        float lat\n",
       "        float lon\n",
       "    }\n",
       "\n",
       "    WEATHER {\n",
       "        string PK_station_id\n",
       "        datetime PK_obs_datetime\n",
       "        float LATITUDE\n",
       "        float LONGITUDE\n",
       "        string NAME\n",
       "        float HourlyDryBulbTemperature\n",
       "        float HourlyVisibility\n",
       "        float HourlyWindSpeed\n",
       "    }\n",
       "\n",
       "    NOAA_STATIONS {\n",
       "        string PK_station_id_norm\n",
       "        float lat\n",
       "        float lon\n",
       "        string neighbor_id\n",
       "        float distance_to_neighbor\n",
       "    }\n",
       "\n",
       "    AIRPORT_WEATHER_STATION {\n",
       "        string PK_iata_code\n",
       "        string PK_station_id\n",
       "        int PK_rank\n",
       "        float dist_km\n",
       "    }\n",
       "\n",
       "    CHECKPOINTS {\n",
       "        string checkpoint_name\n",
       "        string file_path\n",
       "        int rows\n",
       "        int columns\n",
       "        string description\n",
       "    }\n",
       "\n",
       "    %% RELATIONSHIPS\n",
       "    FLIGHTS }o--|| MASTER_AIRPORTS : \"origin (FK_origin_iata_code)\"\n",
       "    FLIGHTS }o--|| MASTER_AIRPORTS : \"destination (FK_dest_iata_code)\"\n",
       "    MASTER_AIRPORTS ||--o{ AIRPORT_WEATHER_STATION : \"airport → nearest stations\"\n",
       "    WEATHER ||--o{ AIRPORT_WEATHER_STATION : \"station in bridge\"\n",
       "    WEATHER }o--|| NOAA_STATIONS : \"normalize/enrich station\"\n",
       "    FLIGHTS ||--o{ CHECKPOINTS : \"pipeline stages\"\n",
       "</div>\n",
       "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
       "<script>mermaid.initialize({startOnLoad:true});</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mermaid_diagram_joins = \"\"\"\n",
    "<div class=\"mermaid\">\n",
    "erDiagram\n",
    "    %% LEGEND\n",
    "    %% PK_... = primary key (or business key)\n",
    "    %% FK_... = foreign key to another table\n",
    "\n",
    "    FLIGHTS {\n",
    "        string PK_flight_row\n",
    "        date FL_DATE\n",
    "        string FK_origin_iata_code\n",
    "        string FK_dest_iata_code\n",
    "        string OP_UNIQUE_CARRIER\n",
    "        int OP_CARRIER_FL_NUM\n",
    "        int CRS_DEP_TIME\n",
    "        int CRS_ARR_TIME\n",
    "        int YEAR\n",
    "        int MONTH\n",
    "        int DAY_OF_MONTH\n",
    "    }\n",
    "\n",
    "    MASTER_AIRPORTS {\n",
    "        string PK_iata_code\n",
    "        string ident\n",
    "        string name\n",
    "        string municipality\n",
    "        string iso_country\n",
    "        string iso_region\n",
    "        string airport_timezone\n",
    "        float lat\n",
    "        float lon\n",
    "    }\n",
    "\n",
    "    WEATHER {\n",
    "        string PK_station_id\n",
    "        datetime PK_obs_datetime\n",
    "        float LATITUDE\n",
    "        float LONGITUDE\n",
    "        string NAME\n",
    "        float HourlyDryBulbTemperature\n",
    "        float HourlyVisibility\n",
    "        float HourlyWindSpeed\n",
    "    }\n",
    "\n",
    "    NOAA_STATIONS {\n",
    "        string PK_station_id_norm\n",
    "        float lat\n",
    "        float lon\n",
    "        string neighbor_id\n",
    "        float distance_to_neighbor\n",
    "    }\n",
    "\n",
    "    AIRPORT_WEATHER_STATION {\n",
    "        string PK_iata_code\n",
    "        string PK_station_id\n",
    "        int PK_rank\n",
    "        float dist_km\n",
    "    }\n",
    "\n",
    "    CHECKPOINTS {\n",
    "        string checkpoint_name\n",
    "        string file_path\n",
    "        int rows\n",
    "        int columns\n",
    "        string description\n",
    "    }\n",
    "\n",
    "    %% RELATIONSHIPS\n",
    "    FLIGHTS }o--|| MASTER_AIRPORTS : \"origin (FK_origin_iata_code)\"\n",
    "    FLIGHTS }o--|| MASTER_AIRPORTS : \"destination (FK_dest_iata_code)\"\n",
    "    MASTER_AIRPORTS ||--o{ AIRPORT_WEATHER_STATION : \"airport → nearest stations\"\n",
    "    WEATHER ||--o{ AIRPORT_WEATHER_STATION : \"station in bridge\"\n",
    "    WEATHER }o--|| NOAA_STATIONS : \"normalize/enrich station\"\n",
    "    FLIGHTS ||--o{ CHECKPOINTS : \"pipeline stages\"\n",
    "</div>\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
    "<script>mermaid.initialize({startOnLoad:true});</script>\n",
    "\"\"\"\n",
    "displayHTML(mermaid_diagram_joins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9a256c-2acc-496a-9748-0ecae1533bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### vi. To-Do List for Airport–Weather Join Pipeline\n",
    "\n",
    "**Phase 0 – Staging (raw → stg)**  \n",
    "- [ ] Load **flights** (3m / 6m / 1y sample) into `stg_flights`  \n",
    "- [ ] Load **airport codes** (current `codes.csv`) into `stg_airport_codes`  \n",
    "- [ ] Load **GitHub airports with timezone** into `stg_airport_tz`  \n",
    "- [ ] Load **weather** (NOAA hourly) into `stg_weather_hourly`  \n",
    "- [ ] Load **station metadata** (`stations.csv`) into `stg_noaa_stations`\n",
    "\n",
    "**Phase 1 – Unified airport dimension**  \n",
    "- [ ] Uppercase and trim IATA codes in **both** airport sources  \n",
    "- [ ] Parse `coordinates` from `stg_airport_codes` → (`codes_lat`, `codes_lon`)  \n",
    "- [ ] Select from `stg_airport_tz` only the needed fields: (`iata_code`, `airport_timezone`, `gh_lat`, `gh_lon`)  \n",
    "- [ ] Left-join timezone + better lat/lon into `stg_airport_codes` and **coalesce** → `dim_airport (master_airports)`  \n",
    "- [ ] Compare `dim_airport` to distinct `ORIGIN` and `DEST` from flights to find **missing airports**\n",
    "\n",
    "**Phase 2 – Weather station dimension**  \n",
    "- [ ] Build `dim_weather_station` = distinct (`STATION`, `LATITUDE`, `LONGITUDE`, `NAME`) from `stg_weather_hourly`  \n",
    "- [ ] Left-join to `stg_noaa_stations` to fill missing coordinates / IDs  \n",
    "- [ ] Validate that all stations used in weather have lat/lon\n",
    "\n",
    "**Phase 3 – Airport ↔ station bridge (nearest K)**  \n",
    "- [ ] Broadcast `dim_airport` (only airports with lat/lon)  \n",
    "- [ ] Cross-join with `dim_weather_station`  \n",
    "- [ ] Compute **Haversine** distance → `dist_km`  \n",
    "- [ ] Window/partition by airport and keep top **K=3** nearest stations  \n",
    "- [ ] Save as `airport_weather_station (iata_code, STATION, dist_km, rank)`\n",
    "\n",
    "**Phase 4 – Time alignment**  \n",
    "- [ ] From flights, build `dep_ts_local` = `FL_DATE` + `CRS_DEP_TIME`  \n",
    "- [ ] Convert `dep_ts_local` to UTC using `dim_airport.airport_timezone` (origin)  \n",
    "- [ ] Repeat for arrival using destination airport  \n",
    "- [ ] (Optional) Materialize `dim_date` / `dim_time` for reporting and easier joins\n",
    "\n",
    "**Phase 5 – Final enrichment views + QA**  \n",
    "- [ ] Create `v_flights_with_origin_weather`:\n",
    "  - join flights → origin airport → bridge (rank=1) → weather on matching UTC hour  \n",
    "- [ ] Create `v_flights_with_dest_weather`:\n",
    "  - join flights → destination airport → bridge (rank=1) → weather on matching UTC hour  \n",
    "- [ ] Coverage report:\n",
    "  - % flights with origin weather  \n",
    "  - % flights with destination weather  \n",
    "  - airports with `dist_km > 300` (flag for manual review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7d340f-708f-4c31-9008-434683c351bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751257f3-0f9f-404a-8e34-1c8bd57475b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration: FUTURE\n  flights_path: dbfs:/student-groups/Group_4_4/future_joins/BTS_OnTime/parquet_airlines_data_2020_2024_std.parquet\n  weather_path: dbfs:/student-groups/Group_4_4/future_joins/NOAA/LCDv2_weather_data_2020_2024_std.parquet\n  joined_output: dbfs:/student-groups/Group_4_4/JOINED_FUTURE.parquet\n  max_year: None\n  max_month: None\n"
     ]
    }
   ],
   "source": [
    "# CONFIG: choose data slice and IO paths\n",
    "\n",
    "# Options: \"3M\", \"1Y\", \"5Y\", \"FUTURE\"\n",
    "DATA_SLICE = \"FUTURE\"\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    \"3M\": {\n",
    "        # course 3-month subset\n",
    "        \"flights_path\": \"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/\",\n",
    "        \"weather_path\": \"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_3m\",\n",
    "        \"joined_output\": \"dbfs:/student-groups/Group_4_4/JOINED_3M.parquet\",\n",
    "        # The 3M dataset is already sliced\n",
    "        \"max_year\": None,\n",
    "        \"max_month\": None,\n",
    "    },\n",
    "    \"1Y\": {\n",
    "        # course 1-year subset\n",
    "        \"flights_path\": \"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/\",\n",
    "        \"weather_path\": \"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_1y/\",\n",
    "        \"joined_output\": \"dbfs:/student-groups/Group_4_4/JOINED_1Y.parquet\",\n",
    "        \"max_year\": None,\n",
    "        \"max_month\": None,\n",
    "    },\n",
    "    \"5Y\": {\n",
    "        # full 2015–2021 flights + weather, but we keep only up to 2019-12\n",
    "        \"flights_path\": \"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/\",\n",
    "        \"weather_path\": \"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data\",\n",
    "        \"joined_output\": \"dbfs:/student-groups/Group_4_4/JOINED_5Y_2015_2019.parquet\",\n",
    "        \"max_year\": 2019,\n",
    "        \"max_month\": 12,\n",
    "    },\n",
    "    \"FUTURE\": {\n",
    "        # placeholders for future pipelines\n",
    "        \"flights_path\": \"dbfs:/student-groups/Group_4_4/future_joins/BTS_OnTime/parquet_airlines_data_2020_2024_std.parquet\",\n",
    "        \"weather_path\": \"dbfs:/student-groups/Group_4_4/future_joins/NOAA/LCDv2_weather_data_2020_2024_std.parquet\",\n",
    "        \"joined_output\": \"dbfs:/student-groups/Group_4_4/JOINED_FUTURE.parquet\",\n",
    "        \"max_year\": None,\n",
    "        \"max_month\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = DATA_CONFIG[DATA_SLICE]\n",
    "\n",
    "print(f\"Using configuration: {DATA_SLICE}\")\n",
    "for k, v in cfg.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9de8c2-a81f-4457-ad75-88af7da2c71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e54164-2350-4368-bece-bd3df9d49924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import Window as W\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0495dccb-14f1-4da0-a930-8338077da535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load stats helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe29119-56a8-497f-988a-93f4ab8ee9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job start: 2025-12-14T18:10:29.575959\n"
     ]
    }
   ],
   "source": [
    "# RUN STATS: start timer & helper functions\n",
    "\n",
    "job_start = datetime.now()\n",
    "print(f\"Job start: {job_start.isoformat()}\")\n",
    "\n",
    "# Ensure dbutils is available (works in Databricks)\n",
    "try:\n",
    "    dbutils  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    dbutils = DBUtils(spark)\n",
    "\n",
    "def get_dir_size(path: str) -> int:\n",
    "    \"\"\"\n",
    "    Recursively compute total size (in bytes) of all files under `path`.\n",
    "    Works for DBFS paths like 'dbfs:/mnt/...'.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for f in dbutils.fs.ls(path):\n",
    "        if f.isDir():\n",
    "            total += get_dir_size(f.path)\n",
    "        else:\n",
    "            total += f.size\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069202dd-c1a1-491a-8659-eaeb239076c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d8553b-b455-49e8-b8e0-8b3607882dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to pretty print databases\n",
    "def show_df(df, n=5):\n",
    "    \"\"\"Pretty print the first `n` rows of a Spark DataFrame using Databricks display.\"\"\"\n",
    "    display(df.limit(n))\n",
    "\n",
    "# Helper function to display columns of a Spark DataFrame\n",
    "def show_columns(df):\n",
    "    \"\"\"Display the column names, data types, and % of null values of a Spark DataFrame.\"\"\"\n",
    "    total_rows = df.count()\n",
    "    null_counts = df.select([sf.count(sf.when(sf.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    percent_null = {c: (null_counts[c] / total_rows * 100) if total_rows > 0 else None for c in df.columns}\n",
    "    col_info = pd.DataFrame({\n",
    "        \"Column\": df.columns,\n",
    "        \"Type\": [t for _, t in df.dtypes],\n",
    "        \"% Null\": [percent_null[c] for c in df.columns]\n",
    "    })\n",
    "    display(col_info)\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "\n",
    "# Helper function haversine calculation\n",
    "def haversine_km_expr(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Great-circle distance on a sphere (WGS84 mean Earth radius).\n",
    "    All arguments are Column[double] in radians.\n",
    "    Returns Column[double] in kilometers.\n",
    "    \"\"\"\n",
    "    dlat = (lat2 - lat1)\n",
    "    dlon = (lon2 - lon1)\n",
    "    a = sf.pow(sf.sin(dlat / 2), 2) + sf.cos(lat1) * sf.cos(lat2) * sf.pow(sf.sin(dlon / 2), 2)\n",
    "    c = 2 * sf.atan2(sf.sqrt(a), sf.sqrt(1 - a))\n",
    "    return sf.lit(6371.0088) * c  # mean Earth radius in km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98aceea-5604-49a1-ba31-fa1f539c1f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffa695f-16b5-4f16-b16e-fab03f66f5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flights - raw rows: 31,339,836, distinct rows: 31,339,836\n"
     ]
    }
   ],
   "source": [
    "# Flights data\n",
    "\n",
    "\"\"\"\n",
    "Flights data\n",
    "\n",
    "     This is a subset of the passenger flight's on-time performance data taken from the TranStats data collection available from the U.S. Department of Transportation (DOT)\n",
    "\n",
    "        https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ \n",
    "\n",
    "    Links to an external site. \n",
    "\n",
    "The flight dataset was downloaded from the US Department of Transportation\n",
    "Links to an external site. and contains flight information from 2015 to 2021\n",
    "(Note flight data for the period [2015-2019] has the following dimensionality  31,746,841 x 109)\n",
    "A Data Dictionary for this dataset is located here:\n",
    "\n",
    "    https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ \n",
    "\"\"\"\n",
    "\n",
    "# Load flights based on DATA_SLICE\n",
    "df_flights = spark.read.parquet(cfg[\"flights_path\"])\n",
    "\n",
    "# Drop exact duplicates\n",
    "n_raw = df_flights.count()\n",
    "df_flights = df_flights.dropDuplicates()\n",
    "n_distinct = df_flights.count()\n",
    "print(f\"Flights - raw rows: {n_raw:,}, distinct rows: {n_distinct:,}\")\n",
    "\n",
    "# Display Results\n",
    "# show_df(df_flights, 5)\n",
    "# show_columns(df_flights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74e0cfb-44fe-4aeb-8bfc-04d1041cffcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather - raw rows: 756,182,081\n"
     ]
    }
   ],
   "source": [
    "# Weather data\n",
    "\n",
    "\"\"\"\n",
    "Weather table\n",
    "\n",
    "    As a frequent flyer, we know that flight departure (and arrival)  often get affected by weather conditions, so it makes sense to collect, and process weather data corresponding to the origin and destination airports at the time of departure and arrival, respectively, and build features based upon this data. \n",
    "    The weather dataset was downloaded from the National Oceanic and Atmospheric Administration repository \n",
    "\n",
    "Links to an external site. and contains weather information from 2015 to 2021\n",
    "\n",
    "    The dimensionality of the weather data for the period [2015-2019] is 630,904,436 x 177\n",
    "\n",
    "Data dictionary (subset): \n",
    "\n",
    "    Please refer to pages 8-12:  https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf \n",
    "\n",
    "Links to an external site. \n",
    "A better version of the data dictionary can be read here: https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf\n",
    "Links to an external site.\n",
    "\n",
    "    A superset of the features is described here:\n",
    "\n",
    "        https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf \n",
    "\n",
    "    Links to an external site.\n",
    "\n",
    "A subset of the features is shared here:\n",
    "\n",
    "    https://docs.google.com/spreadsheets/d/1v0P34NlQKrvXGCACKDeqgxpDwmj3HxaleUiTrY7VRn0/edit#gid=0 \n",
    "\"\"\"\n",
    "\n",
    "# Load weather based on DATA_SLICE\n",
    "df_weather = spark.read.parquet(cfg[\"weather_path\"])\n",
    "\n",
    "print(f\"Weather - raw rows: {df_weather.count():,}\")\n",
    "\n",
    "# Allowed hourly report types (no QCLCD daily/monthly summaries)\n",
    "ALLOWED_RPT = [\"FM-15\", \"FM-16\", \"FM-12\"]  # METAR, SPECI, SYNOP\n",
    "\n",
    "# Filter to allowed report types AND create obs_utc from DATE\n",
    "weather_hourly = (\n",
    "    df_weather\n",
    "    .filter(sf.col(\"REPORT_TYPE\").isin(ALLOWED_RPT))\n",
    "    .withColumn(\"obs_utc\", sf.col(\"DATE\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# Preference: METAR > SPECI > SYNOP, then latest timestamp\n",
    "weather_ranked = weather_hourly.withColumn(\n",
    "    \"report_type_rank\",\n",
    "    sf.when(sf.col(\"REPORT_TYPE\")==\"FM-15\", 1)\n",
    "     .when(sf.col(\"REPORT_TYPE\")==\"FM-16\", 2)\n",
    "     .when(sf.col(\"REPORT_TYPE\")==\"FM-12\", 3)\n",
    "     .otherwise(99)\n",
    ")\n",
    "\n",
    "win_st_hr = W.partitionBy(\"STATION\",\"obs_utc\") \\\n",
    "             .orderBy(sf.col(\"report_type_rank\").asc(), sf.col(\"DATE\").desc())\n",
    "\n",
    "weather_best = (\n",
    "    weather_ranked\n",
    "    .withColumn(\"rn\", sf.row_number().over(win_st_hr))\n",
    "    .filter(sf.col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"report_type_rank\")\n",
    ")\n",
    "\n",
    "# Display results\n",
    "# show_df(df_weather, 5)\n",
    "# show_columns(df_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854761dc-0db6-4ff7-bcb4-904f4f654abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather station data\n",
    "\n",
    "\"\"\"\n",
    "Airport dataset\n",
    "    Overall the airport dataset provides some metadata about each airport.\n",
    "    The airport dataset was downloaded from the US Department of Transportation and has the following dimensionality: 18,097 x 10.\n",
    "    It is located here:\n",
    "        dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data\n",
    "\n",
    "\"\"\"\n",
    "df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet\")\n",
    "\n",
    "# Display results\n",
    "# show_df(df_stations, 5)\n",
    "# show_columns(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e03558-9bbb-4538-b258-195ab684e024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airport codes\n",
    "\"\"\"\n",
    "Airport codes Table\n",
    "\n",
    "    Airport codes may refer to either:\n",
    "         IATA airport code, a three-letter code that is used in passenger reservation, ticketing, and baggage-handling systems,\n",
    "        or ICAO airport code which is a four-letter code used by ATC systems and for airports that do not have an IATA airport code (from Wikipedia).\n",
    "    Here you will need to import an external airport code conversion set (source: https://datahub.io/core/airport-codes \n",
    "\n",
    "Links to an external site.) and join the airport codes to the airline's flights table on the IATA code (3-letter code used by passengers)\n",
    "\"\"\"\n",
    "\n",
    "# Download and load airport codes CSV to Spark DataFrame\n",
    "url = \"https://datahub.io/core/airport-codes/_r/-/data/airport-codes.csv\"\n",
    "local_path = \"/tmp/airport-codes.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "df_codes = spark.read.format(\"csv\").option(\"header\", True).load(local_path)\n",
    "\n",
    "# Display results\n",
    "# show_df(df_codes, 5)\n",
    "# show_columns(df_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3d488d-fac2-493f-91d8-8850da1fd203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Airport Timezone & Geolocation Table\n",
    "\n",
    "    This external table augments airport identifiers with the fields we need\n",
    "    for time alignment and spatial joins. Each record includes:\n",
    "        • IATA (3-letter passenger code) and ICAO (4-letter ATC code)\n",
    "        • Airport name and locality metadata\n",
    "        • Latitude and longitude (decimal degrees)\n",
    "        • Time zone in IANA format (e.g., \"America/Los_Angeles\")\n",
    "\n",
    "    Source:\n",
    "        https://raw.githubusercontent.com/lxndrblz/Airports/main/airports.csv\n",
    "        (Project page: https://github.com/lxndrblz/Airports)\n",
    "\n",
    "    Usage:\n",
    "        Import this CSV and join to the flights table on IATA (3-letter code used\n",
    "        in `ORIGIN` / `DEST`). Use the `timezone` column to convert local flight\n",
    "        times (e.g., FL_DATE + CRS_DEP_TIME) to UTC before weather joins, and use\n",
    "        `latitude`/`longitude` to compute nearest NOAA weather stations.\n",
    "\n",
    "    Notes:\n",
    "        • Normalize IATA to uppercase and drop duplicates before joining.\n",
    "        • Prefer this table’s lat/lon and timezone; if an airport is missing,\n",
    "          fall back to the original codes file.\n",
    "        • The timezone field follows IANA; ensure your Spark build supports\n",
    "          IANA names when calling `to_utc_timestamp`.\n",
    "        • /tmp is ephemeral and may be cleared when the cluster shuts down; use DBFS for persistent storage.\n",
    "\"\"\"\n",
    "\n",
    "dbutils.fs.mkdirs(\"dbfs:/student-groups/Group_4_4\")\n",
    "local_path = \"/dbfs/student-groups/Group_4_4/airport-zones.csv\"\n",
    "url = \"https://raw.githubusercontent.com/lxndrblz/Airports/main/airports.csv\"\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "df_zones = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/student-groups/Group_4_4/airport-zones.csv\")\n",
    "\n",
    "# Display results\n",
    "# show_df(df_zones)\n",
    "# show_columns(df_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0581a9d-2e15-496f-8539-a73d0197d9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Raw Dataframes Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7970b1-fa79-4b1a-8a38-73de8baab6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ed662c-53b4-44fd-89d2-eaf4a321d7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build MASTER_AIRPORTS with timezone +  latitude and longitude\n",
    "codes = (\n",
    "    df_codes\n",
    "    .withColumn(\"iata_code\", sf.upper(\"iata_code\"))\n",
    "    .withColumn(\"_coords\", sf.split(sf.regexp_replace(sf.col(\"coordinates\"), \"\\\\s+\", \"\"), \",\"))\n",
    "    .withColumn(\"codes_lon\", sf.col(\"_coords\").getItem(0).cast(\"double\"))\n",
    "    .withColumn(\"codes_lat\", sf.col(\"_coords\").getItem(1).cast(\"double\"))\n",
    "    .drop(\"_coords\")\n",
    ")\n",
    "\n",
    "# Display results\n",
    "# show_df(codes, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6a79eb-36a2-46e9-b11d-9b38689367bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tz = (\n",
    "    df_zones\n",
    "    .withColumn(\"iata_code\", sf.upper(sf.col(\"code\")))\n",
    "    .select(\n",
    "        sf.col(\"iata_code\"),\n",
    "        sf.col(\"time_zone\").alias(\"airport_timezone\"),\n",
    "        sf.col(\"latitude\").cast(\"double\").alias(\"gh_lat\"),\n",
    "        sf.col(\"longitude\").cast(\"double\").alias(\"gh_lon\"),\n",
    "        sf.col(\"name\").alias(\"gh_name\")\n",
    "    )\n",
    "    .dropna(subset=[\"iata_code\"])\n",
    "    .dropDuplicates([\"iata_code\"])\n",
    ")\n",
    "\n",
    "# Display results\n",
    "# show_df(tz, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfca0c0-515b-429b-8570-cf63eb663e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = (\n",
    "    codes.alias(\"c\")\n",
    "    .join(tz.alias(\"g\"), on=\"iata_code\", how=\"left\")\n",
    "    .withColumn(\"lat\", sf.coalesce(\"g.gh_lat\", \"c.codes_lat\"))\n",
    "    .withColumn(\"lon\", sf.coalesce(\"g.gh_lon\", \"c.codes_lon\"))\n",
    "    .withColumn(\"airport_timezone\", sf.col(\"airport_timezone\"))\n",
    "    .select(\"iata_code\", \"ident\", sf.col(\"c.type\").alias(\"airport_type\"), \n",
    "            \"name\", \"municipality\", \"iso_country\", \"iso_region\", \"airport_timezone\", \"lat\", \"lon\")\n",
    "    .dropna(subset=[\"iata_code\"])\n",
    "    .dropDuplicates([\"iata_code\"])\n",
    ")\n",
    "\n",
    "# Display results\n",
    "# show_df(df_airports, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53deaf9e-4abd-45b5-ac42-2dbb2e219999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Weather Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c37635-febf-4c03-ad8b-9a575ebab4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>station_id</th><th>lat</th><th>lon</th></tr></thead><tbody><tr><td>70275546404</td><td>61.437</td><td>-142.904</td></tr><tr><td>72224503882</td><td>30.212</td><td>-85.683</td></tr><tr><td>72226613850</td><td>32.35</td><td>-86.983</td></tr><tr><td>70104526649</td><td>68.031</td><td>-162.903</td></tr><tr><td>72074924255</td><td>48.35</td><td>-122.667</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "70275546404",
         61.437,
         -142.904
        ],
        [
         "72224503882",
         30.212,
         -85.683
        ],
        [
         "72226613850",
         32.35,
         -86.983
        ],
        [
         "70104526649",
         68.031,
         -162.903
        ],
        [
         "72074924255",
         48.35,
         -122.667
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "station_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "lon",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>station_id</td><td>string</td><td>0.0</td></tr><tr><td>lat</td><td>double</td><td>0.0</td></tr><tr><td>lon</td><td>double</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "station_id",
         "string",
         0.0
        ],
        [
         "lat",
         "double",
         0.0
        ],
        [
         "lon",
         "double",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2237\n"
     ]
    }
   ],
   "source": [
    "# Cleanup Weather stations\n",
    "df_weather_station = (\n",
    "    df_stations\n",
    "    .select(\n",
    "        sf.col(\"station_id\").cast(\"string\").alias(\"station_id\"),\n",
    "        sf.col(\"lat\").cast(\"double\").alias(\"lat\"),\n",
    "        sf.col(\"lon\").cast(\"double\").alias(\"lon\")\n",
    "    )\n",
    "    .dropna(subset=[\"station_id\",\"lat\",\"lon\"])\n",
    "    .dropDuplicates([\"station_id\"])\n",
    ")\n",
    "\n",
    "# Display results\n",
    "show_df(df_weather_station, 5)\n",
    "show_columns(df_weather_station)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8eab614-af6e-48b4-a4a3-6b6d731ef17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>iata_code</th><th>lat</th><th>lon</th><th>lat_rad</th><th>lon_rad</th></tr></thead><tbody><tr><td>AMF</td><td>-4.166667</td><td>141.66667</td><td>-0.07272205798419458</td><td>2.4725498318362753</td></tr><tr><td>HAE</td><td>36.230556</td><td>-112.66944</td><td>0.6323424920281867</td><td>-1.9664526943782</td></tr><tr><td>LFH</td><td>26.64507115</td><td>99.54446199297831</td><td>0.4650442209956519</td><td>1.737378613903828</td></tr><tr><td>JNN</td><td>60.134167</td><td>-45.233612</td><td>1.049539207094121</td><td>-0.7894754619696173</td></tr><tr><td>BNF</td><td>57.0</td><td>-135.0</td><td>0.9948376736367679</td><td>-2.356194490192345</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "AMF",
         -4.166667,
         141.66667,
         -0.07272205798419458,
         2.4725498318362753
        ],
        [
         "HAE",
         36.230556,
         -112.66944,
         0.6323424920281867,
         -1.9664526943782
        ],
        [
         "LFH",
         26.64507115,
         99.54446199297831,
         0.4650442209956519,
         1.737378613903828
        ],
        [
         "JNN",
         60.134167,
         -45.233612,
         1.049539207094121,
         -0.7894754619696173
        ],
        [
         "BNF",
         57.0,
         -135.0,
         0.9948376736367679,
         -2.356194490192345
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "iata_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "lon",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "lat_rad",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "lon_rad",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>iata_code</td><td>string</td><td>0.0</td></tr><tr><td>lat</td><td>double</td><td>0.0</td></tr><tr><td>lon</td><td>double</td><td>0.0</td></tr><tr><td>lat_rad</td><td>double</td><td>0.0</td></tr><tr><td>lon_rad</td><td>double</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "iata_code",
         "string",
         0.0
        ],
        [
         "lat",
         "double",
         0.0
        ],
        [
         "lon",
         "double",
         0.0
        ],
        [
         "lat_rad",
         "double",
         0.0
        ],
        [
         "lon_rad",
         "double",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 9096\n"
     ]
    }
   ],
   "source": [
    "# Prepare airport coordinates in radians\n",
    "airport_radians = (\n",
    "    df_airports\n",
    "    .dropna(subset=[\"lat\", \"lon\"])\n",
    "    .withColumn(\"lat_rad\", sf.radians(\"lat\"))\n",
    "    .withColumn(\"lon_rad\", sf.radians(\"lon\"))\n",
    "    .select(\"iata_code\", \"lat\", \"lon\", \"lat_rad\", \"lon_rad\")  \n",
    ")\n",
    "\n",
    "# Display results\n",
    "show_df(airport_radians, 5)\n",
    "show_columns(airport_radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997f07d8-12fb-4b35-9a7e-7b0ee30ad9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>station_id</th><th>lat</th><th>lon</th><th>st_lat_rad</th><th>st_lon_rad</th></tr></thead><tbody><tr><td>70275546404</td><td>61.437</td><td>-142.904</td><td>1.0722779325477563</td><td>-2.4941453142699768</td></tr><tr><td>72224503882</td><td>30.212</td><td>-85.683</td><td>0.5272988736125268</td><td>-1.4954504629863015</td></tr><tr><td>72226613850</td><td>32.35</td><td>-86.983</td><td>0.5646140130201657</td><td>-1.5181397432622277</td></tr><tr><td>70104526649</td><td>68.031</td><td>-162.903</td><td>1.1873649434242624</td><td>-2.8431937113763226</td></tr><tr><td>72074924255</td><td>48.35</td><td>-122.667</td><td>0.8438666933392583</td><td>-2.140943033543884</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "70275546404",
         61.437,
         -142.904,
         1.0722779325477563,
         -2.4941453142699768
        ],
        [
         "72224503882",
         30.212,
         -85.683,
         0.5272988736125268,
         -1.4954504629863015
        ],
        [
         "72226613850",
         32.35,
         -86.983,
         0.5646140130201657,
         -1.5181397432622277
        ],
        [
         "70104526649",
         68.031,
         -162.903,
         1.1873649434242624,
         -2.8431937113763226
        ],
        [
         "72074924255",
         48.35,
         -122.667,
         0.8438666933392583,
         -2.140943033543884
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "station_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "lon",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "st_lat_rad",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "st_lon_rad",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>station_id</td><td>string</td><td>0.0</td></tr><tr><td>lat</td><td>double</td><td>0.0</td></tr><tr><td>lon</td><td>double</td><td>0.0</td></tr><tr><td>st_lat_rad</td><td>double</td><td>0.0</td></tr><tr><td>st_lon_rad</td><td>double</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "station_id",
         "string",
         0.0
        ],
        [
         "lat",
         "double",
         0.0
        ],
        [
         "lon",
         "double",
         0.0
        ],
        [
         "st_lat_rad",
         "double",
         0.0
        ],
        [
         "st_lon_rad",
         "double",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2237\n"
     ]
    }
   ],
   "source": [
    "# Prepare stations coordinates in radians\n",
    "stations_radians = (\n",
    "    df_weather_station\n",
    "    .dropna(subset=[\"lat\", \"lon\"])\n",
    "    .withColumn(\"st_lat_rad\", sf.radians(\"lat\"))\n",
    "    .withColumn(\"st_lon_rad\", sf.radians(\"lon\"))\n",
    "    .select(sf.col(\"station_id\").alias(\"station_id\"), \"lat\", \"lon\", \"st_lat_rad\", \"st_lon_rad\")\n",
    ")\n",
    "# Display results\n",
    "show_df(stations_radians, 5)\n",
    "show_columns(stations_radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f99c1e-84b3-491a-87df-fe6554be0dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>station_id</th><th>lat</th><th>lon</th><th>st_lat_rad</th><th>st_lon_rad</th></tr></thead><tbody><tr><td>70275546404</td><td>61.437</td><td>-142.904</td><td>1.0722779325477563</td><td>-2.4941453142699768</td></tr><tr><td>72224503882</td><td>30.212</td><td>-85.683</td><td>0.5272988736125268</td><td>-1.4954504629863015</td></tr><tr><td>72226613850</td><td>32.35</td><td>-86.983</td><td>0.5646140130201657</td><td>-1.5181397432622277</td></tr><tr><td>70104526649</td><td>68.031</td><td>-162.903</td><td>1.1873649434242624</td><td>-2.8431937113763226</td></tr><tr><td>72074924255</td><td>48.35</td><td>-122.667</td><td>0.8438666933392583</td><td>-2.140943033543884</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "70275546404",
         61.437,
         -142.904,
         1.0722779325477563,
         -2.4941453142699768
        ],
        [
         "72224503882",
         30.212,
         -85.683,
         0.5272988736125268,
         -1.4954504629863015
        ],
        [
         "72226613850",
         32.35,
         -86.983,
         0.5646140130201657,
         -1.5181397432622277
        ],
        [
         "70104526649",
         68.031,
         -162.903,
         1.1873649434242624,
         -2.8431937113763226
        ],
        [
         "72074924255",
         48.35,
         -122.667,
         0.8438666933392583,
         -2.140943033543884
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "station_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "lon",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "st_lat_rad",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "st_lon_rad",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>station_id</td><td>string</td><td>0.0</td></tr><tr><td>lat</td><td>double</td><td>0.0</td></tr><tr><td>lon</td><td>double</td><td>0.0</td></tr><tr><td>st_lat_rad</td><td>double</td><td>0.0</td></tr><tr><td>st_lon_rad</td><td>double</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "station_id",
         "string",
         0.0
        ],
        [
         "lat",
         "double",
         0.0
        ],
        [
         "lon",
         "double",
         0.0
        ],
        [
         "st_lat_rad",
         "double",
         0.0
        ],
        [
         "st_lon_rad",
         "double",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2237\n"
     ]
    }
   ],
   "source": [
    "# Cross join (broadcast the small side), compute distance, rank, keep top-3 per airport\n",
    "airports_stations_cross = (\n",
    "    sf.broadcast(airport_radians).crossJoin(stations_radians)\n",
    "    .withColumn(\"dist_km\", haversine_km_expr(sf.col(\"lat_rad\"), sf.col(\"lon_rad\"),\n",
    "                                             sf.col(\"st_lat_rad\"), sf.col(\"st_lon_rad\")))\n",
    "    .select(\n",
    "        sf.col(\"iata_code\"),\n",
    "        sf.col(\"station_id\").alias(\"STATION\"),\n",
    "        sf.col(\"dist_km\")\n",
    "    )\n",
    ")\n",
    "station_rank = W.partitionBy(\"iata_code\").orderBy(sf.col(\"dist_km\").asc())\n",
    "airport_weather_station = (\n",
    "    airports_stations_cross\n",
    "    .withColumn(\"rank\", sf.row_number().over(station_rank))\n",
    "    .filter(sf.col(\"rank\") <= 3)\n",
    "    .select(\"iata_code\", \"STATION\", \"dist_km\", \"rank\")\n",
    ")\n",
    "# Display results\n",
    "show_df(stations_radians, 5)\n",
    "show_columns(stations_radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6197fbb8-a3b7-484f-9a74-65188627cfcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>QUARTER</th><th>MONTH</th><th>DAY_OF_MONTH</th><th>DAY_OF_WEEK</th><th>FL_DATE</th><th>OP_UNIQUE_CARRIER</th><th>OP_CARRIER_AIRLINE_ID</th><th>OP_CARRIER</th><th>TAIL_NUM</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN_AIRPORT_ID</th><th>ORIGIN_AIRPORT_SEQ_ID</th><th>ORIGIN_CITY_MARKET_ID</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>ORIGIN_STATE_ABR</th><th>ORIGIN_STATE_FIPS</th><th>ORIGIN_STATE_NM</th><th>ORIGIN_WAC</th><th>DEST_AIRPORT_ID</th><th>DEST_AIRPORT_SEQ_ID</th><th>DEST_CITY_MARKET_ID</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DEST_STATE_ABR</th><th>DEST_STATE_FIPS</th><th>DEST_STATE_NM</th><th>DEST_WAC</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>DEP_DELAY</th><th>DEP_DELAY_NEW</th><th>DEP_DEL15</th><th>DEP_DELAY_GROUP</th><th>DEP_TIME_BLK</th><th>TAXI_OUT</th><th>WHEELS_OFF</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>ARR_DELAY</th><th>ARR_DELAY_NEW</th><th>ARR_DEL15</th><th>ARR_DELAY_GROUP</th><th>ARR_TIME_BLK</th><th>CANCELLED</th><th>CANCELLATION_CODE</th><th>DIVERTED</th><th>CRS_ELAPSED_TIME</th><th>ACTUAL_ELAPSED_TIME</th><th>AIR_TIME</th><th>FLIGHTS</th><th>DISTANCE</th><th>DISTANCE_GROUP</th><th>CARRIER_DELAY</th><th>WEATHER_DELAY</th><th>NAS_DELAY</th><th>SECURITY_DELAY</th><th>LATE_AIRCRAFT_DELAY</th><th>FIRST_DEP_TIME</th><th>TOTAL_ADD_GTIME</th><th>LONGEST_ADD_GTIME</th><th>DIV_AIRPORT_LANDINGS</th><th>DIV_REACHED_DEST</th><th>DIV_ACTUAL_ELAPSED_TIME</th><th>DIV_ARR_DELAY</th><th>DIV_DISTANCE</th><th>DIV1_AIRPORT</th><th>DIV1_AIRPORT_ID</th><th>DIV1_AIRPORT_SEQ_ID</th><th>DIV1_WHEELS_ON</th><th>DIV1_TOTAL_GTIME</th><th>DIV1_LONGEST_GTIME</th><th>DIV1_WHEELS_OFF</th><th>DIV1_TAIL_NUM</th><th>DIV2_AIRPORT</th><th>DIV2_AIRPORT_ID</th><th>DIV2_AIRPORT_SEQ_ID</th><th>DIV2_WHEELS_ON</th><th>DIV2_TOTAL_GTIME</th><th>DIV2_LONGEST_GTIME</th><th>DIV2_WHEELS_OFF</th><th>DIV2_TAIL_NUM</th><th>DIV3_AIRPORT</th><th>DIV3_AIRPORT_ID</th><th>DIV3_AIRPORT_SEQ_ID</th><th>DIV3_WHEELS_ON</th><th>DIV3_TOTAL_GTIME</th><th>DIV3_LONGEST_GTIME</th><th>DIV3_WHEELS_OFF</th><th>DIV3_TAIL_NUM</th><th>DIV4_AIRPORT</th><th>DIV4_AIRPORT_ID</th><th>DIV4_AIRPORT_SEQ_ID</th><th>DIV4_WHEELS_ON</th><th>DIV4_TOTAL_GTIME</th><th>DIV4_LONGEST_GTIME</th><th>DIV4_WHEELS_OFF</th><th>DIV4_TAIL_NUM</th><th>DIV5_AIRPORT</th><th>DIV5_AIRPORT_ID</th><th>DIV5_AIRPORT_SEQ_ID</th><th>DIV5_WHEELS_ON</th><th>DIV5_TOTAL_GTIME</th><th>DIV5_LONGEST_GTIME</th><th>DIV5_WHEELS_OFF</th><th>DIV5_TAIL_NUM</th><th>YEAR</th><th>CRS_DEP_TIME_str</th><th>dep_hh</th><th>dep_mm</th><th>FL_DATE_str</th><th>dep_local_ts</th></tr></thead><tbody><tr><td>2</td><td>6</td><td>26</td><td>7</td><td>2022-06-26</td><td>OH</td><td>20397</td><td>OH</td><td>N570NN</td><td>5404</td><td>11481</td><td>1148102</td><td>31481</td><td>ECP</td><td>Panama City, FL</td><td>FL</td><td>12</td><td>Florida</td><td>33</td><td>11057</td><td>1105703</td><td>31057</td><td>CLT</td><td>Charlotte, NC</td><td>NC</td><td>37</td><td>North Carolina</td><td>36</td><td>1100</td><td>1053</td><td>-7.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>1100-1159</td><td>16.0</td><td>1109</td><td>1320</td><td>9.0</td><td>1339</td><td>1329</td><td>-10.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>1300-1359</td><td>0.0</td><td>null</td><td>0.0</td><td>99.0</td><td>96.0</td><td>71.0</td><td>1.0</td><td>437.0</td><td>2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2022</td><td>1100</td><td>11</td><td>0</td><td>2022-06-26</td><td>2022-06-26T11:00:00Z</td></tr><tr><td>2</td><td>6</td><td>30</td><td>4</td><td>2022-06-30</td><td>OH</td><td>20397</td><td>OH</td><td>N556NN</td><td>5409</td><td>10785</td><td>1078502</td><td>30785</td><td>BTV</td><td>Burlington, VT</td><td>VT</td><td>50</td><td>Vermont</td><td>16</td><td>11278</td><td>1127805</td><td>30852</td><td>DCA</td><td>Washington, DC</td><td>VA</td><td>51</td><td>Virginia</td><td>38</td><td>1240</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1200-1259</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1424</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1400-1459</td><td>1.0</td><td>A</td><td>0.0</td><td>104.0</td><td>null</td><td>null</td><td>1.0</td><td>437.0</td><td>2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2022</td><td>1240</td><td>12</td><td>40</td><td>2022-06-30</td><td>2022-06-30T12:40:00Z</td></tr><tr><td>2</td><td>6</td><td>3</td><td>5</td><td>2022-06-03</td><td>OH</td><td>20397</td><td>OH</td><td>N543EA</td><td>5415</td><td>14576</td><td>1457606</td><td>34576</td><td>ROC</td><td>Rochester, NY</td><td>NY</td><td>36</td><td>New York</td><td>22</td><td>11057</td><td>1105703</td><td>31057</td><td>CLT</td><td>Charlotte, NC</td><td>NC</td><td>37</td><td>North Carolina</td><td>36</td><td>500</td><td>452</td><td>-8.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>0001-0559</td><td>14.0</td><td>506</td><td>639</td><td>5.0</td><td>652</td><td>644</td><td>-8.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>0600-0659</td><td>0.0</td><td>null</td><td>0.0</td><td>112.0</td><td>112.0</td><td>93.0</td><td>1.0</td><td>573.0</td><td>3</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2022</td><td>0500</td><td>5</td><td>0</td><td>2022-06-03</td><td>2022-06-03T05:00:00Z</td></tr><tr><td>2</td><td>6</td><td>21</td><td>2</td><td>2022-06-21</td><td>OH</td><td>20397</td><td>OH</td><td>N521AE</td><td>5434</td><td>11057</td><td>1105703</td><td>31057</td><td>CLT</td><td>Charlotte, NC</td><td>NC</td><td>37</td><td>North Carolina</td><td>36</td><td>15249</td><td>1524906</td><td>35249</td><td>TLH</td><td>Tallahassee, FL</td><td>FL</td><td>12</td><td>Florida</td><td>33</td><td>1800</td><td>1832</td><td>32.0</td><td>32.0</td><td>1.0</td><td>2</td><td>1800-1859</td><td>52.0</td><td>1924</td><td>2019</td><td>3.0</td><td>1932</td><td>2022</td><td>50.0</td><td>50.0</td><td>1.0</td><td>3</td><td>1900-1959</td><td>0.0</td><td>null</td><td>0.0</td><td>92.0</td><td>110.0</td><td>55.0</td><td>1.0</td><td>386.0</td><td>2</td><td>32.0</td><td>0.0</td><td>18.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2022</td><td>1800</td><td>18</td><td>0</td><td>2022-06-21</td><td>2022-06-21T18:00:00Z</td></tr><tr><td>2</td><td>6</td><td>8</td><td>3</td><td>2022-06-08</td><td>OH</td><td>20397</td><td>OH</td><td>N574NN</td><td>5451</td><td>11057</td><td>1105703</td><td>31057</td><td>CLT</td><td>Charlotte, NC</td><td>NC</td><td>37</td><td>North Carolina</td><td>36</td><td>11721</td><td>1172105</td><td>31721</td><td>FNT</td><td>Flint, MI</td><td>MI</td><td>26</td><td>Michigan</td><td>43</td><td>1105</td><td>1104</td><td>-1.0</td><td>0.0</td><td>0.0</td><td>-1</td><td>1100-1159</td><td>22.0</td><td>1126</td><td>1252</td><td>2.0</td><td>1311</td><td>1254</td><td>-17.0</td><td>0.0</td><td>0.0</td><td>-2</td><td>1300-1359</td><td>0.0</td><td>null</td><td>0.0</td><td>126.0</td><td>110.0</td><td>86.0</td><td>1.0</td><td>555.0</td><td>3</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2022</td><td>1105</td><td>11</td><td>5</td><td>2022-06-08</td><td>2022-06-08T11:05:00Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         6,
         26,
         7,
         "2022-06-26",
         "OH",
         20397,
         "OH",
         "N570NN",
         5404,
         11481,
         1148102,
         31481,
         "ECP",
         "Panama City, FL",
         "FL",
         12,
         "Florida",
         33,
         11057,
         1105703,
         31057,
         "CLT",
         "Charlotte, NC",
         "NC",
         37,
         "North Carolina",
         36,
         1100,
         1053,
         -7.0,
         0.0,
         0.0,
         -1,
         "1100-1159",
         16.0,
         1109,
         1320,
         9.0,
         1339,
         1329,
         -10.0,
         0.0,
         0.0,
         -1,
         "1300-1359",
         0.0,
         null,
         0.0,
         99.0,
         96.0,
         71.0,
         1.0,
         437.0,
         2,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         0,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         2022,
         "1100",
         11,
         0,
         "2022-06-26",
         "2022-06-26T11:00:00Z"
        ],
        [
         2,
         6,
         30,
         4,
         "2022-06-30",
         "OH",
         20397,
         "OH",
         "N556NN",
         5409,
         10785,
         1078502,
         30785,
         "BTV",
         "Burlington, VT",
         "VT",
         50,
         "Vermont",
         16,
         11278,
         1127805,
         30852,
         "DCA",
         "Washington, DC",
         "VA",
         51,
         "Virginia",
         38,
         1240,
         null,
         null,
         null,
         null,
         null,
         "1200-1259",
         null,
         null,
         null,
         null,
         1424,
         null,
         null,
         null,
         null,
         null,
         "1400-1459",
         1.0,
         "A",
         0.0,
         104.0,
         null,
         null,
         1.0,
         437.0,
         2,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         0,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         2022,
         "1240",
         12,
         40,
         "2022-06-30",
         "2022-06-30T12:40:00Z"
        ],
        [
         2,
         6,
         3,
         5,
         "2022-06-03",
         "OH",
         20397,
         "OH",
         "N543EA",
         5415,
         14576,
         1457606,
         34576,
         "ROC",
         "Rochester, NY",
         "NY",
         36,
         "New York",
         22,
         11057,
         1105703,
         31057,
         "CLT",
         "Charlotte, NC",
         "NC",
         37,
         "North Carolina",
         36,
         500,
         452,
         -8.0,
         0.0,
         0.0,
         -1,
         "0001-0559",
         14.0,
         506,
         639,
         5.0,
         652,
         644,
         -8.0,
         0.0,
         0.0,
         -1,
         "0600-0659",
         0.0,
         null,
         0.0,
         112.0,
         112.0,
         93.0,
         1.0,
         573.0,
         3,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         0,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         2022,
         "0500",
         5,
         0,
         "2022-06-03",
         "2022-06-03T05:00:00Z"
        ],
        [
         2,
         6,
         21,
         2,
         "2022-06-21",
         "OH",
         20397,
         "OH",
         "N521AE",
         5434,
         11057,
         1105703,
         31057,
         "CLT",
         "Charlotte, NC",
         "NC",
         37,
         "North Carolina",
         36,
         15249,
         1524906,
         35249,
         "TLH",
         "Tallahassee, FL",
         "FL",
         12,
         "Florida",
         33,
         1800,
         1832,
         32.0,
         32.0,
         1.0,
         2,
         "1800-1859",
         52.0,
         1924,
         2019,
         3.0,
         1932,
         2022,
         50.0,
         50.0,
         1.0,
         3,
         "1900-1959",
         0.0,
         null,
         0.0,
         92.0,
         110.0,
         55.0,
         1.0,
         386.0,
         2,
         32.0,
         0.0,
         18.0,
         0.0,
         0.0,
         null,
         null,
         null,
         0,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         2022,
         "1800",
         18,
         0,
         "2022-06-21",
         "2022-06-21T18:00:00Z"
        ],
        [
         2,
         6,
         8,
         3,
         "2022-06-08",
         "OH",
         20397,
         "OH",
         "N574NN",
         5451,
         11057,
         1105703,
         31057,
         "CLT",
         "Charlotte, NC",
         "NC",
         37,
         "North Carolina",
         36,
         11721,
         1172105,
         31721,
         "FNT",
         "Flint, MI",
         "MI",
         26,
         "Michigan",
         43,
         1105,
         1104,
         -1.0,
         0.0,
         0.0,
         -1,
         "1100-1159",
         22.0,
         1126,
         1252,
         2.0,
         1311,
         1254,
         -17.0,
         0.0,
         0.0,
         -2,
         "1300-1359",
         0.0,
         null,
         0.0,
         126.0,
         110.0,
         86.0,
         1.0,
         555.0,
         3,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         0,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         2022,
         "1105",
         11,
         5,
         "2022-06-08",
         "2022-06-08T11:05:00Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "QUARTER",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "MONTH",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_MONTH",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_WEEK",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_UNIQUE_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_AIRLINE_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TAIL_NUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_AIRPORT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_AIRPORT_SEQ_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_MARKET_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_STATE_ABR",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_STATE_FIPS",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_STATE_NM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_WAC",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEST_AIRPORT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEST_AIRPORT_SEQ_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_MARKET_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_STATE_ABR",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_STATE_FIPS",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEST_STATE_NM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_WAC",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY_NEW",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DEL15",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY_GROUP",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME_BLK",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_OUT",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_OFF",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY_NEW",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DEL15",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY_GROUP",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME_BLK",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLATION_CODE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIVERTED",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ELAPSED_TIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACTUAL_ELAPSED_TIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "AIR_TIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "FLIGHTS",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE_GROUP",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CARRIER_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "WEATHER_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "NAS_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "SECURITY_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "LATE_AIRCRAFT_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_DEP_TIME",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "TOTAL_ADD_GTIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "LONGEST_ADD_GTIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV_AIRPORT_LANDINGS",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DIV_REACHED_DEST",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV_ACTUAL_ELAPSED_TIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV_ARR_DELAY",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV_DISTANCE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_AIRPORT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_AIRPORT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_AIRPORT_SEQ_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_WHEELS_ON",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_TOTAL_GTIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_LONGEST_GTIME",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_WHEELS_OFF",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "DIV1_TAIL_NUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_AIRPORT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_AIRPORT_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_AIRPORT_SEQ_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_WHEELS_ON",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_TOTAL_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_LONGEST_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_WHEELS_OFF",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV2_TAIL_NUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_AIRPORT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_AIRPORT_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_AIRPORT_SEQ_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_WHEELS_ON",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_TOTAL_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_LONGEST_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_WHEELS_OFF",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV3_TAIL_NUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_AIRPORT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_AIRPORT_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_AIRPORT_SEQ_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_WHEELS_ON",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_TOTAL_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_LONGEST_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_WHEELS_OFF",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV4_TAIL_NUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_AIRPORT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_AIRPORT_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_AIRPORT_SEQ_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_WHEELS_ON",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_TOTAL_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_LONGEST_GTIME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_WHEELS_OFF",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DIV5_TAIL_NUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "YEAR",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME_str",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dep_hh",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dep_mm",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE_str",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dep_local_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>QUARTER</td><td>int</td><td>0.0</td></tr><tr><td>MONTH</td><td>int</td><td>0.0</td></tr><tr><td>DAY_OF_MONTH</td><td>int</td><td>0.0</td></tr><tr><td>DAY_OF_WEEK</td><td>int</td><td>0.0</td></tr><tr><td>FL_DATE</td><td>string</td><td>0.0</td></tr><tr><td>OP_UNIQUE_CARRIER</td><td>string</td><td>0.0</td></tr><tr><td>OP_CARRIER_AIRLINE_ID</td><td>int</td><td>0.0</td></tr><tr><td>OP_CARRIER</td><td>string</td><td>0.0</td></tr><tr><td>TAIL_NUM</td><td>string</td><td>0.7675151841892216</td></tr><tr><td>OP_CARRIER_FL_NUM</td><td>int</td><td>3.1908271632308482E-6</td></tr><tr><td>ORIGIN_AIRPORT_ID</td><td>int</td><td>0.0</td></tr><tr><td>ORIGIN_AIRPORT_SEQ_ID</td><td>int</td><td>0.0</td></tr><tr><td>ORIGIN_CITY_MARKET_ID</td><td>int</td><td>0.0</td></tr><tr><td>ORIGIN</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN_CITY_NAME</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN_STATE_ABR</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN_STATE_FIPS</td><td>int</td><td>0.0</td></tr><tr><td>ORIGIN_STATE_NM</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN_WAC</td><td>int</td><td>0.0</td></tr><tr><td>DEST_AIRPORT_ID</td><td>int</td><td>0.0</td></tr><tr><td>DEST_AIRPORT_SEQ_ID</td><td>int</td><td>0.0</td></tr><tr><td>DEST_CITY_MARKET_ID</td><td>int</td><td>0.0</td></tr><tr><td>DEST</td><td>string</td><td>0.0</td></tr><tr><td>DEST_CITY_NAME</td><td>string</td><td>0.0</td></tr><tr><td>DEST_STATE_ABR</td><td>string</td><td>0.0</td></tr><tr><td>DEST_STATE_FIPS</td><td>int</td><td>0.0</td></tr><tr><td>DEST_STATE_NM</td><td>string</td><td>0.0</td></tr><tr><td>DEST_WAC</td><td>int</td><td>0.0</td></tr><tr><td>CRS_DEP_TIME</td><td>int</td><td>0.0</td></tr><tr><td>DEP_TIME</td><td>int</td><td>2.3449037831595545</td></tr><tr><td>DEP_DELAY</td><td>double</td><td>2.3468214702846564</td></tr><tr><td>DEP_DELAY_NEW</td><td>double</td><td>2.3468214702846564</td></tr><tr><td>DEP_DEL15</td><td>double</td><td>2.3468214702846564</td></tr><tr><td>DEP_DELAY_GROUP</td><td>int</td><td>2.3468214702846564</td></tr><tr><td>DEP_TIME_BLK</td><td>string</td><td>0.0</td></tr><tr><td>TAXI_OUT</td><td>double</td><td>2.382016293895093</td></tr><tr><td>WHEELS_OFF</td><td>int</td><td>2.382016293895093</td></tr><tr><td>WHEELS_ON</td><td>int</td><td>2.414521250206925</td></tr><tr><td>TAXI_IN</td><td>double</td><td>2.414521250206925</td></tr><tr><td>CRS_ARR_TIME</td><td>int</td><td>0.0</td></tr><tr><td>ARR_TIME</td><td>int</td><td>2.4144446703550075</td></tr><tr><td>ARR_DELAY</td><td>double</td><td>2.6208816153345538</td></tr><tr><td>ARR_DELAY_NEW</td><td>double</td><td>2.6208816153345538</td></tr><tr><td>ARR_DEL15</td><td>double</td><td>2.6208816153345538</td></tr><tr><td>ARR_DELAY_GROUP</td><td>int</td><td>2.6208816153345538</td></tr><tr><td>ARR_TIME_BLK</td><td>string</td><td>0.0</td></tr><tr><td>CANCELLED</td><td>double</td><td>0.0</td></tr><tr><td>CANCELLATION_CODE</td><td>string</td><td>97.60803470700995</td></tr><tr><td>DIVERTED</td><td>double</td><td>0.0</td></tr><tr><td>CRS_ELAPSED_TIME</td><td>double</td><td>3.8289925958770175E-5</td></tr><tr><td>ACTUAL_ELAPSED_TIME</td><td>double</td><td>2.6208816153345538</td></tr><tr><td>AIR_TIME</td><td>double</td><td>2.6208816153345538</td></tr><tr><td>FLIGHTS</td><td>double</td><td>0.0</td></tr><tr><td>DISTANCE</td><td>double</td><td>0.0</td></tr><tr><td>DISTANCE_GROUP</td><td>int</td><td>0.0</td></tr><tr><td>CARRIER_DELAY</td><td>double</td><td>81.9536037138165</td></tr><tr><td>WEATHER_DELAY</td><td>double</td><td>81.9536037138165</td></tr><tr><td>NAS_DELAY</td><td>double</td><td>81.9536037138165</td></tr><tr><td>SECURITY_DELAY</td><td>double</td><td>81.9536037138165</td></tr><tr><td>LATE_AIRCRAFT_DELAY</td><td>double</td><td>81.9536037138165</td></tr><tr><td>FIRST_DEP_TIME</td><td>int</td><td>99.37850663928171</td></tr><tr><td>TOTAL_ADD_GTIME</td><td>double</td><td>99.37860874575094</td></tr><tr><td>LONGEST_ADD_GTIME</td><td>double</td><td>99.3786119365781</td></tr><tr><td>DIV_AIRPORT_LANDINGS</td><td>int</td><td>3.031285805069305E-4</td></tr><tr><td>DIV_REACHED_DEST</td><td>double</td><td>99.77109644096414</td></tr><tr><td>DIV_ACTUAL_ELAPSED_TIME</td><td>double</td><td>99.7937513138231</td></tr><tr><td>DIV_ARR_DELAY</td><td>double</td><td>99.79356624584761</td></tr><tr><td>DIV_DISTANCE</td><td>double</td><td>99.77109644096414</td></tr><tr><td>DIV1_AIRPORT</td><td>string</td><td>99.76083793163436</td></tr><tr><td>DIV1_AIRPORT_ID</td><td>int</td><td>99.76083793163436</td></tr><tr><td>DIV1_AIRPORT_SEQ_ID</td><td>int</td><td>99.76083793163436</td></tr><tr><td>DIV1_WHEELS_ON</td><td>int</td><td>99.76085069494302</td></tr><tr><td>DIV1_TOTAL_GTIME</td><td>double</td><td>99.76084112246151</td></tr><tr><td>DIV1_LONGEST_GTIME</td><td>double</td><td>99.76084112246151</td></tr><tr><td>DIV1_WHEELS_OFF</td><td>int</td><td>99.79237925814289</td></tr><tr><td>DIV1_TAIL_NUM</td><td>string</td><td>99.79238563979722</td></tr><tr><td>DIV2_AIRPORT</td><td>string</td><td>99.99776003933141</td></tr><tr><td>DIV2_AIRPORT_ID</td><td>string</td><td>99.99774727602276</td></tr><tr><td>DIV2_AIRPORT_SEQ_ID</td><td>string</td><td>99.99774727602276</td></tr><tr><td>DIV2_WHEELS_ON</td><td>string</td><td>99.99776003933141</td></tr><tr><td>DIV2_TOTAL_GTIME</td><td>string</td><td>99.99776003933141</td></tr><tr><td>DIV2_LONGEST_GTIME</td><td>string</td><td>99.99776003933141</td></tr><tr><td>DIV2_WHEELS_OFF</td><td>string</td><td>99.99896617199911</td></tr><tr><td>DIV2_TAIL_NUM</td><td>string</td><td>99.99896617199911</td></tr><tr><td>DIV3_AIRPORT</td><td>string</td><td>99.99998723669134</td></tr><tr><td>DIV3_AIRPORT_ID</td><td>string</td><td>99.99998723669134</td></tr><tr><td>DIV3_AIRPORT_SEQ_ID</td><td>string</td><td>99.99998723669134</td></tr><tr><td>DIV3_WHEELS_ON</td><td>string</td><td>99.99998723669134</td></tr><tr><td>DIV3_TOTAL_GTIME</td><td>string</td><td>99.99998723669134</td></tr><tr><td>DIV3_LONGEST_GTIME</td><td>string</td><td>99.99998723669134</td></tr><tr><td>DIV3_WHEELS_OFF</td><td>string</td><td>99.99999361834567</td></tr><tr><td>DIV3_TAIL_NUM</td><td>string</td><td>99.99999361834567</td></tr><tr><td>DIV4_AIRPORT</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_AIRPORT_ID</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_AIRPORT_SEQ_ID</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_WHEELS_ON</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_TOTAL_GTIME</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_LONGEST_GTIME</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_WHEELS_OFF</td><td>string</td><td>100.0</td></tr><tr><td>DIV4_TAIL_NUM</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_AIRPORT</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_AIRPORT_ID</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_AIRPORT_SEQ_ID</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_WHEELS_ON</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_TOTAL_GTIME</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_LONGEST_GTIME</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_WHEELS_OFF</td><td>string</td><td>100.0</td></tr><tr><td>DIV5_TAIL_NUM</td><td>string</td><td>100.0</td></tr><tr><td>YEAR</td><td>int</td><td>0.0</td></tr><tr><td>CRS_DEP_TIME_str</td><td>string</td><td>0.0</td></tr><tr><td>dep_hh</td><td>int</td><td>0.0</td></tr><tr><td>dep_mm</td><td>int</td><td>0.0</td></tr><tr><td>FL_DATE_str</td><td>string</td><td>0.0</td></tr><tr><td>dep_local_ts</td><td>timestamp</td><td>3.1908271632308482E-6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "QUARTER",
         "int",
         0.0
        ],
        [
         "MONTH",
         "int",
         0.0
        ],
        [
         "DAY_OF_MONTH",
         "int",
         0.0
        ],
        [
         "DAY_OF_WEEK",
         "int",
         0.0
        ],
        [
         "FL_DATE",
         "string",
         0.0
        ],
        [
         "OP_UNIQUE_CARRIER",
         "string",
         0.0
        ],
        [
         "OP_CARRIER_AIRLINE_ID",
         "int",
         0.0
        ],
        [
         "OP_CARRIER",
         "string",
         0.0
        ],
        [
         "TAIL_NUM",
         "string",
         0.7675151841892216
        ],
        [
         "OP_CARRIER_FL_NUM",
         "int",
         3.1908271632308482E-6
        ],
        [
         "ORIGIN_AIRPORT_ID",
         "int",
         0.0
        ],
        [
         "ORIGIN_AIRPORT_SEQ_ID",
         "int",
         0.0
        ],
        [
         "ORIGIN_CITY_MARKET_ID",
         "int",
         0.0
        ],
        [
         "ORIGIN",
         "string",
         0.0
        ],
        [
         "ORIGIN_CITY_NAME",
         "string",
         0.0
        ],
        [
         "ORIGIN_STATE_ABR",
         "string",
         0.0
        ],
        [
         "ORIGIN_STATE_FIPS",
         "int",
         0.0
        ],
        [
         "ORIGIN_STATE_NM",
         "string",
         0.0
        ],
        [
         "ORIGIN_WAC",
         "int",
         0.0
        ],
        [
         "DEST_AIRPORT_ID",
         "int",
         0.0
        ],
        [
         "DEST_AIRPORT_SEQ_ID",
         "int",
         0.0
        ],
        [
         "DEST_CITY_MARKET_ID",
         "int",
         0.0
        ],
        [
         "DEST",
         "string",
         0.0
        ],
        [
         "DEST_CITY_NAME",
         "string",
         0.0
        ],
        [
         "DEST_STATE_ABR",
         "string",
         0.0
        ],
        [
         "DEST_STATE_FIPS",
         "int",
         0.0
        ],
        [
         "DEST_STATE_NM",
         "string",
         0.0
        ],
        [
         "DEST_WAC",
         "int",
         0.0
        ],
        [
         "CRS_DEP_TIME",
         "int",
         0.0
        ],
        [
         "DEP_TIME",
         "int",
         2.3449037831595545
        ],
        [
         "DEP_DELAY",
         "double",
         2.3468214702846564
        ],
        [
         "DEP_DELAY_NEW",
         "double",
         2.3468214702846564
        ],
        [
         "DEP_DEL15",
         "double",
         2.3468214702846564
        ],
        [
         "DEP_DELAY_GROUP",
         "int",
         2.3468214702846564
        ],
        [
         "DEP_TIME_BLK",
         "string",
         0.0
        ],
        [
         "TAXI_OUT",
         "double",
         2.382016293895093
        ],
        [
         "WHEELS_OFF",
         "int",
         2.382016293895093
        ],
        [
         "WHEELS_ON",
         "int",
         2.414521250206925
        ],
        [
         "TAXI_IN",
         "double",
         2.414521250206925
        ],
        [
         "CRS_ARR_TIME",
         "int",
         0.0
        ],
        [
         "ARR_TIME",
         "int",
         2.4144446703550075
        ],
        [
         "ARR_DELAY",
         "double",
         2.6208816153345538
        ],
        [
         "ARR_DELAY_NEW",
         "double",
         2.6208816153345538
        ],
        [
         "ARR_DEL15",
         "double",
         2.6208816153345538
        ],
        [
         "ARR_DELAY_GROUP",
         "int",
         2.6208816153345538
        ],
        [
         "ARR_TIME_BLK",
         "string",
         0.0
        ],
        [
         "CANCELLED",
         "double",
         0.0
        ],
        [
         "CANCELLATION_CODE",
         "string",
         97.60803470700995
        ],
        [
         "DIVERTED",
         "double",
         0.0
        ],
        [
         "CRS_ELAPSED_TIME",
         "double",
         3.8289925958770175E-5
        ],
        [
         "ACTUAL_ELAPSED_TIME",
         "double",
         2.6208816153345538
        ],
        [
         "AIR_TIME",
         "double",
         2.6208816153345538
        ],
        [
         "FLIGHTS",
         "double",
         0.0
        ],
        [
         "DISTANCE",
         "double",
         0.0
        ],
        [
         "DISTANCE_GROUP",
         "int",
         0.0
        ],
        [
         "CARRIER_DELAY",
         "double",
         81.9536037138165
        ],
        [
         "WEATHER_DELAY",
         "double",
         81.9536037138165
        ],
        [
         "NAS_DELAY",
         "double",
         81.9536037138165
        ],
        [
         "SECURITY_DELAY",
         "double",
         81.9536037138165
        ],
        [
         "LATE_AIRCRAFT_DELAY",
         "double",
         81.9536037138165
        ],
        [
         "FIRST_DEP_TIME",
         "int",
         99.37850663928171
        ],
        [
         "TOTAL_ADD_GTIME",
         "double",
         99.37860874575094
        ],
        [
         "LONGEST_ADD_GTIME",
         "double",
         99.3786119365781
        ],
        [
         "DIV_AIRPORT_LANDINGS",
         "int",
         3.031285805069305E-4
        ],
        [
         "DIV_REACHED_DEST",
         "double",
         99.77109644096414
        ],
        [
         "DIV_ACTUAL_ELAPSED_TIME",
         "double",
         99.7937513138231
        ],
        [
         "DIV_ARR_DELAY",
         "double",
         99.79356624584761
        ],
        [
         "DIV_DISTANCE",
         "double",
         99.77109644096414
        ],
        [
         "DIV1_AIRPORT",
         "string",
         99.76083793163436
        ],
        [
         "DIV1_AIRPORT_ID",
         "int",
         99.76083793163436
        ],
        [
         "DIV1_AIRPORT_SEQ_ID",
         "int",
         99.76083793163436
        ],
        [
         "DIV1_WHEELS_ON",
         "int",
         99.76085069494302
        ],
        [
         "DIV1_TOTAL_GTIME",
         "double",
         99.76084112246151
        ],
        [
         "DIV1_LONGEST_GTIME",
         "double",
         99.76084112246151
        ],
        [
         "DIV1_WHEELS_OFF",
         "int",
         99.79237925814289
        ],
        [
         "DIV1_TAIL_NUM",
         "string",
         99.79238563979722
        ],
        [
         "DIV2_AIRPORT",
         "string",
         99.99776003933141
        ],
        [
         "DIV2_AIRPORT_ID",
         "string",
         99.99774727602276
        ],
        [
         "DIV2_AIRPORT_SEQ_ID",
         "string",
         99.99774727602276
        ],
        [
         "DIV2_WHEELS_ON",
         "string",
         99.99776003933141
        ],
        [
         "DIV2_TOTAL_GTIME",
         "string",
         99.99776003933141
        ],
        [
         "DIV2_LONGEST_GTIME",
         "string",
         99.99776003933141
        ],
        [
         "DIV2_WHEELS_OFF",
         "string",
         99.99896617199911
        ],
        [
         "DIV2_TAIL_NUM",
         "string",
         99.99896617199911
        ],
        [
         "DIV3_AIRPORT",
         "string",
         99.99998723669134
        ],
        [
         "DIV3_AIRPORT_ID",
         "string",
         99.99998723669134
        ],
        [
         "DIV3_AIRPORT_SEQ_ID",
         "string",
         99.99998723669134
        ],
        [
         "DIV3_WHEELS_ON",
         "string",
         99.99998723669134
        ],
        [
         "DIV3_TOTAL_GTIME",
         "string",
         99.99998723669134
        ],
        [
         "DIV3_LONGEST_GTIME",
         "string",
         99.99998723669134
        ],
        [
         "DIV3_WHEELS_OFF",
         "string",
         99.99999361834567
        ],
        [
         "DIV3_TAIL_NUM",
         "string",
         99.99999361834567
        ],
        [
         "DIV4_AIRPORT",
         "string",
         100.0
        ],
        [
         "DIV4_AIRPORT_ID",
         "string",
         100.0
        ],
        [
         "DIV4_AIRPORT_SEQ_ID",
         "string",
         100.0
        ],
        [
         "DIV4_WHEELS_ON",
         "string",
         100.0
        ],
        [
         "DIV4_TOTAL_GTIME",
         "string",
         100.0
        ],
        [
         "DIV4_LONGEST_GTIME",
         "string",
         100.0
        ],
        [
         "DIV4_WHEELS_OFF",
         "string",
         100.0
        ],
        [
         "DIV4_TAIL_NUM",
         "string",
         100.0
        ],
        [
         "DIV5_AIRPORT",
         "string",
         100.0
        ],
        [
         "DIV5_AIRPORT_ID",
         "string",
         100.0
        ],
        [
         "DIV5_AIRPORT_SEQ_ID",
         "string",
         100.0
        ],
        [
         "DIV5_WHEELS_ON",
         "string",
         100.0
        ],
        [
         "DIV5_TOTAL_GTIME",
         "string",
         100.0
        ],
        [
         "DIV5_LONGEST_GTIME",
         "string",
         100.0
        ],
        [
         "DIV5_WHEELS_OFF",
         "string",
         100.0
        ],
        [
         "DIV5_TAIL_NUM",
         "string",
         100.0
        ],
        [
         "YEAR",
         "int",
         0.0
        ],
        [
         "CRS_DEP_TIME_str",
         "string",
         0.0
        ],
        [
         "dep_hh",
         "int",
         0.0
        ],
        [
         "dep_mm",
         "int",
         0.0
        ],
        [
         "FL_DATE_str",
         "string",
         0.0
        ],
        [
         "dep_local_ts",
         "timestamp",
         3.1908271632308482E-6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31339836\n"
     ]
    }
   ],
   "source": [
    "# Prediction timestamp (origin): build local time from schedule (no leakage), then T–2h, then local→UTC\n",
    "fl = df_flights\n",
    "crs = (\n",
    "    fl\n",
    "    .withColumn(\"CRS_DEP_TIME_str\", sf.lpad(sf.col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\"))\n",
    "    .withColumn(\"dep_hh\", sf.col(\"CRS_DEP_TIME_str\").substr(1, 2).cast(\"int\"))\n",
    "    .withColumn(\"dep_mm\", sf.col(\"CRS_DEP_TIME_str\").substr(3, 2).cast(\"int\"))\n",
    "    .withColumn(\"FL_DATE_str\", sf.col(\"FL_DATE\").cast(\"string\"))\n",
    "    .withColumn(\n",
    "        \"dep_local_ts\",\n",
    "        sf.to_timestamp(\n",
    "            sf.concat_ws(\" \", \"FL_DATE_str\", sf.format_string(\"%02d:%02d:00\", sf.col(\"dep_hh\"), sf.col(\"dep_mm\"))),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display results\n",
    "show_df(crs, 5)\n",
    "show_columns(crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ebd67b-cfd7-4c54-9300-5b66514f40cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>flight_id</th><th>FL_DATE</th><th>ORIGIN</th><th>DEST</th><th>prediction_local_ts</th><th>prediction_utc</th></tr></thead><tbody><tr><td>2020-07-16|UA|342|EWR|FLL</td><td>2020-07-16</td><td>EWR</td><td>FLL</td><td>2020-07-16T16:50:00Z</td><td>2020-07-16T20:50:00Z</td></tr><tr><td>2020-07-15|UA|2232|IAD|FLL</td><td>2020-07-15</td><td>IAD</td><td>FLL</td><td>2020-07-15T15:45:00Z</td><td>2020-07-15T19:45:00Z</td></tr><tr><td>2020-07-15|UA|1087|SFO|IAH</td><td>2020-07-15</td><td>SFO</td><td>IAH</td><td>2020-07-15T08:55:00Z</td><td>2020-07-15T15:55:00Z</td></tr><tr><td>2020-07-15|UA|753|SNA|SFO</td><td>2020-07-15</td><td>SNA</td><td>SFO</td><td>2020-07-15T06:15:00Z</td><td>2020-07-15T13:15:00Z</td></tr><tr><td>2020-07-15|UA|306|DEN|ORD</td><td>2020-07-15</td><td>DEN</td><td>ORD</td><td>2020-07-15T17:45:00Z</td><td>2020-07-15T23:45:00Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2020-07-16|UA|342|EWR|FLL",
         "2020-07-16",
         "EWR",
         "FLL",
         "2020-07-16T16:50:00Z",
         "2020-07-16T20:50:00Z"
        ],
        [
         "2020-07-15|UA|2232|IAD|FLL",
         "2020-07-15",
         "IAD",
         "FLL",
         "2020-07-15T15:45:00Z",
         "2020-07-15T19:45:00Z"
        ],
        [
         "2020-07-15|UA|1087|SFO|IAH",
         "2020-07-15",
         "SFO",
         "IAH",
         "2020-07-15T08:55:00Z",
         "2020-07-15T15:55:00Z"
        ],
        [
         "2020-07-15|UA|753|SNA|SFO",
         "2020-07-15",
         "SNA",
         "SFO",
         "2020-07-15T06:15:00Z",
         "2020-07-15T13:15:00Z"
        ],
        [
         "2020-07-15|UA|306|DEN|ORD",
         "2020-07-15",
         "DEN",
         "ORD",
         "2020-07-15T17:45:00Z",
         "2020-07-15T23:45:00Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "flight_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "prediction_local_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "prediction_utc",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>flight_id</td><td>string</td><td>0.0</td></tr><tr><td>FL_DATE</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN</td><td>string</td><td>0.0</td></tr><tr><td>DEST</td><td>string</td><td>0.0</td></tr><tr><td>prediction_local_ts</td><td>timestamp</td><td>3.1908271632308482E-6</td></tr><tr><td>prediction_utc</td><td>timestamp</td><td>3.1908271632308482E-6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "flight_id",
         "string",
         0.0
        ],
        [
         "FL_DATE",
         "string",
         0.0
        ],
        [
         "ORIGIN",
         "string",
         0.0
        ],
        [
         "DEST",
         "string",
         0.0
        ],
        [
         "prediction_local_ts",
         "timestamp",
         3.1908271632308482E-6
        ],
        [
         "prediction_utc",
         "timestamp",
         3.1908271632308482E-6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31339836\n"
     ]
    }
   ],
   "source": [
    "fl_origin = (\n",
    "    crs.alias(\"f\")\n",
    "    .join(df_airports.alias(\"a\"), sf.col(\"f.ORIGIN\")==sf.col(\"a.iata_code\"), \"left\")\n",
    "    .withColumn(\"airport_timezone\", sf.col(\"a.airport_timezone\"))                     # expose TZ as a plain column\n",
    "    .withColumn(\"prediction_local_ts\", sf.expr(\"dep_local_ts - INTERVAL 2 HOURS\"))    # (equiv to T–2h)\n",
    "    .withColumn(\n",
    "        \"prediction_utc\",\n",
    "        sf.expr(\"to_utc_timestamp(prediction_local_ts, airport_timezone)\")            # use expr() so TZ column works\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"flight_id\",\n",
    "        sf.concat_ws(\"|\",\n",
    "            sf.col(\"FL_DATE\").cast(\"string\"),\n",
    "            sf.col(\"OP_UNIQUE_CARRIER\"),\n",
    "            sf.col(\"OP_CARRIER_FL_NUM\"),\n",
    "            sf.col(\"ORIGIN\"),\n",
    "            sf.col(\"DEST\")\n",
    "        )\n",
    "    )\n",
    "    .select(\"flight_id\",\"FL_DATE\",\"ORIGIN\",\"DEST\",\"prediction_local_ts\",\"prediction_utc\")\n",
    ")\n",
    "\n",
    "# Display results\n",
    "show_df(fl_origin, 5)\n",
    "show_columns(fl_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5718b5-c253-468e-a5c6-f040a9356530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>flight_id</th><th>FL_DATE</th><th>ORIGIN</th><th>DEST</th><th>prediction_local_ts</th><th>prediction_utc</th><th>cand_station</th><th>station_rank</th></tr></thead><tbody><tr><td>2021-07-17|OH|5502|ABE|CLT</td><td>2021-07-17</td><td>ABE</td><td>CLT</td><td>2021-07-17T15:04:00Z</td><td>2021-07-17T19:04:00Z</td><td>72517014737</td><td>1</td></tr><tr><td>2021-07-17|OH|5502|ABE|CLT</td><td>2021-07-17</td><td>ABE</td><td>CLT</td><td>2021-07-17T15:04:00Z</td><td>2021-07-17T19:04:00Z</td><td>72032464753</td><td>2</td></tr><tr><td>2021-07-17|OH|5502|ABE|CLT</td><td>2021-07-17</td><td>ABE</td><td>CLT</td><td>2021-07-17T15:04:00Z</td><td>2021-07-17T19:04:00Z</td><td>72511354786</td><td>3</td></tr><tr><td>2022-03-20|WN|1657|ALB|BWI</td><td>2022-03-20</td><td>ALB</td><td>BWI</td><td>2022-03-20T15:50:00Z</td><td>2022-03-20T19:50:00Z</td><td>72518014735</td><td>1</td></tr><tr><td>2022-03-20|WN|1657|ALB|BWI</td><td>2022-03-20</td><td>ALB</td><td>BWI</td><td>2022-03-20T15:50:00Z</td><td>2022-03-20T19:50:00Z</td><td>74499404741</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2021-07-17|OH|5502|ABE|CLT",
         "2021-07-17",
         "ABE",
         "CLT",
         "2021-07-17T15:04:00Z",
         "2021-07-17T19:04:00Z",
         "72517014737",
         1
        ],
        [
         "2021-07-17|OH|5502|ABE|CLT",
         "2021-07-17",
         "ABE",
         "CLT",
         "2021-07-17T15:04:00Z",
         "2021-07-17T19:04:00Z",
         "72032464753",
         2
        ],
        [
         "2021-07-17|OH|5502|ABE|CLT",
         "2021-07-17",
         "ABE",
         "CLT",
         "2021-07-17T15:04:00Z",
         "2021-07-17T19:04:00Z",
         "72511354786",
         3
        ],
        [
         "2022-03-20|WN|1657|ALB|BWI",
         "2022-03-20",
         "ALB",
         "BWI",
         "2022-03-20T15:50:00Z",
         "2022-03-20T19:50:00Z",
         "72518014735",
         1
        ],
        [
         "2022-03-20|WN|1657|ALB|BWI",
         "2022-03-20",
         "ALB",
         "BWI",
         "2022-03-20T15:50:00Z",
         "2022-03-20T19:50:00Z",
         "74499404741",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "flight_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "prediction_local_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "prediction_utc",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "cand_station",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "station_rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>flight_id</td><td>string</td><td>0.0</td></tr><tr><td>FL_DATE</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN</td><td>string</td><td>0.0</td></tr><tr><td>DEST</td><td>string</td><td>0.0</td></tr><tr><td>prediction_local_ts</td><td>timestamp</td><td>3.1908271632308482E-6</td></tr><tr><td>prediction_utc</td><td>timestamp</td><td>3.1908271632308482E-6</td></tr><tr><td>cand_station</td><td>string</td><td>0.0</td></tr><tr><td>station_rank</td><td>int</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "flight_id",
         "string",
         0.0
        ],
        [
         "FL_DATE",
         "string",
         0.0
        ],
        [
         "ORIGIN",
         "string",
         0.0
        ],
        [
         "DEST",
         "string",
         0.0
        ],
        [
         "prediction_local_ts",
         "timestamp",
         3.1908271632308482E-6
        ],
        [
         "prediction_utc",
         "timestamp",
         3.1908271632308482E-6
        ],
        [
         "cand_station",
         "string",
         0.0
        ],
        [
         "station_rank",
         "int",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 94019508\n"
     ]
    }
   ],
   "source": [
    "# As-of weather (origin): join candidate stations; filter obs_utc ≤ prediction_utc and within 6h; choose latest by rank/time\n",
    "origin_candidates = (\n",
    "    fl_origin.alias(\"f\")\n",
    "    .join(airport_weather_station.alias(\"b\"), sf.col(\"f.ORIGIN\") == sf.col(\"b.iata_code\"), how=\"left\")\n",
    "    .select(\"f.*\", sf.col(\"b.STATION\").alias(\"cand_station\"), sf.col(\"b.rank\").alias(\"station_rank\"))\n",
    ")\n",
    "\n",
    "# Display results\n",
    "show_df(origin_candidates)\n",
    "show_columns(origin_candidates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ad7fc7-22b1-4e4f-9c8b-2bc09f46f588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>flight_id</th><th>FL_DATE</th><th>ORIGIN</th><th>DEST</th><th>prediction_local_ts</th><th>prediction_utc</th><th>cand_station</th><th>station_rank</th><th>cand_station_dis_km</th><th>STATION</th><th>obs_utc</th><th>HourlyDryBulbTemperature</th><th>HourlyDewPointTemperature</th><th>HourlyWetBulbTemperature</th><th>HourlyPrecipitation</th><th>HourlyWindSpeed</th><th>HourlyWindDirection</th><th>HourlyWindGustSpeed</th><th>HourlyVisibility</th><th>HourlyRelativeHumidity</th><th>HourlyStationPressure</th><th>HourlySeaLevelPressure</th><th>HourlyAltimeterSetting</th><th>HourlySkyConditions</th><th>HourlyPresentWeatherType</th></tr></thead><tbody><tr><td>2021-12-20|OH|5353|PNS|CLT</td><td>2021-12-20</td><td>PNS</td><td>CLT</td><td>2021-12-20T03:40:00Z</td><td>2021-12-20T09:40:00Z</td><td>A0002453848</td><td>3</td><td>22.18721781147289</td><td>A0002453848</td><td>2021-12-20T06:53:00Z</td><td>10.0</td><td>3.9</td><td>7.2</td><td>0.0</td><td>2.6</td><td>30.0</td><td>null</td><td>16.093</td><td>66.0</td><td>1017.2</td><td>1021.6</td><td>1021.3</td><td>CLR:00</td><td>null</td></tr><tr><td>2021-12-20|OH|5353|PNS|CLT</td><td>2021-12-20</td><td>PNS</td><td>CLT</td><td>2021-12-20T03:40:00Z</td><td>2021-12-20T09:40:00Z</td><td>A0002453848</td><td>3</td><td>22.18721781147289</td><td>A0002453848</td><td>2021-12-20T04:53:00Z</td><td>10.0</td><td>3.3</td><td>6.9</td><td>0.0</td><td>2.6</td><td>10.0</td><td>null</td><td>16.093</td><td>63.0</td><td>1016.9</td><td>1021.4</td><td>1021.0</td><td>CLR:00</td><td>null</td></tr><tr><td>2021-12-20|OH|5353|PNS|CLT</td><td>2021-12-20</td><td>PNS</td><td>CLT</td><td>2021-12-20T03:40:00Z</td><td>2021-12-20T09:40:00Z</td><td>A0002453848</td><td>3</td><td>22.18721781147289</td><td>A0002453848</td><td>2021-12-20T05:53:00Z</td><td>10.0</td><td>3.9</td><td>7.2</td><td>0.0</td><td>3.6</td><td>350.0</td><td>null</td><td>16.093</td><td>66.0</td><td>1017.2</td><td>1021.6</td><td>1021.3</td><td>CLR:00</td><td>null</td></tr><tr><td>2021-12-20|OH|5353|PNS|CLT</td><td>2021-12-20</td><td>PNS</td><td>CLT</td><td>2021-12-20T03:40:00Z</td><td>2021-12-20T09:40:00Z</td><td>A0002453848</td><td>3</td><td>22.18721781147289</td><td>A0002453848</td><td>2021-12-20T03:53:00Z</td><td>10.0</td><td>3.3</td><td>6.9</td><td>0.0</td><td>3.1</td><td>20.0</td><td>null</td><td>16.093</td><td>63.0</td><td>1016.6</td><td>1021.1</td><td>1020.7</td><td>CLR:00</td><td>null</td></tr><tr><td>2021-12-20|OH|5353|PNS|CLT</td><td>2021-12-20</td><td>PNS</td><td>CLT</td><td>2021-12-20T03:40:00Z</td><td>2021-12-20T09:40:00Z</td><td>A0002453848</td><td>3</td><td>22.18721781147289</td><td>A0002453848</td><td>2021-12-20T07:53:00Z</td><td>10.0</td><td>3.9</td><td>7.2</td><td>0.0</td><td>3.1</td><td>20.0</td><td>null</td><td>16.093</td><td>66.0</td><td>1017.2</td><td>1021.7</td><td>1021.3</td><td>CLR:00</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2021-12-20|OH|5353|PNS|CLT",
         "2021-12-20",
         "PNS",
         "CLT",
         "2021-12-20T03:40:00Z",
         "2021-12-20T09:40:00Z",
         "A0002453848",
         3,
         22.18721781147289,
         "A0002453848",
         "2021-12-20T06:53:00Z",
         10.0,
         3.9,
         7.2,
         "0.0",
         "2.6",
         "30.0",
         null,
         "16.093",
         66.0,
         1017.2,
         1021.6,
         1021.3,
         "CLR:00",
         null
        ],
        [
         "2021-12-20|OH|5353|PNS|CLT",
         "2021-12-20",
         "PNS",
         "CLT",
         "2021-12-20T03:40:00Z",
         "2021-12-20T09:40:00Z",
         "A0002453848",
         3,
         22.18721781147289,
         "A0002453848",
         "2021-12-20T04:53:00Z",
         10.0,
         3.3,
         6.9,
         "0.0",
         "2.6",
         "10.0",
         null,
         "16.093",
         63.0,
         1016.9,
         1021.4,
         1021.0,
         "CLR:00",
         null
        ],
        [
         "2021-12-20|OH|5353|PNS|CLT",
         "2021-12-20",
         "PNS",
         "CLT",
         "2021-12-20T03:40:00Z",
         "2021-12-20T09:40:00Z",
         "A0002453848",
         3,
         22.18721781147289,
         "A0002453848",
         "2021-12-20T05:53:00Z",
         10.0,
         3.9,
         7.2,
         "0.0",
         "3.6",
         "350.0",
         null,
         "16.093",
         66.0,
         1017.2,
         1021.6,
         1021.3,
         "CLR:00",
         null
        ],
        [
         "2021-12-20|OH|5353|PNS|CLT",
         "2021-12-20",
         "PNS",
         "CLT",
         "2021-12-20T03:40:00Z",
         "2021-12-20T09:40:00Z",
         "A0002453848",
         3,
         22.18721781147289,
         "A0002453848",
         "2021-12-20T03:53:00Z",
         10.0,
         3.3,
         6.9,
         "0.0",
         "3.1",
         "20.0",
         null,
         "16.093",
         63.0,
         1016.6,
         1021.1,
         1020.7,
         "CLR:00",
         null
        ],
        [
         "2021-12-20|OH|5353|PNS|CLT",
         "2021-12-20",
         "PNS",
         "CLT",
         "2021-12-20T03:40:00Z",
         "2021-12-20T09:40:00Z",
         "A0002453848",
         3,
         22.18721781147289,
         "A0002453848",
         "2021-12-20T07:53:00Z",
         10.0,
         3.9,
         7.2,
         "0.0",
         "3.1",
         "20.0",
         null,
         "16.093",
         66.0,
         1017.2,
         1021.7,
         1021.3,
         "CLR:00",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "flight_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "prediction_local_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "prediction_utc",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "cand_station",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "station_rank",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "cand_station_dis_km",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "STATION",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "obs_utc",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "HourlyDryBulbTemperature",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlyDewPointTemperature",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWetBulbTemperature",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPrecipitation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindSpeed",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindDirection",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindGustSpeed",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlyVisibility",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HourlyRelativeHumidity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlyStationPressure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlySeaLevelPressure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlyAltimeterSetting",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "HourlySkyConditions",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPresentWeatherType",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column</th><th>Type</th><th>% Null</th></tr></thead><tbody><tr><td>flight_id</td><td>string</td><td>0.0</td></tr><tr><td>FL_DATE</td><td>string</td><td>0.0</td></tr><tr><td>ORIGIN</td><td>string</td><td>0.0</td></tr><tr><td>DEST</td><td>string</td><td>0.0</td></tr><tr><td>prediction_local_ts</td><td>timestamp</td><td>3.785509250275451E-7</td></tr><tr><td>prediction_utc</td><td>timestamp</td><td>3.785509250275451E-7</td></tr><tr><td>cand_station</td><td>string</td><td>0.0</td></tr><tr><td>station_rank</td><td>int</td><td>0.0</td></tr><tr><td>cand_station_dis_km</td><td>double</td><td>0.0</td></tr><tr><td>STATION</td><td>string</td><td>1.0768679804860328</td></tr><tr><td>obs_utc</td><td>timestamp</td><td>1.0768679804860328</td></tr><tr><td>HourlyDryBulbTemperature</td><td>double</td><td>1.576432424839042</td></tr><tr><td>HourlyDewPointTemperature</td><td>double</td><td>1.7169477521691499</td></tr><tr><td>HourlyWetBulbTemperature</td><td>double</td><td>5.358901911186522</td></tr><tr><td>HourlyPrecipitation</td><td>string</td><td>46.48280461697667</td></tr><tr><td>HourlyWindSpeed</td><td>string</td><td>2.026427518314319</td></tr><tr><td>HourlyWindDirection</td><td>string</td><td>16.281274527260813</td></tr><tr><td>HourlyWindGustSpeed</td><td>double</td><td>84.47185493557068</td></tr><tr><td>HourlyVisibility</td><td>string</td><td>6.538925397293487</td></tr><tr><td>HourlyRelativeHumidity</td><td>double</td><td>1.7509117197135462</td></tr><tr><td>HourlyStationPressure</td><td>double</td><td>4.871820562137221</td></tr><tr><td>HourlySeaLevelPressure</td><td>double</td><td>40.46821464854994</td></tr><tr><td>HourlyAltimeterSetting</td><td>double</td><td>6.2153251769102225</td></tr><tr><td>HourlySkyConditions</td><td>string</td><td>9.0442024224251</td></tr><tr><td>HourlyPresentWeatherType</td><td>string</td><td>83.64686855759955</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "flight_id",
         "string",
         0.0
        ],
        [
         "FL_DATE",
         "string",
         0.0
        ],
        [
         "ORIGIN",
         "string",
         0.0
        ],
        [
         "DEST",
         "string",
         0.0
        ],
        [
         "prediction_local_ts",
         "timestamp",
         3.785509250275451E-7
        ],
        [
         "prediction_utc",
         "timestamp",
         3.785509250275451E-7
        ],
        [
         "cand_station",
         "string",
         0.0
        ],
        [
         "station_rank",
         "int",
         0.0
        ],
        [
         "cand_station_dis_km",
         "double",
         0.0
        ],
        [
         "STATION",
         "string",
         1.0768679804860328
        ],
        [
         "obs_utc",
         "timestamp",
         1.0768679804860328
        ],
        [
         "HourlyDryBulbTemperature",
         "double",
         1.576432424839042
        ],
        [
         "HourlyDewPointTemperature",
         "double",
         1.7169477521691499
        ],
        [
         "HourlyWetBulbTemperature",
         "double",
         5.358901911186522
        ],
        [
         "HourlyPrecipitation",
         "string",
         46.48280461697667
        ],
        [
         "HourlyWindSpeed",
         "string",
         2.026427518314319
        ],
        [
         "HourlyWindDirection",
         "string",
         16.281274527260813
        ],
        [
         "HourlyWindGustSpeed",
         "double",
         84.47185493557068
        ],
        [
         "HourlyVisibility",
         "string",
         6.538925397293487
        ],
        [
         "HourlyRelativeHumidity",
         "double",
         1.7509117197135462
        ],
        [
         "HourlyStationPressure",
         "double",
         4.871820562137221
        ],
        [
         "HourlySeaLevelPressure",
         "double",
         40.46821464854994
        ],
        [
         "HourlyAltimeterSetting",
         "double",
         6.2153251769102225
        ],
        [
         "HourlySkyConditions",
         "string",
         9.0442024224251
        ],
        [
         "HourlyPresentWeatherType",
         "string",
         83.64686855759955
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "% Null",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 792495752\n"
     ]
    }
   ],
   "source": [
    "# Restrict to weather rows 'as-of' prediction_utc (no future) and within a bounded lookback window \n",
    "weather_required = [\n",
    "    \"HourlyDryBulbTemperature\",\"HourlyDewPointTemperature\",\"HourlyWetBulbTemperature\",\n",
    "    \"HourlyPrecipitation\",\"HourlyWindSpeed\",\"HourlyWindDirection\",\"HourlyWindGustSpeed\",\n",
    "    \"HourlyVisibility\",\"HourlyRelativeHumidity\",\"HourlyStationPressure\",\"HourlySeaLevelPressure\",\n",
    "    \"HourlyAltimeterSetting\",\"HourlySkyConditions\",\"HourlyPresentWeatherType\"\n",
    "]\n",
    "wx_present = [c for c in weather_required if c in weather_best.columns]\n",
    "weather = weather_best.select(\n",
    "    \"STATION\",\n",
    "    sf.col(\"obs_utc\"),\n",
    "    *[sf.col(c) for c in wx_present]\n",
    ")\n",
    "\n",
    "# weather_cols = [\"STATION\", sf.col(\"DATE\").cast(\"timestamp\").alias(\"obs_utc\")] + [sf.col(c) for c in wx_present]\n",
    "# weather = df_weather.select(*weather_cols)\n",
    "\n",
    "# In origin_candidates, also carry the distance so we can publish origin_station_dis\n",
    "origin_candidates = (\n",
    "    fl_origin.alias(\"f\")\n",
    "    .join(airport_weather_station.alias(\"b\"), sf.col(\"f.ORIGIN\") == sf.col(\"b.iata_code\"), how=\"left\")\n",
    "    .select(\"f.*\",\n",
    "            sf.col(\"b.STATION\").alias(\"cand_station\"),\n",
    "            sf.col(\"b.rank\").alias(\"station_rank\"),\n",
    "            sf.col(\"b.dist_km\").alias(\"cand_station_dis_km\"))\n",
    ")\n",
    "\n",
    "# 6-hour lookback \n",
    "weather_join = (\n",
    "    origin_candidates.alias(\"x\")\n",
    "    .join(\n",
    "        weather.alias(\"w\"),\n",
    "        on=[\n",
    "            sf.col(\"w.STATION\") == sf.col(\"x.cand_station\"),\n",
    "            sf.col(\"w.obs_utc\") <= sf.col(\"x.prediction_utc\"),\n",
    "            sf.col(\"w.obs_utc\") >= sf.expr(\"timestampadd(HOUR, -6, x.prediction_utc)\")\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display results\n",
    "show_df(weather_join, 5)\n",
    "show_columns(weather_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17668c7a-5399-4701-9505-208ad9aee53a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Station selection window, prefer lower station_rank (1, then 2, then 3), and the latest obs_utc within the window\n",
    "window = W.partitionBy(\"flight_id\").orderBy(sf.col(\"station_rank\").asc(), sf.col(\"obs_utc\").desc())\n",
    "origin_asof = (\n",
    "    weather_join\n",
    "    .withColumn(\"rn\", sf.row_number().over(window))\n",
    "    .filter(sf.col(\"rn\") == 1)\n",
    "    .withColumn(\"asof_minutes\", sf.floor((sf.unix_timestamp(\"prediction_utc\") - sf.unix_timestamp(\"obs_utc\"))/60.0))\n",
    "    .select(\n",
    "        \"flight_id\",\"ORIGIN\",\"prediction_utc\",\n",
    "        sf.col(\"cand_station\").alias(\"origin_station_id\"),\n",
    "        sf.col(\"cand_station_dis_km\").alias(\"origin_station_dis\"),\n",
    "        sf.col(\"obs_utc\").alias(\"origin_obs_utc\"),\n",
    "        \"asof_minutes\",\n",
    "        *wx_present,\n",
    "        \"station_rank\"\n",
    "    )\n",
    ")\n",
    "# Display results\n",
    "# show_df(origin_asof, 5)\n",
    "# show_columns(origin_asof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7626ebf9-72f5-401a-bc54-cca0efc30d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Origin station lat/lon\n",
    "origin_asof_enriched = (\n",
    "    origin_asof.alias(\"o\")\n",
    "    .join(\n",
    "        df_weather_station.select(\n",
    "            sf.col(\"station_id\").alias(\"STATION\"),\n",
    "            sf.col(\"lat\").alias(\"origin_station_lat\"),\n",
    "            sf.col(\"lon\").alias(\"origin_station_lon\")\n",
    "        ).alias(\"s\"),\n",
    "        sf.col(\"o.origin_station_id\")==sf.col(\"s.STATION\"),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "need_from_w = [\"flight_id\",\"prediction_utc\",\"origin_obs_utc\",\"asof_minutes\",\n",
    "               \"origin_station_id\",\"origin_station_dis\",\"origin_station_lat\",\"origin_station_lon\"] + wx_present\n",
    "origin_asof_enriched = origin_asof_enriched.select(*[c for c in need_from_w if c in origin_asof_enriched.columns])\n",
    "\n",
    "\n",
    "# Airport lat/lon (origin & dest)\n",
    "air_min = df_airports.select(\n",
    "    \"iata_code\",\n",
    "    sf.col(\"lat\").alias(\"airport_lat\"),\n",
    "    sf.col(\"lon\").alias(\"airport_lon\"),\n",
    "    sf.col(\"airport_type\")\n",
    ")\n",
    "origin_air_geo = air_min.select(\n",
    "    sf.col(\"iata_code\").alias(\"ORIGIN\"),\n",
    "    sf.col(\"airport_lat\").alias(\"origin_airport_lat\"),\n",
    "    sf.col(\"airport_lon\").alias(\"origin_airport_lon\"),\n",
    "    sf.col(\"airport_type\").alias(\"origin_type\")   \n",
    ")\n",
    "dest_air_geo = air_min.select(\n",
    "    sf.col(\"iata_code\").alias(\"DEST\"),\n",
    "    sf.col(\"airport_lat\").alias(\"dest_airport_lat\"),\n",
    "    sf.col(\"airport_lon\").alias(\"dest_airport_lon\"),\n",
    "    sf.col(\"airport_type\").alias(\"dest_type\")     # <-- new\n",
    ")\n",
    "\n",
    "# Dest station (rank-1) for location helpers (no dest weather to avoid leakage)\n",
    "dest_rank1 = (\n",
    "    airport_weather_station\n",
    "    .filter(sf.col(\"rank\")==1)\n",
    "    .select(\n",
    "        sf.col(\"iata_code\").alias(\"DEST\"),\n",
    "        sf.col(\"STATION\").alias(\"dest_station_id\"),\n",
    "        sf.col(\"dist_km\").alias(\"dest_station_dis\")\n",
    "    )\n",
    ")\n",
    "dest_station_geo = (\n",
    "    dest_rank1.alias(\"d\")\n",
    "    .join(\n",
    "        df_weather_station.select(\n",
    "            sf.col(\"station_id\").alias(\"STATION\"),\n",
    "            sf.col(\"lat\").alias(\"dest_station_lat\"),\n",
    "            sf.col(\"lon\").alias(\"dest_station_lon\")\n",
    "        ).alias(\"s\"),\n",
    "        sf.col(\"d.dest_station_id\")==sf.col(\"s.STATION\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\"DEST\",\"dest_station_id\",\"dest_station_dis\",\"dest_station_lat\",\"dest_station_lon\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee32aaa-3eeb-4721-a3c3-e833c26c8e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rebuild a stable flight_id on the raw flights for the final join\n",
    "flights_keyed = (\n",
    "    df_flights\n",
    "    .withColumn(\n",
    "        \"flight_id\",\n",
    "        sf.concat_ws(\"|\",\n",
    "            sf.col(\"FL_DATE\").cast(\"string\"),\n",
    "            sf.col(\"OP_UNIQUE_CARRIER\"),\n",
    "            sf.col(\"OP_CARRIER_FL_NUM\"),\n",
    "            sf.col(\"ORIGIN\"),\n",
    "            sf.col(\"DEST\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display results\n",
    "# show_df(flights_keyed, 5)\n",
    "# show_columns(flights_keyed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f895ce0-b2ff-4613-aeed-9b82b998a07a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3740109152944026>, line 68\u001B[0m\n",
       "\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# Persist for the team using the configured output path\u001B[39;00m\n",
       "\u001B[1;32m     63\u001B[0m out_path \u001B[38;5;241m=\u001B[39m cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoined_output\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m     64\u001B[0m (final_curated\n",
       "\u001B[1;32m     65\u001B[0m  \u001B[38;5;241m.\u001B[39mwrite\n",
       "\u001B[1;32m     66\u001B[0m  \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     67\u001B[0m  \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 68\u001B[0m  \u001B[38;5;241m.\u001B[39msave(out_path))\n",
       "\u001B[1;32m     70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrote joined dataset to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1734\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1733\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1734\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    271\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3027.save.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 1520 (mapPartitionsInternal at PhotonExec.scala:643) has failed the maximum allowable number of times: 4. Most recent failure reason:\n",
       "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 364 partition 176\n",
       "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:2271)\n",
       "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:2191)\n",
       "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:2190)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:2190)\n",
       "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1800)\n",
       "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1734)\n",
       "\tat com.databricks.photon.ShuffledBlockRDD.x$3$lzycompute$1(ShuffledBlockRDD.scala:188)\n",
       "\tat com.databricks.photon.ShuffledBlockRDD.x$3$1(ShuffledBlockRDD.scala:180)\n",
       "\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$lzycompute$1(ShuffledBlockRDD.scala:180)\n",
       "\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$1(ShuffledBlockRDD.scala:180)\n",
       "\tat com.databricks.photon.ShuffledBlockRDD.createIterator$1(ShuffledBlockRDD.scala:245)\n",
       "\tat com.databricks.photon.ShuffledBlockRDD.compute(ShuffledBlockRDD.scala:268)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n",
       "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:104)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:83)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:649)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n",
       "\t\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\t\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\t\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n",
       "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n",
       "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n",
       "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n",
       "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n",
       "\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o3027.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 1520 (mapPartitionsInternal at PhotonExec.scala:643) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 364 partition 176\n\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:2271)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:2191)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:2190)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:2190)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1800)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1734)\n\tat com.databricks.photon.ShuffledBlockRDD.x$3$lzycompute$1(ShuffledBlockRDD.scala:188)\n\tat com.databricks.photon.ShuffledBlockRDD.x$3$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$lzycompute$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.createIterator$1(ShuffledBlockRDD.scala:245)\n\tat com.databricks.photon.ShuffledBlockRDD.compute(ShuffledBlockRDD.scala:268)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:104)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:83)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:649)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n\t\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\t\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\t\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o3027.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 1520 (mapPartitionsInternal at PhotonExec.scala:643) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 364 partition 176\n\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:2271)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:2191)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:2190)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:2190)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1800)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1734)\n\tat com.databricks.photon.ShuffledBlockRDD.x$3$lzycompute$1(ShuffledBlockRDD.scala:188)\n\tat com.databricks.photon.ShuffledBlockRDD.x$3$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$lzycompute$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.createIterator$1(ShuffledBlockRDD.scala:245)\n\tat com.databricks.photon.ShuffledBlockRDD.compute(ShuffledBlockRDD.scala:268)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:104)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:83)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:649)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n\t\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\t\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\t\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-3740109152944026>, line 68\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# Persist for the team using the configured output path\u001B[39;00m\n\u001B[1;32m     63\u001B[0m out_path \u001B[38;5;241m=\u001B[39m cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoined_output\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     64\u001B[0m (final_curated\n\u001B[1;32m     65\u001B[0m  \u001B[38;5;241m.\u001B[39mwrite\n\u001B[1;32m     66\u001B[0m  \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     67\u001B[0m  \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 68\u001B[0m  \u001B[38;5;241m.\u001B[39msave(out_path))\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrote joined dataset to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1734\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1734\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    271\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3027.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 1520 (mapPartitionsInternal at PhotonExec.scala:643) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 364 partition 176\n\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:2271)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:2191)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:2190)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:2190)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1800)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1734)\n\tat com.databricks.photon.ShuffledBlockRDD.x$3$lzycompute$1(ShuffledBlockRDD.scala:188)\n\tat com.databricks.photon.ShuffledBlockRDD.x$3$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$lzycompute$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.blocksByAddress$1(ShuffledBlockRDD.scala:180)\n\tat com.databricks.photon.ShuffledBlockRDD.createIterator$1(ShuffledBlockRDD.scala:245)\n\tat com.databricks.photon.ShuffledBlockRDD.compute(ShuffledBlockRDD.scala:268)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:104)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat com.databricks.photon.PhotonInputRDD.compute(photonPhysicalOperators.scala:203)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:384)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:83)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:649)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4708)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4706)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4618)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4605)\n\t\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\t\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\t\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4605)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4594)\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$1(DAGScheduler.scala:3812)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:3395)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4966)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4870)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4869)\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4855)\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assemble final dataset: one row per flight\n",
    "final_joined = (\n",
    "    flights_keyed.alias(\"f\")\n",
    "    .join(origin_asof_enriched.alias(\"w\"), \"flight_id\", \"left\")\n",
    "    .join(origin_air_geo, [\"ORIGIN\"], \"left\")\n",
    "    .join(dest_air_geo,   [\"DEST\"],   \"left\")\n",
    "    .join(dest_station_geo, [\"DEST\"], \"left\")\n",
    ")\n",
    "\n",
    "# Column groups\n",
    "model_inputs = [\n",
    "    # Flight & schedule\n",
    "    \"FL_DATE\",\"YEAR\",\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"OP_UNIQUE_CARRIER\",\"OP_CARRIER\",\"OP_CARRIER_FL_NUM\",\"TAIL_NUM\",\n",
    "    \"CRS_DEP_TIME\",\"CRS_ARR_TIME\",\"CRS_ELAPSED_TIME\",\n",
    "    \"ORIGIN\",\"ORIGIN_AIRPORT_ID\",\"ORIGIN_CITY_NAME\",\"ORIGIN_STATE_ABR\",\n",
    "    \"DEST\",\"DEST_AIRPORT_ID\",\"DEST_CITY_NAME\",\"DEST_STATE_ABR\",\n",
    "    \"DISTANCE\",\"DISTANCE_GROUP\",\n",
    "    # Origin weather (as-of T–2h)\n",
    "] + wx_present + [\n",
    "    # Location helpers\n",
    "    \"origin_station_lat\",\"origin_station_lon\",\"origin_airport_lat\",\"origin_airport_lon\",\n",
    "    \"dest_station_lat\",\"dest_station_lon\",\"dest_airport_lat\",\"dest_airport_lon\",\n",
    "    \"origin_station_dis\",\"dest_station_dis\", \"origin_type\",\"dest_type\"\n",
    "]\n",
    "\n",
    "labels_eval = [\"DEP_DEL15\",\"DEP_DELAY\",\"ARR_DEL15\",\"ARR_DELAY\"]\n",
    "post_flight = [\n",
    "    \"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\",\n",
    "    \"DEP_TIME\",\"ARR_TIME\",\"TAXI_OUT\",\"TAXI_IN\",\"WHEELS_OFF\",\"WHEELS_ON\",\"ACTUAL_ELAPSED_TIME\",\"AIR_TIME\"\n",
    "]\n",
    "flags = [\"CANCELLED\",\"CANCELLATION_CODE\",\"DIVERTED\"]\n",
    "\n",
    "provenance = [\"flight_id\",\"prediction_utc\",\"origin_obs_utc\",\"asof_minutes\",\"origin_station_id\",\"dest_station_id\"]\n",
    "\n",
    "# Keep only columns that exist (different weather slices may miss some)\n",
    "def present(cols, df_cols): \n",
    "    s = set(df_cols); \n",
    "    return [c for c in cols if c in s]\n",
    "\n",
    "keep = provenance \\\n",
    "     + present(model_inputs, final_joined.columns) \\\n",
    "     + present(labels_eval, final_joined.columns) \\\n",
    "     + present(post_flight, final_joined.columns) \\\n",
    "     + present(flags, final_joined.columns)\n",
    "\n",
    "final_curated = final_joined.select(*keep)\n",
    "\n",
    "\n",
    "# Optional: limit to an end date if configured\n",
    "max_year = cfg.get(\"max_year\")\n",
    "max_month = cfg.get(\"max_month\")\n",
    "\n",
    "if max_year is not None:\n",
    "    if max_month is None:\n",
    "        max_month = 12\n",
    "    final_curated = final_curated.filter(\n",
    "        (sf.col(\"YEAR\") < max_year)\n",
    "        | ((sf.col(\"YEAR\") == max_year) & (sf.col(\"MONTH\") <= max_month))\n",
    "    )\n",
    "\n",
    "# Persist for the team using the configured output path\n",
    "out_path = cfg[\"joined_output\"]\n",
    "(final_curated\n",
    " .write\n",
    " .format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .save(out_path))\n",
    "\n",
    "print(f\"Wrote joined dataset to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ef9ad6-9518-4666-a293-80c71e8dc387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Repartition the DataFrame to reduce shuffle pressure\n",
    "final_curated = final_curated.repartition(200)\n",
    "\n",
    "# 2. Write with overwrite mode as before\n",
    "final_curated.write.format(\"parquet\").mode(\"overwrite\").save(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0ea9f9-03b3-4d7c-9864-44fa8e8c0f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1Y Join from 5Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4ab3d1-d466-41cc-8f1a-76709b9aed64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build a 1Y (2015) subset only when running the 5Y config\n",
    "if DATA_SLICE == \"5Y\":\n",
    "    df_joined_5Y = spark.read.parquet(DATA_CONFIG[\"5Y\"][\"joined_output\"])\n",
    "    print(\"Source row count (5Y):\", df_joined_5Y.count())\n",
    "\n",
    "    df_2015 = df_joined_5Y.filter(sf.col(\"YEAR\") == 2015)\n",
    "    print(\"Destination row count (1Y 2015):\", df_2015.count())\n",
    "\n",
    "    df_2015.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_4/JOINED_1Y_2015.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0446170-f3c8-42d4-968f-e0e7a878e779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba7230a-3049-41d4-a8c0-f0999e488ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job end:   2025-12-15T07:26:37.315842\nRuntime:   47767.7 seconds (796.13 minutes)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:516: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Expected bytes, got a 'float' object\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warn(msg)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>metric</th><th>value</th></tr></thead><tbody><tr><td>data_slice</td><td>FUTURE</td></tr><tr><td>job_start</td><td>2025-12-14T18:10:29.575959</td></tr><tr><td>job_end</td><td>2025-12-15T07:26:37.315842</td></tr><tr><td>runtime_minutes</td><td>796.13</td></tr><tr><td>row_count</td><td>31339836</td></tr><tr><td>feature_count</td><td>75</td></tr><tr><td>data_start_date</td><td>2020-01-01</td></tr><tr><td>data_end_date</td><td>2024-12-31</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "data_slice",
         "FUTURE"
        ],
        [
         "job_start",
         "2025-12-14T18:10:29.575959"
        ],
        [
         "job_end",
         "2025-12-15T07:26:37.315842"
        ],
        [
         "runtime_minutes",
         "796.13"
        ],
        [
         "row_count",
         "31339836"
        ],
        [
         "feature_count",
         "75"
        ],
        [
         "data_start_date",
         "2020-01-01"
        ],
        [
         "data_end_date",
         "2024-12-31"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "metric",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>role</th><th>name</th><th>path</th><th>size_mb</th></tr></thead><tbody><tr><td>input</td><td>flights</td><td>dbfs:/student-groups/Group_4_4/future_joins/BTS_OnTime/parquet_airlines_data_2020_2024_std.parquet</td><td>975.11</td></tr><tr><td>input</td><td>weather</td><td>dbfs:/student-groups/Group_4_4/future_joins/NOAA/LCDv2_weather_data_2020_2024_std.parquet</td><td>21917.69</td></tr><tr><td>output</td><td>joined_output</td><td>dbfs:/student-groups/Group_4_4/JOINED_FUTURE.parquet</td><td>2489.98</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "input",
         "flights",
         "dbfs:/student-groups/Group_4_4/future_joins/BTS_OnTime/parquet_airlines_data_2020_2024_std.parquet",
         975.11
        ],
        [
         "input",
         "weather",
         "dbfs:/student-groups/Group_4_4/future_joins/NOAA/LCDv2_weather_data_2020_2024_std.parquet",
         21917.69
        ],
        [
         "output",
         "joined_output",
         "dbfs:/student-groups/Group_4_4/JOINED_FUTURE.parquet",
         2489.98
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "role",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size_mb",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN STATS: summary of this join job\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "job_end = datetime.now()\n",
    "runtime_seconds = (job_end - job_start).total_seconds()\n",
    "runtime_minutes = round(runtime_seconds / 60, 2)\n",
    "\n",
    "print(f\"Job end:   {job_end.isoformat()}\")\n",
    "print(f\"Runtime:   {runtime_seconds:.1f} seconds ({runtime_minutes} minutes)\")\n",
    "\n",
    "# --- Dataset-level stats (final_curated) ---\n",
    "\n",
    "# Number of samples and features\n",
    "row_count = final_curated.count()              # triggers a Spark job\n",
    "feature_count = len(final_curated.columns)\n",
    "\n",
    "# Date coverage (uses FL_DATE or change to YEAR/MONTH if you prefer)\n",
    "date_bounds = final_curated.agg(\n",
    "    sf.min(\"FL_DATE\").alias(\"min_date\"),\n",
    "    sf.max(\"FL_DATE\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "data_start_date = date_bounds[\"min_date\"]\n",
    "data_end_date = date_bounds[\"max_date\"]\n",
    "\n",
    "# --- File sizes (inputs/outputs) ---\n",
    "\n",
    "INPUT_PATHS = {\n",
    "    \"flights\": cfg[\"flights_path\"],\n",
    "    \"weather\": cfg[\"weather_path\"],\n",
    "    # You can add more here if you want:\n",
    "    # \"airport_codes\": \"dbfs:/path/to/airport-codes.csv\",\n",
    "    # \"airports_with_tz\": \"dbfs:/path/to/airports.csv\",\n",
    "    # \"stations\": \"dbfs:/path/to/stations_with_neighbors.parquet\",\n",
    "}\n",
    "\n",
    "OUTPUT_PATHS = {\n",
    "    \"joined_output\": out_path,   # out_path was used when writing final_curated\n",
    "}\n",
    "\n",
    "file_rows = []\n",
    "\n",
    "for name, path in INPUT_PATHS.items():\n",
    "    size_bytes = get_dir_size(path)\n",
    "    file_rows.append({\n",
    "        \"role\": \"input\",\n",
    "        \"name\": name,\n",
    "        \"path\": path,\n",
    "        \"size_mb\": round(size_bytes / (1024 * 1024), 2),\n",
    "    })\n",
    "\n",
    "for name, path in OUTPUT_PATHS.items():\n",
    "    size_bytes = get_dir_size(path)\n",
    "    file_rows.append({\n",
    "        \"role\": \"output\",\n",
    "        \"name\": name,\n",
    "        \"path\": path,\n",
    "        \"size_mb\": round(size_bytes / (1024 * 1024), 2),\n",
    "    })\n",
    "\n",
    "files_df = pd.DataFrame(file_rows)\n",
    "\n",
    "# --- High-level run summary ---\n",
    "\n",
    "summary_rows = [\n",
    "    {\"metric\": \"data_slice\",        \"value\": DATA_SLICE},\n",
    "    {\"metric\": \"job_start\",         \"value\": job_start.isoformat()},\n",
    "    {\"metric\": \"job_end\",           \"value\": job_end.isoformat()},\n",
    "    {\"metric\": \"runtime_minutes\",   \"value\": runtime_minutes},\n",
    "    {\"metric\": \"row_count\",         \"value\": row_count},\n",
    "    {\"metric\": \"feature_count\",     \"value\": feature_count},\n",
    "    {\"metric\": \"data_start_date\",   \"value\": str(data_start_date)},\n",
    "    {\"metric\": \"data_end_date\",     \"value\": str(data_end_date)},\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "display(summary_df)\n",
    "display(files_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707da052-147c-464f-a605-fc466f94a552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a8f7dc-9c49-4721-ac76-6922f5b38a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_check = spark.read.parquet(out_path)\n",
    "display(df_check.limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2020-2024 Database Custom Joins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}