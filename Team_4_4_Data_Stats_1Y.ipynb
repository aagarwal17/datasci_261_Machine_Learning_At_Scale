{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24f7a47-9bde-43ef-a901-9e13e9ff6c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nFLIGHT DELAY PREDICTION - PRESENTATION STATISTICS\n================================================================================\nGenerated: 2025-11-19 04:08:26\n================================================================================\n\n================================================================================\n1. RAW OTPW DATA (12 Months)\n================================================================================\nTrying: dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M/OTPW_12M_2019.csv.gz\n  Not found, trying next path...\nTrying: dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M/OTPW_12M_2015.csv.gz\n✓ Successfully loaded 2015 data\nCounting rows (this may take 1-2 minutes)...\n✓ Rows: 5,811,854\n✓ Columns: 216\nCalculating delay statistics...\n✓ Delayed flights: 1,055,735 (18.2%)\n\nChecking data quality (sample columns)...\n✓ Nulls/empty in key columns: DEP_DEL15: 1.5%\n✓ Raw data statistics collected\n\n================================================================================\n2. JOINED DATA (Airline + Weather)\n================================================================================\n✓ Rows: 7,422,037\n✓ Columns: 75\n✓ Delayed flights: 1,362,164 (18.4%)\n✓ Average null % (sampled cols): 0.1%\n\n================================================================================\n3. CLEANED DATA (After Cleaning & Imputation)\n================================================================================\n✓ Rows: 7,258,941\n✓ Columns: 75\n✓ Rows removed: 163,096 (2.2%)\n✓ Columns removed: 0\n✓ Delayed flights: 1,351,453 (18.6%)\n✓ Class balance: 81.4% on-time, 18.6% delayed\n✓ No nulls in critical columns\n✓ Unique ORIGIN: 357\n✓ Unique DEST: 359\n✓ Unique OP_UNIQUE_CARRIER: 17\n\n================================================================================\n4. FEATURE ENGINEERED DATA (Intermediate)\n================================================================================\n✓ Rows: 7,258,941\n✓ Columns: 124\n✓ Features added: 49\n✓ Engineered features present: 11/13\n\n  Present engineered features:\n    • departure_hour\n    • departure_month\n    • departure_dayofweek\n    • is_weekend\n    • is_peak_hour\n    • season\n    • total_flights_per_origin_day\n    • rolling_origin_num_flights_24h\n    ... and 3 more\n\n✓ Final delayed flights: 1,351,453 (18.6%)\n\n================================================================================\n5. FINAL FEATURE ENGINEERED DATA (Ready for Modeling)\n================================================================================\n✓ Rows: 7,258,941\n✓ Columns: 104\n✓ Columns removed in final cleaning: 20\n✓ Final delayed flights: 1,351,453 (18.6%)\n✓ Final class balance: 81.4% on-time, 18.6% delayed\n✓ No nulls in critical columns (ready for modeling)\n\n================================================================================\nPRESENTATION SUMMARY STATISTICS\n================================================================================\n\n\uD83D\uDCCA DATA PIPELINE PROGRESSION:\n--------------------------------------------------------------------------------\n\nRaw OTPW Data (12M):\n  • Rows: 5,811,854\n  • Columns: 216\n  • Delay Rate: 18.2%\n\nJoined Data (Airline + Weather):\n  • Rows: 7,422,037\n  • Columns: 75\n  • Delay Rate: 18.4%\n\nCleaned Data:\n  • Rows: 7,258,941\n  • Columns: 75\n  • Delay Rate: 18.6%\n\nFeature Engineered (Intermediate):\n  • Rows: 7,258,941\n  • Columns: 124\n  • Delay Rate: 18.6%\n\nFinal Feature Set (Ready for Modeling):\n  • Rows: 7,258,941\n  • Columns: 104\n  • Delay Rate: 18.6%\n\n================================================================================\n\n\uD83D\uDCC8 KEY METRICS FOR TALK TRACK:\n--------------------------------------------------------------------------------\n\n1. DATA SCALE:\n   • Started with: 5,811,854 records (RAW data)\n   • Final dataset: 7,258,941 records\n   • Data reduction: -24.9% (cleaned invalid/cancelled flights)\n\n2. CLASS BALANCE:\n   • On-time flights: 81.4%\n   • Delayed flights: 18.6%\n   • Imbalance ratio: 4.37:1 (on-time:delayed)\n   • Note: Imbalanced dataset requiring special handling (SMOTE, class weights, etc.)\n\n3. FEATURE ENGINEERING:\n   • Original columns (joined): 75\n   • Features added/engineered: 29\n   • Final feature count: 104\n   • Intermediate feature count: 124\n   • Features removed in final cleaning: 20\n\n4. DATA QUALITY:\n   • Rows removed during cleaning: 163,096 (2.2%)\n   • Nulls after imputation: 0\n   • Data integrity: High (cancelled/diverted flights removed, nulls imputed)\n\n5. DATASET CHARACTERISTICS:\n   • Time period: 12 months (2015)\n   • Data sources: 2 (Bureau of Transportation Statistics + NOAA Weather)\n   • Prediction task: Binary classification (delay ≥15 min)\n   • Target variable: DEP_DEL15 (1 = delayed, 0 = on-time)\n\n================================================================================\nSAVING STATISTICS\n================================================================================\n\n\n   Stage      Rows  Columns Delay_Rate_% Rows_Removed\n     Raw 5,811,854      216         18.2          NaN\n  Joined 7,422,037       75         18.4          NaN\n Cleaned 7,258,941       75         18.6      163,096\nFeatures 7,258,941      124         18.6          NaN\n   Final 7,258,941      104         18.6          NaN\n\n✓ Statistics saved to: /dbfs/student-groups/Group_4_4/presentation_statistics.csv\n\n================================================================================\nSTATISTICS COLLECTION COMPLETE!\n================================================================================\n\n\uD83D\uDCA1 Access detailed stats via the 'stats' dictionary\n   Example: stats['features']['delay_rate']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Statistics Collection for Flight Delay Prediction Project Presentation\n",
    "Collects key metrics from raw, joined, cleaned, and feature-engineered datasets\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, countDistinct, avg, stddev\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session (if not already active)\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.appName(\"PresentationStats\").getOrCreate()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FLIGHT DELAY PREDICTION - PRESENTATION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define file paths based on actual data files\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "RAW_DATA_BASE = \"dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M\"\n",
    "\n",
    "# Raw OTPW data path - try 2019 first to match joined data, fallback to 2015\n",
    "RAW_DATA_PATHS = [\n",
    "    f\"{RAW_DATA_BASE}/OTPW_12M_2019.csv.gz\",  # Try 2019 first\n",
    "    f\"{RAW_DATA_BASE}/OTPW_12M_2015.csv.gz\",  # Fallback to 2015\n",
    "]\n",
    "\n",
    "# Actual file paths from your directory\n",
    "JOINED_1Y_PATH = f\"{BASE_DIR}/JOINED_1Y_2019.parquet\"\n",
    "CLEANED_1Y_PATH = f\"{BASE_DIR}/joined_1Y_clean_imputed.parquet\"\n",
    "FEATURE_ENG_PATH = f\"{BASE_DIR}/joined_1Y_feat.parquet\"\n",
    "FINAL_FEATURE_PATH = f\"{BASE_DIR}/joined_1Y_final_feature_clean.parquet\"\n",
    "\n",
    "# Dictionary to store all statistics\n",
    "stats = {}\n",
    "\n",
    "# ============================================================================\n",
    "# 1. RAW OTPW DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. RAW OTPW DATA (12 Months)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Raw data is stored as compressed CSV in nested folders\n",
    "    # Try different years\n",
    "    df_raw = None\n",
    "    raw_data_year = None\n",
    "    \n",
    "    for raw_path in RAW_DATA_PATHS:\n",
    "        try:\n",
    "            print(f\"Trying: {raw_path}\")\n",
    "            df_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(raw_path)\n",
    "            # Extract year from path\n",
    "            if \"2019\" in raw_path:\n",
    "                raw_data_year = \"2019\"\n",
    "            elif \"2015\" in raw_path:\n",
    "                raw_data_year = \"2015\"\n",
    "            print(f\"✓ Successfully loaded {raw_data_year} data\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  Not found, trying next path...\")\n",
    "            continue\n",
    "    \n",
    "    if df_raw is None:\n",
    "        raise Exception(\"Could not find raw OTPW data in any expected location\")\n",
    "    \n",
    "    df_raw.cache()\n",
    "    \n",
    "    print(\"Counting rows (this may take 1-2 minutes)...\")\n",
    "    raw_rows = df_raw.count()\n",
    "    raw_cols = len(df_raw.columns)\n",
    "    \n",
    "    stats['raw'] = {\n",
    "        'rows': raw_rows,\n",
    "        'columns': raw_cols,\n",
    "        'description': f'Raw OTPW data (12 months of {raw_data_year})',\n",
    "        'year': raw_data_year\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Rows: {raw_rows:,}\")\n",
    "    print(f\"✓ Columns: {raw_cols}\")\n",
    "    \n",
    "    # Check if this has basic flight data\n",
    "    # Note: CSV data might have all string types, so we need to check carefully\n",
    "    if 'DEP_DEL15' in df_raw.columns:\n",
    "        print(\"Calculating delay statistics...\")\n",
    "        # Cast to integer for proper counting (handles both string \"1\"/\"0\" and actual ints)\n",
    "        delay_count = df_raw.filter(col('DEP_DEL15').cast('int') == 1).count()\n",
    "        delay_rate = (delay_count / raw_rows) * 100\n",
    "        stats['raw']['delays'] = delay_count\n",
    "        stats['raw']['delay_rate'] = delay_rate\n",
    "        print(f\"✓ Delayed flights: {delay_count:,} ({delay_rate:.1f}%)\")\n",
    "    \n",
    "    # Sample null statistics (checking a few key columns)\n",
    "    print(\"\\nChecking data quality (sample columns)...\")\n",
    "    sample_cols = ['DEP_DEL15', 'ORIGIN', 'DEST', 'DISTANCE', 'CRS_DEP_TIME']\n",
    "    sample_cols = [c for c in sample_cols if c in df_raw.columns][:5]\n",
    "    \n",
    "    null_info = []\n",
    "    for col_name in sample_cols:\n",
    "        null_count = df_raw.filter(col(col_name).isNull() | (col(col_name) == '')).count()\n",
    "        if null_count > 0:\n",
    "            null_pct = (null_count / raw_rows) * 100\n",
    "            null_info.append(f\"{col_name}: {null_pct:.1f}%\")\n",
    "    \n",
    "    if null_info:\n",
    "        print(f\"✓ Nulls/empty in key columns: {', '.join(null_info)}\")\n",
    "    else:\n",
    "        print(f\"✓ No nulls in sampled columns\")\n",
    "    \n",
    "    df_raw.unpersist()\n",
    "    print(\"✓ Raw data statistics collected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load raw data: {e}\")\n",
    "    print(\"  (This is optional - continuing with joined data)\")\n",
    "    stats['raw'] = {'note': 'Raw data not loaded (optional)', 'error': str(e)}\n",
    "\n",
    "# ============================================================================\n",
    "# 2. JOINED DATA (AIRLINE + WEATHER)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. JOINED DATA (Airline + Weather)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    df_joined = spark.read.parquet(JOINED_1Y_PATH)\n",
    "    df_joined.cache()\n",
    "    \n",
    "    joined_rows = df_joined.count()\n",
    "    joined_cols = len(df_joined.columns)\n",
    "    \n",
    "    stats['joined'] = {\n",
    "        'rows': joined_rows,\n",
    "        'columns': joined_cols,\n",
    "        'description': 'Raw joined airline + weather data'\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Rows: {joined_rows:,}\")\n",
    "    print(f\"✓ Columns: {joined_cols}\")\n",
    "    \n",
    "    # Target variable distribution\n",
    "    if 'DEP_DEL15' in df_joined.columns:\n",
    "        delay_count = df_joined.filter(col('DEP_DEL15') == 1).count()\n",
    "        delay_rate = (delay_count / joined_rows) * 100\n",
    "        stats['joined']['delays'] = delay_count\n",
    "        stats['joined']['delay_rate'] = delay_rate\n",
    "        print(f\"✓ Delayed flights: {delay_count:,} ({delay_rate:.1f}%)\")\n",
    "    \n",
    "    # Calculate nulls in joined data\n",
    "    null_counts = []\n",
    "    for column in df_joined.columns[:10]:  # Sample first 10 columns for speed\n",
    "        null_count = df_joined.filter(col(column).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_counts.append((column, null_count, (null_count/joined_rows)*100))\n",
    "    \n",
    "    if null_counts:\n",
    "        avg_null_pct = sum([x[2] for x in null_counts]) / len(null_counts)\n",
    "        stats['joined']['avg_null_percentage'] = avg_null_pct\n",
    "        print(f\"✓ Average null % (sampled cols): {avg_null_pct:.1f}%\")\n",
    "    \n",
    "    df_joined.unpersist()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load joined data: {e}\")\n",
    "    stats['joined'] = {'error': str(e)}\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CLEANED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. CLEANED DATA (After Cleaning & Imputation)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    df_cleaned = spark.read.parquet(CLEANED_1Y_PATH)\n",
    "    \n",
    "    df_cleaned.cache()\n",
    "    \n",
    "    cleaned_rows = df_cleaned.count()\n",
    "    cleaned_cols = len(df_cleaned.columns)\n",
    "    \n",
    "    stats['cleaned'] = {\n",
    "        'rows': cleaned_rows,\n",
    "        'columns': cleaned_cols,\n",
    "        'description': 'After cleaning and imputation'\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Rows: {cleaned_rows:,}\")\n",
    "    print(f\"✓ Columns: {cleaned_cols}\")\n",
    "    \n",
    "    # Calculate data reduction\n",
    "    if 'joined' in stats and 'rows' in stats['joined']:\n",
    "        rows_removed = stats['joined']['rows'] - cleaned_rows\n",
    "        pct_removed = (rows_removed / stats['joined']['rows']) * 100\n",
    "        stats['cleaned']['rows_removed'] = rows_removed\n",
    "        stats['cleaned']['pct_removed'] = pct_removed\n",
    "        print(f\"✓ Rows removed: {rows_removed:,} ({pct_removed:.1f}%)\")\n",
    "        \n",
    "        cols_removed = stats['joined']['columns'] - cleaned_cols\n",
    "        stats['cleaned']['cols_removed'] = cols_removed\n",
    "        print(f\"✓ Columns removed: {cols_removed}\")\n",
    "    \n",
    "    # Target variable distribution after cleaning\n",
    "    if 'DEP_DEL15' in df_cleaned.columns:\n",
    "        delay_count = df_cleaned.filter(col('DEP_DEL15') == 1).count()\n",
    "        delay_rate = (delay_count / cleaned_rows) * 100\n",
    "        stats['cleaned']['delays'] = delay_count\n",
    "        stats['cleaned']['delay_rate'] = delay_rate\n",
    "        print(f\"✓ Delayed flights: {delay_count:,} ({delay_rate:.1f}%)\")\n",
    "        print(f\"✓ Class balance: {100-delay_rate:.1f}% on-time, {delay_rate:.1f}% delayed\")\n",
    "    \n",
    "    # Check for remaining nulls (should be minimal after imputation)\n",
    "    null_check_cols = ['DEP_DEL15', 'DISTANCE', 'CRS_DEP_TIME']\n",
    "    null_check_cols = [c for c in null_check_cols if c in df_cleaned.columns]\n",
    "    \n",
    "    remaining_nulls = 0\n",
    "    for col_name in null_check_cols:\n",
    "        null_count = df_cleaned.filter(col(col_name).isNull()).count()\n",
    "        remaining_nulls += null_count\n",
    "    \n",
    "    stats['cleaned']['remaining_nulls_checked'] = remaining_nulls\n",
    "    if remaining_nulls == 0:\n",
    "        print(f\"✓ No nulls in critical columns\")\n",
    "    else:\n",
    "        print(f\"⚠ Remaining nulls in critical columns: {remaining_nulls:,}\")\n",
    "    \n",
    "    # Unique values for key categorical features\n",
    "    cat_features = ['ORIGIN', 'DEST', 'OP_UNIQUE_CARRIER']\n",
    "    cat_features = [c for c in cat_features if c in df_cleaned.columns]\n",
    "    \n",
    "    for feat in cat_features:\n",
    "        unique_count = df_cleaned.select(feat).distinct().count()\n",
    "        print(f\"✓ Unique {feat}: {unique_count}\")\n",
    "    \n",
    "    df_cleaned.unpersist()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load cleaned data: {e}\")\n",
    "    stats['cleaned'] = {'error': str(e)}\n",
    "\n",
    "# ============================================================================\n",
    "# 4. FEATURE ENGINEERED DATA (Intermediate)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. FEATURE ENGINEERED DATA (Intermediate)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    df_features = spark.read.parquet(FEATURE_ENG_PATH)\n",
    "    \n",
    "    df_features.cache()\n",
    "    \n",
    "    features_rows = df_features.count()\n",
    "    features_cols = len(df_features.columns)\n",
    "    \n",
    "    stats['features'] = {\n",
    "        'rows': features_rows,\n",
    "        'columns': features_cols,\n",
    "        'description': 'After feature engineering'\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Rows: {features_rows:,}\")\n",
    "    print(f\"✓ Columns: {features_cols}\")\n",
    "    \n",
    "    # Features added\n",
    "    if 'cleaned' in stats and 'columns' in stats['cleaned']:\n",
    "        features_added = features_cols - stats['cleaned']['columns']\n",
    "        stats['features']['features_added'] = features_added\n",
    "        print(f\"✓ Features added: {features_added}\")\n",
    "    \n",
    "    # Check for engineered features (based on chat history)\n",
    "    engineered_features = [\n",
    "        'departure_hour', 'departure_month', 'departure_dayofweek',\n",
    "        'is_weekend', 'is_peak_hour', 'season', 'hour_category',\n",
    "        'total_flights_per_origin_day',\n",
    "        'rolling_origin_num_flights_24h', 'rolling_origin_delay_ratio_24h',\n",
    "        'is_holiday_window', 'weather_severity_index', 'distance_category'\n",
    "    ]\n",
    "    \n",
    "    present_features = [f for f in engineered_features if f in df_features.columns]\n",
    "    stats['features']['engineered_features_present'] = len(present_features)\n",
    "    \n",
    "    print(f\"✓ Engineered features present: {len(present_features)}/{len(engineered_features)}\")\n",
    "    \n",
    "    if present_features:\n",
    "        print(\"\\n  Present engineered features:\")\n",
    "        for feat in present_features[:8]:  # Show first 8\n",
    "            print(f\"    • {feat}\")\n",
    "        if len(present_features) > 8:\n",
    "            print(f\"    ... and {len(present_features) - 8} more\")\n",
    "    \n",
    "    # Final delay rate\n",
    "    if 'DEP_DEL15' in df_features.columns:\n",
    "        delay_count = df_features.filter(col('DEP_DEL15') == 1).count()\n",
    "        delay_rate = (delay_count / features_rows) * 100\n",
    "        stats['features']['delays'] = delay_count\n",
    "        stats['features']['delay_rate'] = delay_rate\n",
    "        print(f\"\\n✓ Final delayed flights: {delay_count:,} ({delay_rate:.1f}%)\")\n",
    "    \n",
    "    df_features.unpersist()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load feature engineered data: {e}\")\n",
    "    stats['features'] = {'error': str(e)}\n",
    "\n",
    "# ============================================================================\n",
    "# 5. FINAL FEATURE ENGINEERED DATA (Ready for Modeling)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. FINAL FEATURE ENGINEERED DATA (Ready for Modeling)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    df_final = spark.read.parquet(FINAL_FEATURE_PATH)\n",
    "    df_final.cache()\n",
    "    \n",
    "    final_rows = df_final.count()\n",
    "    final_cols = len(df_final.columns)\n",
    "    \n",
    "    stats['final'] = {\n",
    "        'rows': final_rows,\n",
    "        'columns': final_cols,\n",
    "        'description': 'Final cleaned feature set for modeling'\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Rows: {final_rows:,}\")\n",
    "    print(f\"✓ Columns: {final_cols}\")\n",
    "    \n",
    "    # Calculate difference from intermediate feature set\n",
    "    if 'features' in stats and 'columns' in stats['features']:\n",
    "        cols_removed = stats['features']['columns'] - final_cols\n",
    "        if cols_removed > 0:\n",
    "            print(f\"✓ Columns removed in final cleaning: {cols_removed}\")\n",
    "        elif cols_removed < 0:\n",
    "            print(f\"✓ Columns added in final step: {abs(cols_removed)}\")\n",
    "    \n",
    "    # Final delay rate\n",
    "    if 'DEP_DEL15' in df_final.columns:\n",
    "        delay_count = df_final.filter(col('DEP_DEL15') == 1).count()\n",
    "        delay_rate = (delay_count / final_rows) * 100\n",
    "        stats['final']['delays'] = delay_count\n",
    "        stats['final']['delay_rate'] = delay_rate\n",
    "        print(f\"✓ Final delayed flights: {delay_count:,} ({delay_rate:.1f}%)\")\n",
    "        print(f\"✓ Final class balance: {100-delay_rate:.1f}% on-time, {delay_rate:.1f}% delayed\")\n",
    "    \n",
    "    # Check for any remaining nulls\n",
    "    critical_cols = ['DEP_DEL15', 'ORIGIN', 'DEST', 'OP_UNIQUE_CARRIER']\n",
    "    critical_cols = [c for c in critical_cols if c in df_final.columns]\n",
    "    \n",
    "    total_nulls = 0\n",
    "    for col_name in critical_cols:\n",
    "        null_count = df_final.filter(col(col_name).isNull()).count()\n",
    "        total_nulls += null_count\n",
    "    \n",
    "    if total_nulls == 0:\n",
    "        print(f\"✓ No nulls in critical columns (ready for modeling)\")\n",
    "    else:\n",
    "        print(f\"⚠ Warning: {total_nulls:,} nulls remaining in critical columns\")\n",
    "    \n",
    "    df_final.unpersist()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load final feature data: {e}\")\n",
    "    stats['final'] = {'error': str(e)}\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SUMMARY FOR PRESENTATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRESENTATION SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA DATA PIPELINE PROGRESSION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stages = ['raw', 'joined', 'cleaned', 'features', 'final']\n",
    "stage_names = [\n",
    "    'Raw OTPW Data (12M)',\n",
    "    'Joined Data (Airline + Weather)', \n",
    "    'Cleaned Data',\n",
    "    'Feature Engineered (Intermediate)',\n",
    "    'Final Feature Set (Ready for Modeling)'\n",
    "]\n",
    "\n",
    "for stage, name in zip(stages, stage_names):\n",
    "    if stage in stats and 'rows' in stats[stage]:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  • Rows: {stats[stage]['rows']:,}\")\n",
    "        print(f\"  • Columns: {stats[stage]['columns']}\")\n",
    "        if 'delay_rate' in stats[stage]:\n",
    "            print(f\"  • Delay Rate: {stats[stage]['delay_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n\uD83D\uDCC8 KEY METRICS FOR TALK TRACK:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate overall metrics using the best available data\n",
    "start_stage = 'raw' if 'raw' in stats and 'rows' in stats['raw'] else 'joined'\n",
    "end_stage = 'final' if 'final' in stats and 'rows' in stats['final'] else 'features'\n",
    "\n",
    "if start_stage in stats and end_stage in stats and 'rows' in stats[start_stage] and 'rows' in stats[end_stage]:\n",
    "    total_data_reduction = stats[start_stage]['rows'] - stats[end_stage]['rows']\n",
    "    pct_data_reduction = (total_data_reduction / stats[start_stage]['rows']) * 100\n",
    "    \n",
    "    print(f\"\\n1. DATA SCALE:\")\n",
    "    print(f\"   • Started with: {stats[start_stage]['rows']:,} records ({start_stage.upper()} data)\")\n",
    "    print(f\"   • Final dataset: {stats[end_stage]['rows']:,} records\")\n",
    "    print(f\"   • Data reduction: {pct_data_reduction:.1f}% (cleaned invalid/cancelled flights)\")\n",
    "\n",
    "if end_stage in stats and 'delay_rate' in stats[end_stage]:\n",
    "    print(f\"\\n2. CLASS BALANCE:\")\n",
    "    print(f\"   • On-time flights: {100 - stats[end_stage]['delay_rate']:.1f}%\")\n",
    "    print(f\"   • Delayed flights: {stats[end_stage]['delay_rate']:.1f}%\")\n",
    "    imbalance_ratio = (100 - stats[end_stage]['delay_rate']) / stats[end_stage]['delay_rate']\n",
    "    print(f\"   • Imbalance ratio: {imbalance_ratio:.2f}:1 (on-time:delayed)\")\n",
    "    print(f\"   • Note: Imbalanced dataset requiring special handling (SMOTE, class weights, etc.)\")\n",
    "\n",
    "# Feature engineering impact\n",
    "if 'joined' in stats and end_stage in stats and 'columns' in stats['joined'] and 'columns' in stats[end_stage]:\n",
    "    features_added = stats[end_stage]['columns'] - stats['joined']['columns']\n",
    "    print(f\"\\n3. FEATURE ENGINEERING:\")\n",
    "    print(f\"   • Original columns (joined): {stats['joined']['columns']}\")\n",
    "    print(f\"   • Features added/engineered: {features_added}\")\n",
    "    print(f\"   • Final feature count: {stats[end_stage]['columns']}\")\n",
    "    \n",
    "    # Show intermediate steps if available\n",
    "    if 'features' in stats and 'columns' in stats['features']:\n",
    "        print(f\"   • Intermediate feature count: {stats['features']['columns']}\")\n",
    "        if end_stage == 'final':\n",
    "            features_removed = stats['features']['columns'] - stats['final']['columns']\n",
    "            if features_removed > 0:\n",
    "                print(f\"   • Features removed in final cleaning: {features_removed}\")\n",
    "\n",
    "if 'cleaned' in stats:\n",
    "    print(f\"\\n4. DATA QUALITY:\")\n",
    "    if 'rows_removed' in stats['cleaned']:\n",
    "        print(f\"   • Rows removed during cleaning: {stats['cleaned']['rows_removed']:,} ({stats['cleaned']['pct_removed']:.1f}%)\")\n",
    "    if 'remaining_nulls_checked' in stats['cleaned']:\n",
    "        print(f\"   • Nulls after imputation: {stats['cleaned']['remaining_nulls_checked']}\")\n",
    "    print(f\"   • Data integrity: High (cancelled/diverted flights removed, nulls imputed)\")\n",
    "\n",
    "print(f\"\\n5. DATASET CHARACTERISTICS:\")\n",
    "\n",
    "# Determine the year from the data\n",
    "data_year = \"2019\"  # Default based on JOINED_1Y_2019.parquet\n",
    "if 'raw' in stats and 'year' in stats['raw']:\n",
    "    data_year = stats['raw']['year']\n",
    "\n",
    "print(f\"   • Time period: 12 months ({data_year})\")\n",
    "print(f\"   • Data sources: 2 (Bureau of Transportation Statistics + NOAA Weather)\")\n",
    "print(f\"   • Prediction task: Binary classification (delay ≥15 min)\")\n",
    "print(f\"   • Target variable: DEP_DEL15 (1 = delayed, 0 = on-time)\")\n",
    "\n",
    "# Save statistics to file for reference\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert to pandas DataFrame for easy viewing\n",
    "summary_data = []\n",
    "for stage in ['raw', 'joined', 'cleaned', 'features', 'final']:\n",
    "    if stage in stats and 'rows' in stats[stage]:\n",
    "        row_data = {\n",
    "            'Stage': stage.capitalize(),\n",
    "            'Rows': f\"{stats[stage]['rows']:,}\",\n",
    "            'Columns': stats[stage]['columns']\n",
    "        }\n",
    "        if 'delay_rate' in stats[stage]:\n",
    "            row_data['Delay_Rate_%'] = f\"{stats[stage]['delay_rate']:.1f}\"\n",
    "        if 'rows_removed' in stats[stage]:\n",
    "            row_data['Rows_Removed'] = f\"{stats[stage]['rows_removed']:,}\"\n",
    "        summary_data.append(row_data)\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"/dbfs/student-groups/Group_4_4/presentation_statistics.csv\"\n",
    "try:\n",
    "    df_summary.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Statistics saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Could not save to CSV: {e}\")\n",
    "    print(\"Statistics available in memory as 'stats' dictionary\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICS COLLECTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Make stats dictionary available for further use\n",
    "print(\"\\n\uD83D\uDCA1 Access detailed stats via the 'stats' dictionary\")\n",
    "print(\"   Example: stats['features']['delay_rate']\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Team_4_4_Data_Stats_1Y",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}