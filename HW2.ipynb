{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 - Naive Bayes in Hadoop MR\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2025`__\n",
    "\n",
    "In the live sessions for week 2 and week 3 you got some practice designing and debugging Hadoop Streaming jobs. In this homework we'll use Hadoop MapReduce to implement your first parallelized machine learning algorithm: Naive Bayes. As you develop your implementation you'll test it on a small dataset that matches the 'Chinese Example' in the _Manning, Raghavan and Shutze_ reading for Week 2. For the main task in this assignment you'll be working with a small subset of the Enron Spam/Ham Corpus. By the end of this assignment you should be able to:\n",
    "* __... describe__ the Naive Bayes algorithm including both training and inference.\n",
    "* __... perform__ EDA on a corpus using Hadoop MR.\n",
    "* __... implement__ parallelized Naive Bayes.\n",
    "* __... constrast__ partial, unordered and total order sort and their implementations in Hadoop Streaming.\n",
    "* __... explain__ how smoothing affects the bias and variance of a Multinomial Naive Bayes model.\n",
    "\n",
    "As always, your work will be graded both on the correctness of your output and on the clarity and design of your code.\n",
    "\n",
    "## IMPORTANT NOTE (READ THIS TO SAVE YOURSELF LOTS OF TIME)\n",
    "\n",
    "#### WE HAVE MIGRATED OVER TO GRADESCOPE. THE FOLLOWING FILES ARE REQUIRED TO BE UPLOADED FOR GRADESCOPE\n",
    "\n",
    "Notebook (Don't change the name)\n",
    "- HW2.ipynb\n",
    "\n",
    "1.) \n",
    "- mapper.py\n",
    "- reducer.py\n",
    "\n",
    "2.) \n",
    "- chineseResults.txt\n",
    "\n",
    "3.)\n",
    "- chineseModelUnsmoothed.txt\n",
    "- chineseModelSmoothed.txt\n",
    "\n",
    "4.) THERE ARE CELLS TO CREATE THESE FILES BELOW.\n",
    "- Unsmoothed_results.txt\n",
    "- Smoothed_results.txt\n",
    "- Unsmoothed_NBmodel.txt\n",
    "- Smoothed_NBmodel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Before starting, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 3.2.4\n",
      "Source code repository https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r bd7653f7bc314f79f77d426015074df3eac9d6da\n",
      "Compiled by bigtop on 2025-09-10T20:59Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum feb1e797681713e36fa183c0646b3381\n",
      "This command was run using /usr/lib/hadoop/hadoop-common-3.2.4.jar\n"
     ]
    }
   ],
   "source": [
    "!hadoop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars (paths) - ADJUST AS NEEDED\n",
    "JAR_FILE = \"/usr/lib/hadoop/hadoop-streaming.jar\"\n",
    "HDFS_DIR = \"/user/root/HW2\"\n",
    "HOME_DIR = \"/media/notebooks/Assignments/HW2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/HW2\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "ENRON = \"data/enronemail_1h.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 05:03 HW2\n"
     ]
    }
   ],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -mkdir -p {HDFS_DIR}\n",
    "!hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Hadoop MapReduce Key Takeaways\n",
    "\n",
    "This assignment will be the only one in which you use Hadoop Streaming to implement a distributed algorithm. The key reason we continue to teach Hadoop streaming is because of the way it forces the programmer to think carefully about what is happening under the hood when you parallelize a calculation. This question will briefly highlight some of the most important concepts that you need to understand about Hadoop Streaming and MapReduce before we move on to Spark next week.   \n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) Multiple Choice:__ What \"programming paradigm\" is Hadoop MapReduce based on? \n",
    "\n",
    "* __b) Multiple Answers:__ What are the main ideas of this programming paradigm and how does MapReduce exemplify these ideas? (Select 3)\n",
    "\n",
    "* __c) Short Essay:__ What is the Hadoop Shuffle? When does it happen? Why is it potentially costly? Describe one specific thing we can we do to mitigate the cost associated with this stage of our Hadoop Streaming jobs.\n",
    "\n",
    "* __d) Multiple Choice:__ In Hadoop Streaming why do the input and output record format of a combiner script have to be the same? [__`HINT`__ What level of combining does the framework guarantee? what is the relationship between the record format your mapper emits and the format your reducer expects to receive?_]\n",
    "\n",
    "* __e) Multiple Choice:__ To what extent can you control the level of parallelization of your Hadoop Streaming jobs? Please be specific.\n",
    "\n",
    "* __f) Multiple Choice:__ What change in the kind of computing resources available prompted the creation of parallel computation frameworks like Hadoop?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# q1a\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: What \"programming paradigm\" is Hadoop MapReduce based on?\n",
    "\n",
    "#   a.) Object-Oriented Programming\n",
    "#   b.) Dynamic Programming\n",
    "#   c.) Recursive Programming\n",
    "#   d.) Functional Programming\n",
    "#   e.) None of the provided responses\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"d\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adf\n"
     ]
    }
   ],
   "source": [
    "# q1b\n",
    "### MULTIPLE ANSWERS\n",
    "### QUESTION: What are the main ideas of this programming paradigm and how does MapReduce exemplify these ideas? (Select 3)\n",
    "\n",
    "#   a.) The programming paradigms allows functions to accept other functions passed to them as\n",
    "#       parameters (higher order functions). In Hadoop Map-Reduce, you can write your own mappers\n",
    "#       and reducers and execute them against your data via map and reduce.\n",
    "\n",
    "#   b.) The programming paradigm is designed to keep data structures that hold all\n",
    "#       of the data in memory at once. In a map-reduce framework,\n",
    "#       this stateful quality allows you to process large amounts of information in parallel.\n",
    "\n",
    "#   c.) This programming paradigm optimizes execution of functions using recursion,\n",
    "#       allowing for efficient processing of calls to functions made over a large\n",
    "#       body of inputs. In map-reduce, this ability allows frameworks like Hadoop\n",
    "#       to apply map, but not reduce functions, with relatively low run times.\n",
    "\n",
    "#   d.) This paradigm attempts to avoid state changes/mutable data. In Hadoop Map-Reduce,\n",
    "#       this means that data is read from source, processed during the map phase, saved to disk,\n",
    "#       processed during reduce phase, and then again saved to disk.\n",
    "\n",
    "#   e.) The results of a Hadoop Map-Reduce job can vary slightly from job to job,\n",
    "#       depending on the under-the-hood operations that the framework puts in place.\n",
    "#       We as programmers can't always control these under-the-hood decisions.\n",
    "#       In practice, this means that a particular one set of inputs may produce\n",
    "#       results that are slightly different each time the map-reduce job is executed.\n",
    "\n",
    "#   f.) Another core idea of this programming programming is the idea of deterministic output,\n",
    "#       in which a function will always return the same output, given the same input.\n",
    "#       We can rely on a map-reduce job produce the same results, given the same source data.\n",
    "\n",
    "### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), y.), and z.), enter \"xyz\")\n",
    "answer = \"adf\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Hadoop Shuffle is the step that happens in between the map phase and the reduce phase.\n",
      "It’s the process of moving the mapper outputs around the cluster so that all the values for\n",
      "a given key end up on the same reducer. This happens right after the map tasks finish and\n",
      "before the reducers can start working.\n",
      "\n",
      "The shuffle can get expensive because it often requires writing data to disk, sorting it,\n",
      "and sending large amounts of information across the network. For really big jobs, the\n",
      "extra movement of data can slow things down a lot.\n",
      "\n",
      "One way to cut down on the cost is by using a combiner. A combiner runs after the mapper\n",
      "and does a mini-reduce at the local level, so less data has to be sent over the network during the\n",
      "shuffle stage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1c\n",
    "### SHORT ESSAY\n",
    "### QUESTION: What is the Hadoop Shuffle? When does it happen? Why is it potentially costly?\n",
    "#             Describe one specific thing we can we do to mitigate the cost associated\n",
    "#             with this stage of our Hadoop Streaming jobs.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "The Hadoop Shuffle is the step that happens in between the map phase and the reduce phase.\n",
    "It’s the process of moving the mapper outputs around the cluster so that all the values for\n",
    "a given key end up on the same reducer. This happens right after the map tasks finish and\n",
    "before the reducers can start working.\n",
    "\n",
    "The shuffle can get expensive because it often requires writing data to disk, sorting it,\n",
    "and sending large amounts of information across the network. For really big jobs, the\n",
    "extra movement of data can slow things down a lot.\n",
    "\n",
    "One way to cut down on the cost is by using a combiner. A combiner runs after the mapper\n",
    "and does a mini-reduce at the local level, so less data has to be sent over the network during the\n",
    "shuffle stage.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "# q1d\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: In Hadoop Streaming why do the input and output record format of a combiner\n",
    "#             script have to be the same? [HINT: What level of combining does the\n",
    "#             framework guarantee? What is the relationship between the record\n",
    "#             format your mapper emits and the format your reducer expects to receive?]\n",
    "\n",
    "#   a.) The combiner processes the Key/Value records produced by the mapper.\n",
    "#       As such a combiner is a replacement function for a reducer.\n",
    "\n",
    "#   b.) Since Hadoop does not guarantee that a combiner will be executed,\n",
    "#       our record format has to work whether or not it goes through the combiner.\n",
    "#       In other words, the signature of the input and output of the combiner\n",
    "#       must match the signature of the input to the reducer.\n",
    "\n",
    "#   c.) If using combiners, you can only use a specific record format which\n",
    "#       is determined by the Hadoop environment you are working in. Since this\n",
    "#       is the case, the record must be in the same system-specified format\n",
    "#       for the input and output of all combiners.\n",
    "\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"b\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "# q1e\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: To what extent can you control the level of parallelization of your Hadoop Streaming jobs?\n",
    "\n",
    "#   a.) We can explicitly control both the number of reducers used and the number of mappers used\n",
    "#       (for example by setting the -numReduceTasks parameter).\n",
    "\n",
    "#   b.) We can explicitly control the number of reducers used (for example by setting the\n",
    "#       -numReduceTasks parameter), but we can't force Hadoop to use the number of mappers we desire.\n",
    "\n",
    "#   c.) We can't explicitly control the number of reducers or the number of mappers used\n",
    "#       (but can make suggestions to the framework, for example by setting the -numReduceTasks parameter).\n",
    "\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"b\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "# q1f\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: What change in the kind of computing resources available prompted the creation of\n",
    "#             parallel computation frameworks like Hadoop?\n",
    "\n",
    "#   a.) The rise of cloud computing providers like GCP, AWS, and Microsoft Azure,\n",
    "#       and the availability of OS-level virtualization frameworks such as Docker,\n",
    "#       and orchestration frameworks such as Kubernetes, and Powerpoint made it\n",
    "#       possible to orchestrate multiple virtual computers in the cloud, giving rise to frameworks like Hadoop.\n",
    "\n",
    "#   b.) The rise of parallel computing frameworks was made possible by an increase\n",
    "#       in the availability of cheap commodity hardware. Instead of investing in super\n",
    "#       computers, the idea is to link together a lot of less powerful machines.\n",
    "\n",
    "#   c.) The rise of High Performance Computers made it possible to run huge numbers\n",
    "#       of calculations quickly on a single machine, which gave rise to frameworks like\n",
    "#       Hadoop which could take advantage of that extra computing power to run programs on huge datasets.\n",
    "\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"b\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: MapReduce Design Patterns \n",
    "\n",
    "In the last two live sessions and in your readings from Lin & Dyer you encountered a number of techniques for manipulating the logistics of a MapReduce implementation to ensure that the right information is available at the right time and location. In this question we'll review a few of the key techniques you learned.   \n",
    "\n",
    "### Q2 Tasks:\n",
    "\n",
    "* __a) Multiple Answers:__ What are counters (in the context of Hadoop Streaming)? How are they useful? What kinds of counters does Hadoop provide for you? How do you create your own custom counter? (Select 4)\n",
    "\n",
    "* __b) Multiple Choice:__ What are composite keys? How are they useful? How are they related to the idea of custom partitioning?\n",
    "\n",
    "* __c) Multiple Choice:__ What is the order inversion pattern? What problem does it help solve? How do we implement it? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "# q2a\n",
    "### MULTIPLE ANSWERS\n",
    "### QUESTION: What are counters (in the context of Hadoop Streaming)? How are they useful? What kinds of\n",
    "#             counters does Hadoop provide for you? How do you create your own custom counter? (Select 4)\n",
    "\n",
    "#   a.) Counters are a shared variable that is incremented and decremented\n",
    "#       atomically by the Hadoop framework. This means that all running\n",
    "#       instances within a job can update this variable to get a total\n",
    "#       count at the end. This is a departure from the principle of statelessness\n",
    "#       but is very useful for confirming that your jobs are running properly or\n",
    "#       aggregating summary statistics while performing other computations.\n",
    "\n",
    "#   b.) The built-in counters tell us information about IO, timing, and\n",
    "#       job orchestration. These are useful because such information can help\n",
    "#       you optimize your jobs. Especially useful are the Job Counters that tell\n",
    "#       you how many map, combine, and reduce tasks were run as well as the Map-Reduce\n",
    "#       Framework counters that tell you how many lines were input and output from your tasks.\n",
    "\n",
    "#   c.) To create a custom counter we just write to standard output.\n",
    "#       For example: sys.stderr.write(\"reporter:counter:MyWordCounter,count,1\\n\")\n",
    "\n",
    "#   d.) It is important to keep in mind that counter values are not available to\n",
    "#       mapper and reducer functions, and are only exposed after the job has finished.\n",
    "\n",
    "#   e.) Hadoop counters provide visibility into the inner workings of a Hadoop\n",
    "#       map-reduce job using NumPy and pandas.\n",
    "\n",
    "#   f.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), y.), and z.), enter \"xyz\")\n",
    "answer = \"abcd\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q2b\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: What are composite keys in Hadoop? Hint: How are they useful;\n",
    "#             How are they related to the idea of custom partitioning;\n",
    "\n",
    "#             Select the combination of the statements below that best answers the question posed above.\n",
    "#             i.  A composite key in Hadoop composes two or more fields or pieces of information from\n",
    "#                 the mapper output record to form the key that Hadoop can use during the shuffle phase.\n",
    "\n",
    "\n",
    "#             ii. In Hadoop a composite key can be used when we want to control which sets of records are\n",
    "#                 shuffled (partition the data) to the same reducer node. For example, the following specifies\n",
    "#                 a composite key that is used to partition mapper records such that records in the\n",
    "#                 same partition are sent to the same reducer:\n",
    "\n",
    "#                   -D stream.num.map.output.key.fields=4 \\\n",
    "#                   -D map.output.key.field.separator=. \\\n",
    "#                   -D mapreduce.partition.keypartitioner.options=-k1,2\n",
    "\n",
    "#             iii. In Hadoop a composite key can be used to determine the sort order of records arriving\n",
    "#                  at a reducer. For example, this can be accomplished as follows:\n",
    "\n",
    "#                   -D stream.num.map.output.key.fields=4 \\\n",
    "#                   -D mapreduce.map.output.key.field.separator=. \\\n",
    "#                   -D mapreduce.partition.keycomparator.options=-k1,1 -k2,2 -k3n,3\n",
    "\n",
    "#             iv. The (composite) key for partitioning and the (composite) key for sorting records arriving\n",
    "#                 at the reduced need not be the same. Secondary sorting is an example of a use case where\n",
    "#                 the fields used for partitioning and sorting are different. \n",
    "\n",
    "\n",
    "#   a.) i, ii, iii, iv\n",
    "#   b.) i, ii, iv\n",
    "#   c.) i, ii\n",
    "#   d.) i, ii, iii\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q2c\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: What is the order inversion pattern? What problem does it help solve? How do we implement it?\n",
    "\n",
    "#   a.) The Order Inversion (OI) design pattern can be used to control the order of reducer records in the\n",
    "#       MapReduce framework (which is useful because some computations require ordered data).\n",
    "#       The order inversion pattern is when we use a special key (think of a key having a value\n",
    "#       such as __total_word_frequency; notice that __ appear before any key starting with\n",
    "#       alphanumerical characters)  to ensure that a record (key, and value pair) is read and\n",
    "#       processed before all other records at the reduce stage. In some situations, this can help\n",
    "#       us avoid a second shuffle, for example, if you want to normalize word count frequency values\n",
    "#       by the total word count. Without using the OI pattern, the reducers would not have access\n",
    "#       to the total word until all records were processed by the reducers. The OI pattern enables\n",
    "#       the processing of records with special keys, such as __total_word_frequency, in advance\n",
    "#       of processing all individual word frequencies, leading to the total word count. This then\n",
    "#       enables the normalization step as the reducer processes each individual word count record. \n",
    "\n",
    "#   b.) The Order Inversion (OI) design pattern is when we choose a custom sort order\n",
    "#       for keys or values in the reduce stage, so that we get the desired results we want\n",
    "#       from the program execution faster. For example, in a word counting problem we may\n",
    "#       want the most common words to be output first, so we invert the order of the reducer\n",
    "#       output records so that they are sorted by descending order of the values, instead of\n",
    "#       the default behavior of sorting by the ascending order of the keys.\n",
    "\n",
    "#   c.) The Order Inversion (OI) design pattern is when we switch the keys and the values\n",
    "#       around so that each record output by the mapper can be reduced by its computed value\n",
    "#       instead of by its key (the default behavior). For example, if the input to the\n",
    "#       mappers for a word counting problem is a tuple of (count, word) then we would need to\n",
    "#       invert the order of the key and the value, so that the reducer can then combine the counts\n",
    "#       by using the word as the key.\n",
    "\n",
    "#   d.) The order inversion pattern does not apply to Hadoop map-reduce jobs.\n",
    "\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Understanding Total Order Sort\n",
    "\n",
    "The key challenge in distributed computing is to break a problem into a set of sub-problems that can be performed without communicating with each other. Ideally, we should be able to define an arbitrary number of splits and still get the right result, but that is not always possible. Parallelization becomes particularly challenging when we need to make comparisons between records, for example when sorting. Total Order Sort allows us to order large datasets in a way that enables efficient retrieval of results. Before beginning this assignment, make sure you have read and understand the [Total Order Sort Notebook](https://github.com/UCB-w261/main/blob/main/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb) in GCS folder (__`/GCS/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb`__). You can skip the first two MRJob sections, but the rest of section III and all of section IV are **very** important (and apply to Hadoop Streaming) so make sure to read them closely. Feel free to read the Spark sections as well but you won't be responsible for that material until later in the course. To verify your understanding, answer the following questions.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) Short Essay:__ What is the difference between a Partial Sort, an Unordered Total Sort, and a Total Order Sort? From the programmer's perspective, what does Total Order Sort allow us to do that we can't with Unordered Total? Why is this important with large datasets?\n",
    "\n",
    "* __b) Multiple Choice:__ Which phase of a MapReduce job is leveraged to implement Total Order Sort? Which default behaviors must be changed. Why must they be changed?\n",
    "\n",
    "* __c) Short Essay:__ Describe in words how to configure a Hadoop Streaming job for the custom sorting and partitioning that is required for Total Order Sort.  \n",
    "\n",
    "* __d) Multiple Choice:__ Explain why we need to use an inverse hash code function.\n",
    "\n",
    "* __e) Multiple Choice:__ Where does this function need to be located so that a Total Order Sort can be performed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "These are all examples of how data can be sorted in distributed systems. For Partial Sort, \n",
      "each reducer sorts the data it receives locally, but there’s no guarantee about how the outputs \n",
      "of different reducers relate to each other. For example, reducer 1 might output sorted keys that \n",
      "overlap with reducer 2’s keys. We might know the data is sorted within each partition, but not \n",
      "across the whole dataset. Unordered Total Sort has data partitioned so that each reducer gets \n",
      "a disjoint key range. This ensures that, reducer 1 processes only smaller keys and reducer 2 \n",
      "processes only larger keys. However, the final outputs are not in a single global order because \n",
      "Hadoop doesn’t enforce an order in how reducers write results. We would still need to merge or \n",
      "reorder the output files afterward. Finally, Total Order Sort makes the entire dataset sorted \n",
      "across all reducers. Each reducer’s output is sorted internally and the reducers’ outputs are \n",
      "written in key order. Thus, if you concatenate the reducer outputs, you get one single sorted file.\n",
      "\n",
      "From a programmer’s point of view, total order sort is valuable because it produces one globally \n",
      "ordered dataset straight from MapReduce without the need for an extra merge step. This is especially\n",
      "important with very large datasets, where merging reducer outputs would be too costly. It also makes\n",
      "the system more scalable since we can divide the work across many reducers while still preserving a\n",
      "consistent global order. Finally, it improves usability by enabling efficient operations on the\n",
      "distributed data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3a\n",
    "### SHORT ESSAY\n",
    "### QUESTION: What is the difference between a Partial Sort, an Unordered Total Sort, and a Total Order Sort?\n",
    "#             From the programmer's perspective, what does Total Order Sort allow us to do that we can't\n",
    "#             with Unordered Total? Why is this important with large datasets?\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "\n",
    "These are all examples of how data can be sorted in distributed systems. For Partial Sort, \n",
    "each reducer sorts the data it receives locally, but there’s no guarantee about how the outputs \n",
    "of different reducers relate to each other. For example, reducer 1 might output sorted keys that \n",
    "overlap with reducer 2’s keys. We might know the data is sorted within each partition, but not \n",
    "across the whole dataset. Unordered Total Sort has data partitioned so that each reducer gets \n",
    "a disjoint key range. This ensures that, reducer 1 processes only smaller keys and reducer 2 \n",
    "processes only larger keys. However, the final outputs are not in a single global order because \n",
    "Hadoop doesn’t enforce an order in how reducers write results. We would still need to merge or \n",
    "reorder the output files afterward. Finally, Total Order Sort makes the entire dataset sorted \n",
    "across all reducers. Each reducer’s output is sorted internally and the reducers’ outputs are \n",
    "written in key order. Thus, if you concatenate the reducer outputs, you get one single sorted file.\n",
    "\n",
    "From a programmer’s point of view, total order sort is valuable because it produces one globally \n",
    "ordered dataset straight from MapReduce without the need for an extra merge step. This is especially\n",
    "important with very large datasets, where merging reducer outputs would be too costly. It also makes\n",
    "the system more scalable since we can divide the work across many reducers while still preserving a\n",
    "consistent global order. Finally, it improves usability by enabling efficient operations on the\n",
    "distributed data.\n",
    "\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q3b\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Which phase of a MapReduce job is leveraged to implement Total Order Sort? Which default behaviors\n",
    "#             must be changed. Why must they be changed?\n",
    "\n",
    "#   a.) The shuffle/sort phase is where the sorting functionality takes place. To implement Total\n",
    "#       Order Sort, we must override the default partitioning behavior. By default, Hadoop\n",
    "#       does an alphanumerically increasing sort on a single key field within each\n",
    "#       partition resulting in partial order sort.\n",
    "\n",
    "#   b.) The shuffle/sort phase is where the sorting functionality takes place.\n",
    "#       To implement Total Order Sort, we must override the default partitioning behavior.\n",
    "#       By default, the Hadoop combiner does an alphanumerically increasing sort on a\n",
    "#       single key field within each partition resulting in unordered total sort.\n",
    "\n",
    "#   c.) To implement Total Order Sort, we must override the default partitioning behavior\n",
    "#       in the shuffle/sort phase. By default, Hadoop does not sort records, so one has to\n",
    "#       provide a post-processing step to sort the output partition files using, say,\n",
    "#       the Unix sort command.\n",
    "\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To configure a Hadoop Streaming job for Total Order Sort, the main goal is to ensure that all the data\n",
      "is globally ordered across all reducers. By default, Hadoop Streaming sorts records within each\n",
      "partition but does not control how records are distributed across partitions. To achieve Total Order\n",
      "Sort, we need to override the default partitioning by using a KeyFieldBasedPartitioner and supplying\n",
      "either a custom partition file or a method to compute partition keys such that each reducer receives a\n",
      "distinct, ordered subset of the data. This ensures that after all reducers finish, the concatenated\n",
      "output is fully sorted.\n",
      "\n",
      "Additionally, the job configuration must specify the proper key comparator to handle secondary sorting\n",
      "of values if needed. For example, setting the mapreduce.job.output.key.comparator.class to\n",
      "KeyFieldBasedComparator allows us to define which fields of the composite key to sort and whether to\n",
      "sort them numerically or lexicographically. Other settings, such as stream.map.output.field.separator\n",
      "and mapreduce.partition.keycomparator.options, help control how fields are parsed and sorted, ensuring\n",
      "that the shuffle phase respects the intended order.\n",
      "\n",
      "Finally, the mapper may need to prepend or transform the key so that it hashes to the correct partition\n",
      "index according to the custom partitioning scheme. This involves computing an inverse hash\n",
      "function or using percentile-based partition keys derived from a random sample of the dataset.\n",
      "Combined, these steps allow Hadoop Streaming to distribute the data across reducers while maintaining a\n",
      "total order, enabling efficient downstream operations without additional post-processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3c\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Describe in words how to configure a Hadoop Streaming job for the custom sorting and\n",
    "#             partitioning that is required for Total Order Sort. (hint: feel free to try and write\n",
    "#             a Hadoop Streaming job and come back to this question if you need. Additionally,\n",
    "#             be sure to check the Hadoop Streaming documentation!)\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "To configure a Hadoop Streaming job for Total Order Sort, the main goal is to ensure that all the data\n",
    "is globally ordered across all reducers. By default, Hadoop Streaming sorts records within each\n",
    "partition but does not control how records are distributed across partitions. To achieve Total Order\n",
    "Sort, we need to override the default partitioning by using a KeyFieldBasedPartitioner and supplying\n",
    "either a custom partition file or a method to compute partition keys such that each reducer receives a\n",
    "distinct, ordered subset of the data. This ensures that after all reducers finish, the concatenated\n",
    "output is fully sorted.\n",
    "\n",
    "Additionally, the job configuration must specify the proper key comparator to handle secondary sorting\n",
    "of values if needed. For example, setting the mapreduce.job.output.key.comparator.class to\n",
    "KeyFieldBasedComparator allows us to define which fields of the composite key to sort and whether to\n",
    "sort them numerically or lexicographically. Other settings, such as stream.map.output.field.separator\n",
    "and mapreduce.partition.keycomparator.options, help control how fields are parsed and sorted, ensuring\n",
    "that the shuffle phase respects the intended order.\n",
    "\n",
    "Finally, the mapper may need to prepend or transform the key so that it hashes to the correct partition\n",
    "index according to the custom partitioning scheme. This involves computing an inverse hash\n",
    "function or using percentile-based partition keys derived from a random sample of the dataset.\n",
    "Combined, these steps allow Hadoop Streaming to distribute the data across reducers while maintaining a\n",
    "total order, enabling efficient downstream operations without additional post-processing.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q3d\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Explain why we need to use an inverse hash code function.\n",
    "\n",
    "#   a.) The inverse hash code function lets us know the actual key that Hadoop will use for\n",
    "#       partitioning. By knowing this, we can reorder the partition keys that we are\n",
    "#       using according to the order that they will be sorted in after being hashed.\n",
    "#       Without this, we cannot know which partition key to use for which partition so\n",
    "#       that the end result is ordered by file name.\n",
    "\n",
    "#   b.) The inverse hash code function tells Hadoop which key to use for partitioning,\n",
    "#       allowing us to control which records get sent to which reducer. This means that\n",
    "#       we can know the full sorted order of all records once the Hadoop job finishes.\n",
    "\n",
    "#   c.) The inverse hash code function tells Hadoop us to invert the order of the\n",
    "#       keys which are sent from the mapper to the reducer, allowing us to sort\n",
    "#       the reducer output in descending order.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q3e\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Where does this function (i.e.,  inverse hash code function) need to be located\n",
    "#             so that a Total Order Sort can be performed?\n",
    "\n",
    "#   a.) The inverse hash function must be accessible before partitioning takes place.\n",
    "#       Hence, it needs to reside inside the mapper function.\n",
    "\n",
    "#   b.) The inverse hash function is only used after partitioning takes place.\n",
    "#       Hence, it needs to reside inside the reducer function.\n",
    "\n",
    "#   c.) The inverse hash function must be accessible before and after partitioning takes place.\n",
    "#       Hence, it must reside inside both the combiner and the reducer functions.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "\n",
    "For the main task in this portion of the homework you will train a classifier to determine whether an email represents spam or not. You will train your Naive Bayes model on a 100 record subset of the Enron Spam/Ham corpus available in the HW2 data directory (__`HW2/data/enronemail_1h.txt`__).\n",
    "\n",
    "__Source:__   \n",
    "The original data included about 93,000 emails which were made public after the company's collapse. There have been a number raw and preprocessed versions of this corpus (including those available [here](http://www.aueb.gr/users/ion/data/enron-spam/index.html) and [here](http://www.aueb.gr/users/ion/publications.html)). The subset we will use is limited to emails from 6 Enron employees and a number of spam sources. It is part of [this data set](http://www.aueb.gr/users/ion/data/enron-spam/) which was created by researchers working on personalized Bayesian spam filters. Their original publication is [available here](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf). __`IMPORTANT!`__ _For this homework please limit your analysis to the 100 email subset which we provide. No need to download or run your analysis on any of the original datasets, those links are merely provided as context._\n",
    "\n",
    "__Preprocessing:__  \n",
    "For their work, Metsis et al. (the authors) appeared to have pre-processed the data, not only collapsing all text to lower-case, but additionally separating \"words\" by spaces, where \"words\" unfortunately include punctuation. As a concrete example, the sentence:  \n",
    ">  `Hey Jon, I hope you don't get lost out there this weekend!`  \n",
    "\n",
    "... would have been reduced by Metsis et al. to the form:  \n",
    "> `hey jon , i hope you don ' t get lost out there this weekend !` \n",
    "\n",
    "... so we have reverted the data back toward its original state, removing spaces so that our sample sentence would now look like:\n",
    "> `hey jon, i hope you don't get lost out there this weekend!`  \n",
    "\n",
    "Thus we have at least preserved contractions and other higher-order lexical forms. However, one must be aware that this reversion is not complete, and that some object (specifically web sites) will be ill-formatted, and that all text is still lower-cased.\n",
    "\n",
    "\n",
    "__Format:__   \n",
    "All messages are collated to a tab-delimited format:  \n",
    "\n",
    ">    `ID \\t SPAM \\t SUBJECT \\t CONTENT \\n`  \n",
    "\n",
    "where:  \n",
    ">    `ID = string; unique message identifier`  \n",
    "    `SPAM = binary; with 1 indicating a spam message`  \n",
    "    `SUBJECT = string; title of the message`  \n",
    "    `CONTENT = string; content of the message`   \n",
    "    \n",
    "Note that either of `SUBJECT` or `CONTENT` may be \"NA\", and that all tab (\\t) and newline (\\n) characters have been removed from both of the `SUBJECT` and `CONTENT` columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/HW2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n",
      "0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n",
      "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n",
      "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n",
      "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n",
    "!head -n 5 {ENRON} | cut -c-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 data/enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "# see how many messages/lines are in the file \n",
    "#(this number may be off by 1 if the last line doesn't end with a newline)\n",
    "!wc -l {ENRON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {ENRON} {HDFS_DIR}/enron.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 03:15 /user/root/HW2/chinese-train-output\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 04:16 /user/root/HW2/chinese-train-smooth-output\n",
      "-rw-r--r--   1 root hadoop        119 2025-09-21 03:13 /user/root/HW2/chineseTest.txt\n",
      "-rw-r--r--   1 root hadoop        107 2025-09-21 03:13 /user/root/HW2/chineseTrain.txt\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 04:21 /user/root/HW2/enron-model\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 05:00 /user/root/HW2/enron-smoothed-eval\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 05:04 /user/root/HW2/enron-unsmoothed-eval\n",
      "-rw-r--r--   1 root hadoop     204559 2025-09-21 05:25 /user/root/HW2/enron.txt\n",
      "-rw-r--r--   1 root hadoop      41493 2025-09-21 04:17 /user/root/HW2/enron_test.txt\n",
      "-rw-r--r--   1 root hadoop     163066 2025-09-21 04:17 /user/root/HW2/enron_train.txt\n",
      "drwxr-xr-x   - root hadoop          0 2025-09-21 04:27 /user/root/HW2/smooth-model\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:  Enron Ham/Spam EDA\n",
    "\n",
    "Before building our classifier, let's get aquainted with our data. In particular, we're interested in which words occur more in spam emails than in legitimate (\"ham\") emails. In this question you'll implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. You'll also learn about two new Hadoop streaming parameters that will allow you to control how the records output from your mappers are partitioned for reducing on separate nodes. \n",
    "\n",
    "__`IMPORTANT NOTE:`__ For this question and all subsequent items, you should include both the subject and the body of the email in your analysis (i.e. concatetate them to get the 'text' of the document).\n",
    "\n",
    "### Q4 Tasks:\n",
    "* __a) Code in Notebook:__ Complete the missing components of the code in __`EnronEDA/mapper.py`__ and __`EnronEDA/reducer.py`__ to create a Hadoop  MapReduce job that counts how many times each word in the corpus occurs in an email for each class. Pay close attention to the data format specified in the docstrings of these scripts _-- there are a number of ways to accomplish this task, we've chosen this format to help illustrate a technique in `part e`_. Run the provided unit tests to confirm that your code works as expected, then run the provided Hadoop Streaming command to apply your analysis to the Enron data.\n",
    "\n",
    "* __b) Code in Notebook + Multiple Choice:__ How many times does the word \"__assistance__\" occur in each class? (`HINT:` Use a `grep` command to read from the results file you generated in '`a`' and then report the answer in the space provided.)\n",
    "\n",
    "* __c) Multiple Choice:__ Would it have been possible to add some sorting parameters to the Hadoop streaming command that would cause our `part a` results to be sorted by count? Explain why or why not. (`HINT:` This question demands an understanding of the sequence of the phases of MapReduce.)\n",
    "\n",
    "* __d) Code in Notebook + Short Essay:__ Write a second Hadoop MapReduce job to sort the output of `part a` first by class and then by count. Run your job and save the results to a local file. Then describe in words how you would go about printing the top 10 words in each class given this sorted output. (`HINT 1:` _remember that you can simply pass the `part a` output directory to the input field of this job; `HINT 2:` since this task is just reordering the records from `part a` we don't need to write a mapper or reducer, just use `/bin/cat` for both_)\n",
    "\n",
    "* __e) Code in Notebook:__ A more efficient alternative to '`grep`-ing' for the top 10 words in each class would be to use the Hadoop framework to separate records from each class into its own partition so that we can just read the top lines in each. Rewrite your job from ` part d` to specify 2 reduce tasks and to tell Hadoop to partition based on the second field (which indicates spam/ham in our data). Your code should maintain the secondary sort -- that is each partition should list words from most to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n",
    "!chmod a+x EnronEDA/mapper.py\n",
    "!chmod a+x EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\t1\t1\n",
      "body\t1\t1\n",
      "title\t0\t1\n",
      "body\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/mapper.py (RUN THIS CELL AS IS)\n",
    "# STRING IS:\n",
    "#   d1\t1\ttitle\tbody\n",
    "#   d2\t0\ttitle\tbody\n",
    "!echo -e \"d1\\t1\\ttitle\\tbody\\nd2\\t0\\ttitle\\tbody\" | EnronEDA/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-e one\t1\t1\n",
      "one\t0\t2\n",
      "two\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/reducer.py (RUN THIS CELL AS IS)\n",
    "# STRING IS:\n",
    "#   one\t1\t1\n",
    "#   one\t0\t2\n",
    "#   two\t0\t1\n",
    "!echo -e \"one\\t1\\t1\\none\\t0\\t1\\none\\t0\\t1\\ntwo\\t0\\t1\" | EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob5258217519449079432.jar tmpDir=null\n",
      "2025-09-21 05:25:38,753 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:25:39,058 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:25:39,577 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:25:39,578 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:25:39,794 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0017\n",
      "2025-09-21 05:25:40,531 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 05:25:40,582 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 05:25:40,778 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0017\n",
      "2025-09-21 05:25:40,780 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 05:25:40,977 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 05:25:40,978 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 05:25:41,044 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0017\n",
      "2025-09-21 05:25:41,080 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0017/\n",
      "2025-09-21 05:25:41,081 INFO mapreduce.Job: Running job: job_1758422346528_0017\n",
      "2025-09-21 05:25:49,268 INFO mapreduce.Job: Job job_1758422346528_0017 running in uber mode : false\n",
      "2025-09-21 05:25:49,269 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 05:25:58,502 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2025-09-21 05:25:59,508 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 05:26:06,591 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 05:26:14,651 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 05:26:20,693 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-09-21 05:26:21,699 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 05:26:23,715 INFO mapreduce.Job: Job job_1758422346528_0017 completed successfully\n",
      "2025-09-21 05:26:23,808 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369010\n",
      "\t\tFILE: Number of bytes written=3517647\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=258608\n",
      "\t\tHDFS: Number of bytes written=70551\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=194617896\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=26273700\n",
      "\t\tTotal time spent by all map tasks (ms)=61666\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8325\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=61666\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8325\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=194617896\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=26273700\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=369106\n",
      "\t\tInput split bytes=801\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=369106\n",
      "\t\tReduce input records=31490\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=62980\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=1632\n",
      "\t\tCPU time spent (ms)=16710\n",
      "\t\tPhysical memory (bytes) snapshot=6036426752\n",
      "\t\tVirtual memory (bytes) snapshot=49014398976\n",
      "\t\tTotal committed heap usage (bytes)=5052039168\n",
      "\t\tPeak Map Physical memory (bytes)=622710784\n",
      "\t\tPeak Map Virtual memory (bytes)=4458803200\n",
      "\t\tPeak Reduce Physical memory (bytes)=305885184\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4456349696\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=257807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70551\n",
      "2025-09-21 05:26:23,808 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t1\t8\n",
      "assistance\t0\t2\n"
     ]
    }
   ],
   "source": [
    "# part b - write your grep command here\n",
    "# !grep -P \"^assistance\\t1\\t\" EnronEDA/results.txt\n",
    "# !grep -P \"^assistance\\t0\\t\" EnronEDA/results.txt\n",
    "!grep '^assistance' EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q4b\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: How many times does the word \"assistance\" occur in each class?\n",
    "#             (HINT: Use a grep command to read from the results file you\n",
    "#             generated in 'a' and then report the answer in the space provided.)\n",
    "\n",
    "#   a.) 'assistance' occurs 8 times in Spam emails and only 2 times in real emails.\n",
    "#   b.) 'assistance' occurs only 2 times in Spam emails and 8 times in real emails.\n",
    "#   c.) 'assistance' occurs 10 times in Spam emails and only 2 times in real emails.\n",
    "#   d.) 'assistance' occurs 8 times in Spam emails and only 3 times in real emails.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# q4c\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Would it have been possible to add some sorting parameters to the Hadoop\n",
    "#             streaming command that would cause our part a results to be sorted by count?\n",
    "#             (HINT: This question demands an understanding of the sequence of the phases\n",
    "#             of MapReduce.)\n",
    "\n",
    "#   a.) No, we can't sort on counts in the original job because Hadoop's sorting only allows\n",
    "#       keys to be sorted by letters, and counts are numbers.\n",
    "\n",
    "#   b.) Yes, we can sort on counts in the original job because Hadoop's sorting occurs after\n",
    "#       the reducer stage, which is where our counts are tallied up.\n",
    "\n",
    "#   c.) Yes, we can sort on counts in the original job because Hadoop allows sorting in any\n",
    "#       of the stages of the map-reduce job.\n",
    "\n",
    "#   d.) No, we can't sort on counts in the original job because Hadoop's sorting occurs in the\n",
    "#       phase between the mapper and reducer since our counts aren't tallied up until after the reducer.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"d\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-sort-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part d - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-sort-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob3802559138501684604.jar tmpDir=null\n",
      "2025-09-21 05:26:34,385 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:26:34,631 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:26:35,287 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:26:35,288 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:26:35,838 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0018\n",
      "2025-09-21 05:26:36,161 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2025-09-21 05:26:36,252 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2025-09-21 05:26:36,486 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0018\n",
      "2025-09-21 05:26:36,489 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 05:26:36,685 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 05:26:36,685 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 05:26:36,748 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0018\n",
      "2025-09-21 05:26:36,786 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0018/\n",
      "2025-09-21 05:26:36,788 INFO mapreduce.Job: Running job: job_1758422346528_0018\n",
      "2025-09-21 05:26:45,003 INFO mapreduce.Job: Job job_1758422346528_0018 running in uber mode : false\n",
      "2025-09-21 05:26:45,005 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 05:26:54,135 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-09-21 05:27:01,199 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-09-21 05:27:02,208 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-09-21 05:27:07,248 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2025-09-21 05:27:09,258 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "2025-09-21 05:27:10,264 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2025-09-21 05:27:12,275 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 05:27:17,299 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 05:27:19,318 INFO mapreduce.Job: Job job_1758422346528_0018 completed successfully\n",
      "2025-09-21 05:27:19,414 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=88737\n",
      "\t\tFILE: Number of bytes written=2946778\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=103706\n",
      "\t\tHDFS: Number of bytes written=76611\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=193121952\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10036080\n",
      "\t\tTotal time spent by all map tasks (ms)=61192\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3180\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=61192\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3180\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=193121952\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10036080\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6060\n",
      "\t\tMap output records=6060\n",
      "\t\tMap output bytes=76611\n",
      "\t\tMap output materialized bytes=88791\n",
      "\t\tInput split bytes=1010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6060\n",
      "\t\tReduce shuffle bytes=88791\n",
      "\t\tReduce input records=6060\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=12120\n",
      "\t\tShuffled Maps =10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=10\n",
      "\t\tGC time elapsed (ms)=1460\n",
      "\t\tCPU time spent (ms)=13640\n",
      "\t\tPhysical memory (bytes) snapshot=6241263616\n",
      "\t\tVirtual memory (bytes) snapshot=49005006848\n",
      "\t\tTotal committed heap usage (bytes)=5315231744\n",
      "\t\tPeak Map Physical memory (bytes)=604893184\n",
      "\t\tPeak Map Virtual memory (bytes)=4460597248\n",
      "\t\tPeak Reduce Physical memory (bytes)=309309440\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4466200576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=102696\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76611\n",
      "2025-09-21 05:27:19,415 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# q4d1\n",
    "# part d - write your Hadoop streaming job here\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2n -k3,3nr\" \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-00000 > EnronEDA/sorted_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Since the results are already sorted first by class and then by count, printing the top 10 words in\n",
      "each class is just going to be scanning through the file and keeping track of how many words we’ve \n",
      "seen for each class. I’d read the sorted output line by line, check the class field, and maintain a \n",
      "counter for each class. When I see a line with class 1 (spam), I increment the spam \n",
      "counter and print the word only if I haven’t yet reached 10. I would do the same for class 0 (ham).\n",
      "Once the counter for a class reaches 10, I would stop printing for that class and continue on until\n",
      "both classes have their top 10 words. The sorted order allows the most frequent words for each class\n",
      "appear first in the group, so I don’t need to do any extra sorting or tallying. I just read, check the\n",
      "class, and print the first 10 entries per class. This makes it simple and efficient to pull out the top\n",
      "terms from the data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q4d2\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Describe in words how you would go about printing the top 10\n",
    "#             words in each class given this sorted output.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Since the results are already sorted first by class and then by count, printing the top 10 words in\n",
    "each class is just going to be scanning through the file and keeping track of how many words we’ve \n",
    "seen for each class. I’d read the sorted output line by line, check the class field, and maintain a \n",
    "counter for each class. When I see a line with class 1 (spam), I increment the spam \n",
    "counter and print the word only if I haven’t yet reached 10. I would do the same for class 0 (ham).\n",
    "Once the counter for a class reaches 10, I would stop printing for that class and continue on until\n",
    "both classes have their top 10 words. The sorted order allows the most frequent words for each class\n",
    "appear first in the group, so I don’t need to do any extra sorting or tallying. I just read, check the\n",
    "class, and print the first 10 entries per class. This makes it simple and efficient to pull out the top\n",
    "terms from the data.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-sort-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob7189516461276457532.jar tmpDir=null\n",
      "2025-09-21 05:27:29,913 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:27:30,182 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:27:30,817 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:27:30,818 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:27:31,371 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0019\n",
      "2025-09-21 05:27:31,690 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2025-09-21 05:27:31,773 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2025-09-21 05:27:32,009 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0019\n",
      "2025-09-21 05:27:32,011 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 05:27:32,224 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 05:27:32,224 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 05:27:32,287 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0019\n",
      "2025-09-21 05:27:32,324 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0019/\n",
      "2025-09-21 05:27:32,326 INFO mapreduce.Job: Running job: job_1758422346528_0019\n",
      "2025-09-21 05:27:41,438 INFO mapreduce.Job: Job job_1758422346528_0019 running in uber mode : false\n",
      "2025-09-21 05:27:41,439 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 05:27:49,572 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2025-09-21 05:27:50,581 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-09-21 05:27:55,628 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-09-21 05:27:57,647 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-09-21 05:28:02,690 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2025-09-21 05:28:04,713 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2025-09-21 05:28:07,733 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 05:28:14,779 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-09-21 05:28:15,784 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 05:28:16,799 INFO mapreduce.Job: Job job_1758422346528_0019 completed successfully\n",
      "2025-09-21 05:28:16,900 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=88743\n",
      "\t\tFILE: Number of bytes written=3205180\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=103706\n",
      "\t\tHDFS: Number of bytes written=76611\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=183174240\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27924288\n",
      "\t\tTotal time spent by all map tasks (ms)=58040\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8848\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=58040\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8848\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=183174240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27924288\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6060\n",
      "\t\tMap output records=6060\n",
      "\t\tMap output bytes=76611\n",
      "\t\tMap output materialized bytes=88851\n",
      "\t\tInput split bytes=1010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6060\n",
      "\t\tReduce shuffle bytes=88851\n",
      "\t\tReduce input records=6060\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=12120\n",
      "\t\tShuffled Maps =20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tGC time elapsed (ms)=1685\n",
      "\t\tCPU time spent (ms)=15160\n",
      "\t\tPhysical memory (bytes) snapshot=6603927552\n",
      "\t\tVirtual memory (bytes) snapshot=53484384256\n",
      "\t\tTotal committed heap usage (bytes)=5619843072\n",
      "\t\tPeak Map Physical memory (bytes)=607313920\n",
      "\t\tPeak Map Virtual memory (bytes)=4468150272\n",
      "\t\tPeak Reduce Physical memory (bytes)=354471936\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4472717312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=102696\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76611\n",
      "2025-09-21 05:28:16,900 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# q4e\n",
    "# part e - write your Hadoop streaming job here\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k2,2 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2n -k3,3nr\" \\\n",
    "    -input {HDFS_DIR}/eda-output \\\n",
    "    -output {HDFS_DIR}/eda-sort-output \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv PATH={PATH}\n",
    "\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-* > EnronEDA/sorted_results_partitioned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "the\t0\t549\t\n",
      "to\t0\t398\t\n",
      "ect\t0\t382\t\n",
      "and\t0\t278\t\n",
      "of\t0\t230\t\n",
      "hou\t0\t206\t\n",
      "a\t0\t196\t\n",
      "in\t0\t182\t\n",
      "for\t0\t170\t\n",
      "on\t0\t135\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "and\t1\t392\t\n",
      "your\t1\t357\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "in\t1\t236\t\n",
      "for\t1\t204\t\n",
      "com\t1\t153\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part e - view the top 10 records from each partition (RUN THIS CELL AS IS)\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head 2>&1 | tee q4e.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__\n",
    "<table>\n",
    "<th>part-00000:</th>\n",
    "<th>part-00001:</th>\n",
    "<tr><td><pre>\n",
    "the\t0\t549\t\n",
    "to\t0\t398\t\n",
    "ect\t0\t382\t\n",
    "and\t0\t278\t\n",
    "of\t0\t230\t\n",
    "hou\t0\t206\t\n",
    "a\t0\t196\t\n",
    "in\t0\t182\t\n",
    "for\t0\t170\t\n",
    "on\t0\t135\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1\t698\t\n",
    "to\t1\t566\t\n",
    "and\t1\t392\t\n",
    "your\t1\t357\t\n",
    "a\t1\t347\t\n",
    "you\t1\t345\t\n",
    "of\t1\t336\t\n",
    "in\t1\t236\t\n",
    "for\t1\t204\t\n",
    "com\t1\t153\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Counters and Combiners\n",
    "\n",
    "Tuning the number of mappers & reducers is helpful to optimize very large distributed computations. Doing so successfully requires a thorough understanding of the data size at each stage of the job. As you learned in the week3 live session, counters are an invaluable resource for understanding this kind of detail. In this question, we will take the EDA performed in Question 4 as an opportunity to illustrate some related concepts.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) Multiple Choice:__ Read the Hadoop output from your job in Question 4a to report how many records are emitted by the mappers and how many records are received by the reducers (hint: we are not using combiners here).\n",
    "\n",
    "* __b) Multiple Choice:__ In the context of word counting in Question 4b, what does the number of records emitted by the mapper represent practically?\n",
    "\n",
    "* __c) Code in Notebook:__ Note that we wrote the reducer in Question 4a such that the input and output record format is identical. This makes it easy to use the same reducer script as a combiner. In the space provided below, write the Hadoop Streaming command to re-run your job from Question 4a with this this __combiner step__ added.\n",
    "\n",
    "* __d) Multiple Choice:__ Read the Hadoop output from your job in Question 5c to report how many records are emitted by the mappers and how many records are received by the reducers (hint: we are using combiners here). \n",
    "\n",
    "* __e) Short Essay:__ Compare your results from Question 5d to what you saw in Question 5a. Explain the differences, if any.\n",
    "\n",
    "* __f) Short Essay:__ Describe a scenario where using a combiner would NOT improve the efficiency of the shuffle stage. Explain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# q5a1\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Read the Hadoop output from your job in Question 4a to report how\n",
    "#             many records are emitted by the mappers (hint: we are not using combiners here). \n",
    "\n",
    "#   a.) 20576\n",
    "#   b.) 13096\n",
    "#   c.) 10101\n",
    "#   d.) 31490\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"d\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "# q5a2\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Read the Hadoop output from your job in Question 4a to report how\n",
    "#             many records are received by the reducers (hint: we are not using combiners here). \n",
    "\n",
    "#   a.) 13096\n",
    "#   b.) 31490\n",
    "#   c.) 10101\n",
    "#   d.) 20576\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"b\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "# q5b\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: In the context of word counting in Question 4b, what does the number of records emitted by the mapper\n",
    "#             represent practically?\n",
    "\n",
    "#   a.) The total number of unique words in all documents.\n",
    "#   b.) The total number of documents.\n",
    "#   c.) The total number of words in all documents.\n",
    "#   d.) The average number of words per document.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"c\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part c - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Mapper tokenizes and emits words with their class.\n",
      "INPUT:\n",
      "    ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
      "OUTPUT:\n",
      "    word \\t class \\t count \n",
      "\"\"\"\n",
      "import re\n",
      "import sys\n",
      "\n",
      "# read from standard input\n",
      "for line in sys.stdin:\n",
      "    # parse input\n",
      "    docID, _class, subject, body = line.split('\\t')\n",
      "    # tokenize\n",
      "    words = re.findall(r'[a-z]+', subject + ' ' + body)\n",
      "    \n",
      "############ YOUR CODE HERE #########\n",
      "    # emit each word with its class\n",
      "    for word in words:\n",
      "        print(f\"{word}\\t{_class}\\t1\")\n",
      "############ (END) YOUR CODE #########"
     ]
    }
   ],
   "source": [
    "!cat EnronEDA/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Reducer takes words with their class and partial counts and computes totals.\n",
      "INPUT:\n",
      "    word \\t class \\t partialCount \n",
      "OUTPUT:\n",
      "    word \\t class \\t totalCount  \n",
      "\"\"\"\n",
      "import re\n",
      "import sys\n",
      "\n",
      "# initialize trackers\n",
      "current_word = None\n",
      "spam_count, ham_count = 0,0\n",
      "\n",
      "# read from standard input\n",
      "for line in sys.stdin:\n",
      "    # parse input\n",
      "    word, is_spam, count = line.split('\\t')\n",
      "    \n",
      "############ YOUR CODE HERE #########\n",
      "    count = int(count)\n",
      "    if current_word is None:\n",
      "        current_word = word\n",
      "    if word != current_word:\n",
      "        # output both counts for the finished word\n",
      "        if spam_count > 0:\n",
      "            print(f\"{current_word}\\t1\\t{spam_count}\")\n",
      "        if ham_count > 0:\n",
      "            print(f\"{current_word}\\t0\\t{ham_count}\")\n",
      "        # reset trackers\n",
      "        current_word = word\n",
      "        spam_count, ham_count = 0, 0\n",
      "    # add to the counters\n",
      "    if is_spam == \"1\":\n",
      "        spam_count += count\n",
      "    else:\n",
      "        ham_count += count\n",
      "# flush the last word\n",
      "if current_word is not None:\n",
      "    if spam_count > 0:\n",
      "        print(f\"{current_word}\\t1\\t{spam_count}\")\n",
      "    if ham_count > 0:\n",
      "        print(f\"{current_word}\\t0\\t{ham_count}\")\n",
      "############ (END) YOUR CODE #########"
     ]
    }
   ],
   "source": [
    "!cat EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob1478029178737531473.jar tmpDir=null\n",
      "2025-09-21 05:28:31,118 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:28:31,434 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:28:31,948 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:28:31,948 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:28:32,149 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0020\n",
      "2025-09-21 05:28:32,495 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 05:28:32,568 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 05:28:32,778 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0020\n",
      "2025-09-21 05:28:32,780 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 05:28:32,973 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 05:28:32,973 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 05:28:33,046 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0020\n",
      "2025-09-21 05:28:33,083 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0020/\n",
      "2025-09-21 05:28:33,085 INFO mapreduce.Job: Running job: job_1758422346528_0020\n",
      "2025-09-21 05:28:41,186 INFO mapreduce.Job: Job job_1758422346528_0020 running in uber mode : false\n",
      "2025-09-21 05:28:41,187 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 05:28:51,321 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 05:28:59,394 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 05:29:07,470 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2025-09-21 05:29:08,476 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 05:29:14,517 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-09-21 05:29:15,523 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 05:29:17,545 INFO mapreduce.Job: Job job_1758422346528_0020 completed successfully\n",
      "2025-09-21 05:29:17,645 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149033\n",
      "\t\tFILE: Number of bytes written=3081642\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=258608\n",
      "\t\tHDFS: Number of bytes written=70551\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=207671112\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=26835468\n",
      "\t\tTotal time spent by all map tasks (ms)=65802\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8503\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=65802\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8503\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=207671112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=26835468\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=149129\n",
      "\t\tInput split bytes=801\n",
      "\t\tCombine input records=31490\n",
      "\t\tCombine output records=11433\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=149129\n",
      "\t\tReduce input records=11433\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=22866\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=1574\n",
      "\t\tCPU time spent (ms)=21050\n",
      "\t\tPhysical memory (bytes) snapshot=6240325632\n",
      "\t\tVirtual memory (bytes) snapshot=49077080064\n",
      "\t\tTotal committed heap usage (bytes)=5060952064\n",
      "\t\tPeak Map Physical memory (bytes)=634900480\n",
      "\t\tPeak Map Virtual memory (bytes)=4478160896\n",
      "\t\tPeak Reduce Physical memory (bytes)=331509760\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4476063744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=257807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70551\n",
      "2025-09-21 05:29:17,645 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# q5c\n",
    "# write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/mapper.py,EnronEDA/reducer.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -combiner reducer.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# q5d1\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Read the Hadoop output from your job in Question 5c to report how\n",
    "#             many records are emitted by the mappers (hint: we are not using combiners here). \n",
    "\n",
    "#   a.) 10130\n",
    "#   b.) 13096\n",
    "#   c.) 20576\n",
    "#   d.) 31490\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"d\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "# q5d2\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Read the Hadoop output from your job in Question 5c to report how\n",
    "#             many records are received by the reducers (hint: we are not using combiners here). \n",
    "\n",
    "#   a.) 13096\n",
    "#   b.) 31490\n",
    "#   c.) 10130\n",
    "#   d.) 20576\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"e\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In Question 5a, the number of records received by the reducers was 31,490, matching the number\n",
      "of records outputted by the mappers since no combiner was used. In Question 5d, after adding the\n",
      "combiner in Question 5c, the number of records received by the reducers dropped to 11,433. There is a\n",
      "difference because the combiner partially aggregates the mapper output before it is sent to the\n",
      "reducers. This reduces the amount of intermediate data shuffled across the network, lowering both the\n",
      "reduce input records and the overall network input/output, which improves efficiency without affecting\n",
      "the final output counts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q5e\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Compare your results from Question 5d to what you saw in\n",
    "#             Question 5a. Explain the differences, if any.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "In Question 5a, the number of records received by the reducers was 31,490, matching the number\n",
    "of records outputted by the mappers since no combiner was used. In Question 5d, after adding the\n",
    "combiner in Question 5c, the number of records received by the reducers dropped to 11,433. There is a\n",
    "difference because the combiner partially aggregates the mapper output before it is sent to the\n",
    "reducers. This reduces the amount of intermediate data shuffled across the network, lowering both the\n",
    "reduce input records and the overall network input/output, which improves efficiency without affecting\n",
    "the final output counts.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A combiner won’t help if most of the keys output by the mappers are unique or occur only once.\n",
      "If this is true, there would be nothing that would be aggregated locally before sending the data to\n",
      "the reducers, so every record still has to travel across the network. Thus, the shuffle stage\n",
      "sees no reduction in data volume or network traffic, and the overall job efficiency does not improve.\n",
      "Therefore, combiners are beneficial when there are repeated keys that can be partially reduced at the\n",
      "mapper level.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q5f\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Describe a scenario where using a combiner would NOT improve the efficiency\n",
    "#             of the shuffle stage. Explain.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "A combiner won’t help if most of the keys output by the mappers are unique or occur only once.\n",
    "If this is true, there would be nothing that would be aggregated locally before sending the data to\n",
    "the reducers, so every record still has to travel across the network. Thus, the shuffle stage\n",
    "sees no reduction in data volume or network traffic, and the overall job efficiency does not improve.\n",
    "Therefore, combiners are beneficial when there are repeated keys that can be partially reduced at the\n",
    "mapper level.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Document Classification Task Overview\n",
    "\n",
    "The week 2 assigned reading from Chapter 13 of _Introduction to Information Retrieval_ by Manning, Raghavan and Schutze provides a thorough introduction to the document classification task and the math behind Naive Bayes. In this question we'll use the example from Table 13.1 (reproduced below) to 'train' an unsmoothed Multinomial Naive Bayes model and classify a test document by hand.\n",
    "\n",
    "<table>\n",
    "<th>DocID</th>\n",
    "<th>Class</th>\n",
    "<th>Subject</th>\n",
    "<th>Body</th>\n",
    "<tr><td>Doc1</td><td>1</td><td></td><td>Chinese Beijing Chinese</td></tr>\n",
    "<tr><td>Doc2</td><td>1</td><td></td><td>Chinese Chinese Shanghai</td></tr>\n",
    "<tr><td>Doc3</td><td>1</td><td></td><td>Chinese Macao</td></tr>\n",
    "<tr><td>Doc4</td><td>0</td><td></td><td>Tokyo Japan Chinese</td></tr>\n",
    "</table>\n",
    "\n",
    "### Q6 Tasks:\n",
    "\n",
    "* __a) Multiple Choice:__ Assume we estimate the following probabilities from a collection of SPAM emails and HAM emails. We limit the vocabulary of the Naive Bayes model to the following keywords: Urgent, Sale, Hello.  In the following $Pr(word)$ indicates the probability of that word appearing in any email.\n",
    "\n",
    "* $Pr(Urgent) = 50\\%,$\n",
    "* $Pr(Sale) = 8\\%,$\n",
    "* $Pr(Hello) = 20\\%$\n",
    "    \n",
    "We also know that spam is $40\\%$ of all email, and $80\\%$ of spam email contains “Urgent”, i.e, $Pr(Urgent|SPAM) = 0.8.$\n",
    "\n",
    "Given a multinomial Naive Bayes model learnt from this data with no smoothing, what is the probability that an email is a spam if it contains only one word that is our vocabulary, that of,  “Urgent”?  I.e., $Pr(SPAM|X=Urgent)$ = ?????\n",
    "\n",
    "`HINT`: The [law of total probabilities](https://en.wikipedia.org/wiki/Law_of_total_probability) is used to calculate the denominator for a multinomial Naive Bayes Classifier. But we don't need to use that rule here since our document contains just a single word in the model vocabulary i.e., it contains the word \"Urgent\". Plus we know the $Pr(X) = Pr(Urgent) = 0.5$. Having said that what is $Pr(Urgent|HAM) = 0.3$ (just out of curiosity's sake; this is not needed in this quiz).\n",
    "\n",
    "\n",
    "* $Pr(Urgent) = 0.4 * 0.8 + 0.6 * Pr(Urgent|HAM)$\n",
    "* $0.5 = 0.4 * 0.8 + 0.6 * Pr(Urgent|HAM)$\n",
    "* $Pr(Urgent|HAM) = 0.3$\n",
    "\n",
    "Ordinarily, we tend to not use the denominator in our calculation for classification tasks (as it is common to all the classes). But if we need probabilities then we would calculate this quantity.\n",
    "\n",
    "QUESTION: \n",
    "\n",
    "What is $Pr(SPAM|X=Urgent)$?\n",
    "\n",
    "\n",
    "* __b) Numerical Input:__ Given the following training dataset of 5 documents for a 2 Class problem: HAM versus SPAM.\n",
    "\n",
    "**Training Data**\n",
    "\n",
    "\n",
    "|DocId |Class | Document String\n",
    "|---|---|---|\n",
    "|d1 | HAM | good\n",
    "|d2 | SPAM | very good\n",
    "|d3 | SPAM | good bad\n",
    "|d4 | HAM | very bad\n",
    "|d5 | SPAM | very bad very good\n",
    "\n",
    "\n",
    "The vocabulary of the dataset is [good, very, bad]. The word class conditionals are calculated in the following table(without smoothing). \n",
    "\n",
    "Please fill in the blanks with appropriate answers in the following table:\n",
    "\n",
    "\n",
    "| Word | $\\text{Pr(word|HAM)}$ | $\\text{Pr(word|SPAM)}$ |\n",
    "|---|---|---|\n",
    "| good | $1/3$ | $3/8$ \n",
    "| very | $1/3$ | $3/8$ \n",
    "| bad | \\[Blank_1\\] | \\[Blank_2\\] \n",
    "\n",
    "Please submit your response as a fraction using \"/\" or as a decimal with at least 2 decimal places. Example: 1/16 or 0.06\n",
    "\n",
    "\n",
    "* __c) Numerial Input:__ In this question learn a multinomial Naive Bayes model using all the training data. Please use unigrams features (i.e., single words, like \"very\", \"bad\").  You would then learn the Pr(bad | SPAM) and Pr(bad | ham).  Note: please do not use any higher-order features such as bigrams (e.g., \"very bad\"). \n",
    "\n",
    "Given the following training corpus of documents for a two Class problem: __HAM__ versus __SPAM__.\n",
    "\n",
    "__Training Data__:\n",
    "\n",
    "| DocId | Class | Document String\n",
    "|---|---|---|\n",
    "| d1 | HAM | good\n",
    "| d2 | SPAM | very good\n",
    "| d3 | SPAM | good bad\n",
    "| d4 | HAM | very bad \n",
    "| d5 | SPAM | very bad very good\n",
    "\n",
    "and a test data set consisting of a single test case:\n",
    "\n",
    "__Test Data__\n",
    "\n",
    "| DocId | Class | Document String\n",
    "|---|---|---|\n",
    "| d6 | ?? | good bad very\n",
    "\n",
    "__TASK:__ Learn a multinomial Naive Bayes model with Laplace (plus one) smoothing using all the training data. \n",
    "\n",
    "Given a test document $d6$ calculate the posterior probability for __HAM__ using the learned model.\n",
    "\n",
    "* $Pr(HAM | d6) = ?? \\% $ \n",
    "\n",
    "Recall at a high level the posterior probability of, say, $Pr(HAM|d6)$ is a follows:\n",
    "\n",
    "$Pr(HAM|d6) = Pr(d6|HAM)/( Pr(d6|HAM) + Pr(d6|SPAM))$\n",
    "\n",
    "where $Pr(HAM|d6)$ and $Pr(SPAM|d6)$ can be calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(C_k \\mid x_1, \\dots, x_n)\n",
    "&=\\frac{ p( x_1, \\dots, x_n\\mid C_k)p(C_k) }{p( x_1, \\dots, x_n)}\\\\\n",
    "&=\\frac{  \\prod_{i=1}^n p(x_i \\mid C_k)p(C_k) }{p( x_1, \\dots, x_n)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "\n",
    "$x_1, \\dots, x_n$ are the words in d6, and $C_k$ is the class label (HAM or SPAM).\n",
    "\n",
    "The formula given above is a generic formula. For our example it will be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned} \n",
    "p(HAM \\mid good, bad, very)\n",
    "&=\\frac{  p(HAM)\\prod_{i=1}^3 p(w_i \\mid HAM) }{p( good, bad, very)}\\\\\n",
    "&=\\frac{  p(HAM)\\prod_{i=1}^3 p(w_i \\mid HAM) }{(p(HAM) \\prod_{i=1}^3 p(w_i \\mid HAM)) +(p(SPAM) \\prod_{i=1}^3 p(w_i \\mid SPAM))}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Here $w_i$ will be the $i^{th}$ word $\\text{d6}$.\n",
    "\n",
    "Sometimes the above equations are simplified for the purposes of classification as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p(C_k \\mid x_1, \\dots, x_n)\n",
    "& \\varpropto p(C_k, x_1, \\dots, x_n) \\\\\n",
    "& \\varpropto p(C_k) \\ p(x_1 \\mid C_k) \\ p(x_2 \\mid C_k) \\ p(x_3 \\mid C_k) \\ \\cdots \\\\\n",
    "& \\varpropto p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k) \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "__Please report the probability, $Pr(HAM|d6)$, as an integer percentage (please round).__\n",
    "\n",
    "For example, if $Pr(HAM|d6)  = 0.709493671$ then the $Pr(HAM|d6)$ should be reported as $71$. Please input $71$ for your response.\n",
    "\n",
    "You can calculate these probabilities by hand and you can verify your calculations by running the code given below.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "* __d) Short Essay:__ Equation 13.3 in Manning, Raghavan and Shutze shows how a Multinomial Naive Bayes model classifies a document. It predicts the class, $c$, for which the estimated conditional probability of the class given the document's contents,  $\\hat{P}(c|d)$, is greatest. In this equation what two pieces of information are required to calculate  $\\hat{P}(c|d)$? Your answer should include both mathematical notatation and verbal explanation.\n",
    "\n",
    "* __e) Short Essay:__ The Enron data includes two classes of documents: `spam` and `ham` (they're actually labeled `1` and `0`). In plain English, explain what  $\\hat{P}(c)$ and  $\\hat{P}(t_{k} | c)$ mean in the context of this data. How would we estimate these values from a training corpus?\n",
    "\n",
    "* __f) Multiple Choice:__ How many passes over the data would we need to make to retrieve this information for all classes and all words?\n",
    "\n",
    "* __g) Hand Calculations:__ Above we've reproduced the document classification example from the textbook (we added an empty subject field to mimic the Enron data format). Remember that the classes in this \"Chinese Example\" are `1` (about China) and `0` (not about China). Calculate the class priors and the conditional probabilities for an __unsmoothed__ Multinomial Naive Bayes model trained on this data. Show the calculations that lead to your result using markdown and $\\LaTeX$ in the space provided or by embedding an image of your hand written work. [`NOTE:` _Your results should NOT match those in the text -- they are training a model with +1 smoothing you are training a model without smoothing_]\n",
    "\n",
    "    The following is a sample table in Latex. Please feel free to adapt as needed:\n",
    "\n",
    "    $$\n",
    "    \\begin{matrix} \n",
    "    Word & Freq(word in China Docs) & Freq(word in NOT China Docs) & Pr(w_i|y=China) &Pr(w_i|y= NOT China)\\\\\n",
    "    beijing    & 0&0&0&0\\\\\n",
    "     chinese  & 0&0&0&0\\\\\n",
    "     tokyo      & 0&0&0&0\\\\\n",
    "     shanghai   & 0&0&0&0\\\\\n",
    "     japan      & 0&0&0&0\\\\\n",
    "     macao      & 0&0&0&0\\\\\n",
    "    CLASS PRIORS    & 0&0&0&0\\\\\n",
    "    \\end{matrix}\n",
    "    $$\n",
    "\n",
    "* __h) Hand Calculations:__ Use the model you trained to classify the following test document: `Chinese Chinese Chinese Tokyo Japan`. Show the calculations that lead to your result using markdown and $\\LaTeX$ in the space provided or by embedding an image of your hand written work.\n",
    "\n",
    "    $$\n",
    "    \\begin{align} \n",
    "    Pr(Class| Doc = D_5)  &\\approx \\underset{c_{j} \\in \\{China, not China\\}}{\\operatorname{\\text{arg}max}}  P(Class=c_{j}| Doc = D_5) \\\\\n",
    "                &\\approx \\underset{c_{j} \\in \\{China, not China\\}}{\\operatorname{\\text{arg}max}}  P(Class=c_{j}) \\prod_{w_i \\in Doc=D_5 }P(w_{i}|c_{j})\\\\\n",
    "                 &\\approx \\underset{c_{j} \\in \\{China, not China\\}}{\\operatorname{\\text{arg}max}} (\\frac{..}{...}, ..)\\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "\n",
    "* __i) Short Essay:__ Compare the classification you get from this unsmoothed model in `g`/`h` to the results in the textbook's \"Example 1\" which reflects a model with Laplace plus 1 smoothing. How does smoothing affect our inference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q6a\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: See whole question above. What is Pr(SPAM|X=Urgent)?\n",
    "\n",
    "#   a.) 64%\n",
    "#   b.) 32%\n",
    "#   c.) 8%\n",
    "#   d.) 15%\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n"
     ]
    }
   ],
   "source": [
    "# q6b1\n",
    "### NUMERICAL INPUT\n",
    "### QUESTION: See whole question above if necessary. What is `Blank_1`?\n",
    "\n",
    "# | Word | Pr(word|HAM)| Pr(word|SPAM)\n",
    "# | ---  | ---         | ---\n",
    "# | good | 1/3         | 3/8 \n",
    "# | very | 1/3         | 3/8 \n",
    "# | bad  | Blank_1     | Blank_2\n",
    "\n",
    "\n",
    "### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. USE THE DECIMAL, NOT THE FRACTION. (i.e. \"0.06\", NOT \"1/16\")\n",
    "#       Please submit your response as a DECIMAL with at least 2 decimal places (as shown above)\n",
    "\n",
    "answer = \"0.33\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# q6b2\n",
    "### NUMERICAL INPUT\n",
    "### QUESTION: See whole question above if necessary. What is `Blank_2`?\n",
    "\n",
    "# | Word | Pr(word|HAM)| Pr(word|SPAM)\n",
    "# | ---  | ---         | ---\n",
    "# | good | 1/3         | 3/8 \n",
    "# | very | 1/3         | 3/8 \n",
    "# | bad  | Blank_1     | Blank_2\n",
    "\n",
    "\n",
    "### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. USE THE DECIMAL, NOT THE FRACTION. (i.e. \"0.06\", NOT \"1/16\")\n",
    "#       Please submit your response as a DECIMAL with at least 2 decimal places (as shown above)\n",
    "\n",
    "answer = \"0.25\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Class  bad  good  very\n",
      "d1      0    0     1     0\n",
      "d2      1    0     1     1\n",
      "d3      1    1     1     0\n",
      "d4      0    1     0     1\n",
      "d5      1    1     1     2\n",
      "model_priors: [0.4 0.6]\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "Pr(w_i|ham):  [0.333 0.333 0.333]\n",
      "Pr(w_i|spam):  [0.273 0.364 0.364]\n",
      "    bad  good  very\n",
      "d3    1     1     1\n",
      "likelihood Pr(d6|ham): 0.037037037037037035\n",
      "likelihood Pr(d6|SPAM): 0.03606311044327573\n",
      "unnormalized Pr(D6|ham)*Pr(ham) is : 0.01481\n",
      "unnormalized Pr(D6|SPAM)*Pr(SPAM) is : 0.02164\n",
      "Posterior Probabilities in % is: Pr(Ham|D6) is :      41\n",
      "Posterior Probabilities in % is: Pr(SPAM|D6) is :      59\n"
     ]
    }
   ],
   "source": [
    "# Notebook Only - DON'T REMOVE THIS\n",
    "# part c\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "vocabulary = [\"bad\", \"good\", \"very\"]\n",
    "\n",
    "# Document by term matrix\n",
    "doc_per_term= np.array([[0, 1, 0 ],[0, 1, 1],[1, 1, 0],[1, 0, 1],[1, 1, 2]])\n",
    "\n",
    "# y_train: 0 for Ham and 1 for spam\n",
    "class_per_doc= np.array([0,1,1,0,1])\n",
    "print(pd.DataFrame(np.c_[class_per_doc, doc_per_term], index = [\"d1\", \"d2\",\"d3\", \"d4\", \"d5\"],columns = [\"Class\"]+ vocabulary))\n",
    "\n",
    "## Learn the Naïve Bayes Classification:\n",
    "model_priors = np.bincount(class_per_doc)/ len(class_per_doc)\n",
    "print(f\"model_priors: {model_priors}\")\n",
    "\n",
    "\n",
    "# Calculate Pr(w_i|ham) aka ham  class conditionals\n",
    "print (doc_per_term[class_per_doc==0,:])\n",
    "model_data_given_ham= (np.sum(doc_per_term[class_per_doc==0,:],axis=0)+1)/(np.sum(doc_per_term[class_per_doc==0,:]) +len(vocabulary))\n",
    "print(f\"Pr(w_i|ham):  {np.round(model_data_given_ham, 3)}\")\n",
    "\n",
    "# Calculate Pr(w_i|spam) aka SPAM class conditionals:\n",
    "model_data_given_spam= (np.sum(doc_per_term[class_per_doc==1,:],axis=0)+1)/(np.sum(doc_per_term[class_per_doc==1,:]) +len(vocabulary))\n",
    "print(f\"Pr(w_i|spam):  {np.round(model_data_given_spam, 3)}\")\n",
    "\n",
    "\n",
    "# Test document terms are: bad, good, very\n",
    "d6 = [1, 1, 1] #TEST DOCUMENT\n",
    "print(pd.DataFrame([d6], index = [\"d3\"], columns = vocabulary))\n",
    "\n",
    "# Naïve Bayes Classification\n",
    "# Likelihood\n",
    "# Applying the Unigram Language Model\n",
    "# Calculate Posterior Probabilities using the learnt Naive Bayes Model\n",
    "print(f\"likelihood Pr(d6|ham): {np.prod(np.power(model_data_given_ham, d6))}\")\n",
    "print(f\"likelihood Pr(d6|SPAM): {np.prod(np.power(model_data_given_spam, d6))}\")\n",
    "\n",
    "pr_ham = np.prod(np.power(model_data_given_ham, d6)) * model_priors[0]\n",
    "pr_spam = np.prod(np.power(model_data_given_spam, d6))* model_priors[1]\n",
    "print(f\"unnormalized Pr(D6|ham)*Pr(ham) is : {pr_ham:7.5f}\")\n",
    "print(f\"unnormalized Pr(D6|SPAM)*Pr(SPAM) is : {pr_spam:7.5f}\")\n",
    "\n",
    "print(f\"Posterior Probabilities in % is: Pr(Ham|D6) is : {100*pr_ham/(pr_spam+pr_ham):7.0f}\")\n",
    "print(f\"Posterior Probabilities in % is: Pr(SPAM|D6) is : {100*pr_spam/(pr_spam+pr_ham):7.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# q6c\n",
    "### NUMERICAL INPUT\n",
    "### QUESTION: See whole question above if necessary. Please report the probability, Pr(HAM|d6),\n",
    "#             as an integer percentage (please round).\n",
    "\n",
    "### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. (i.e. an answer of 0.709493671 should be entered as \"71\")\n",
    "answer = \"41\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To calculate the estimated probability 𝑃̂_hat(c|d) in Equation 13.3, we need two pieces of information:\n",
      "\n",
      "1. The class prior probability, 𝑃(c) = (number of documents in class c) / (total number of documents), which represents how likely each class is overall in the training data.  \n",
      "2. The conditional probability of each word in the document given the class, 𝑃(w_i|c) = (count of word w_i in class c documents) / (total words in class c documents), which represents how likely each word w_i in the document is to appear in documents of class c.  \n",
      "\n",
      "For a document \"d\" containing words w_1, w_2, ..., w_n, the model estimates:\n",
      "\n",
      "𝑃̂(c|d) ∝ 𝑃(c) × ∏_{i=1}^{n} 𝑃(w_i|c)\n",
      "\n",
      "That is, we multiply the prior probability of the class by the likelihood of observing all the words in the document in that class. \n",
      "The class with the highest value is chosen as the predicted class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q6d\n",
    "### SHORT ESSAY\n",
    "### QUESTION: See whole question above if necessary.\n",
    "#             Equation 13.3 in Manning, Raghavan, and Shutze  (included above for convenience) shows how a Multinomial\n",
    "#             Naive Bayes model classifies a document. It predicts the class, 'c' for which the estimated conditional probability\n",
    "#             of the class given the document's contents, P_hat(c|d), is greatest. In this equation, what two pieces of information\n",
    "#             are required to calculate P_hat(c|d)? Your answer should include both mathematical notation and verbal explanation.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "To calculate the estimated probability 𝑃̂_hat(c|d) in Equation 13.3, we need two pieces of information:\n",
    "\n",
    "1. The class prior probability, 𝑃(c) = (number of documents in class c) / (total number of documents), which represents how likely each class is overall in the training data.  \n",
    "2. The conditional probability of each word in the document given the class, 𝑃(w_i|c) = (count of word w_i in class c documents) / (total words in class c documents), which represents how likely each word w_i in the document is to appear in documents of class c.  \n",
    "\n",
    "For a document \"d\" containing words w_1, w_2, ..., w_n, the model estimates:\n",
    "\n",
    "𝑃̂(c|d) ∝ 𝑃(c) × ∏_{i=1}^{n} 𝑃(w_i|c)\n",
    "\n",
    "That is, we multiply the prior probability of the class by the likelihood of observing all the words in the document in that class. \n",
    "The class with the highest value is chosen as the predicted class.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the context of the Enron email data, 𝑃̂_hat(c) means the estimated probability of a class, either\n",
      "spam (1) or ham (0), based on the training data, which in plain English tells us how common each class\n",
      "is overall. It can be estimated by counting the number of emails in each class and dividing by the\n",
      "total number of emails in the training set. Similarly, 𝑃̂(t_k|c) means the estimated probability of a\n",
      "specific word t_k occurring given a class c, which tells us how likely a given word is to appear in\n",
      "spam emails versus ham emails. It can be estimated by counting how many times that word appears in\n",
      "all documents of class c and dividing by the total number of words in class c documents (or possibly \n",
      "adding 1 if using Laplace smoothing). These probabilities allow a Naive Bayes model to predict the\n",
      "likelihood that a new email belongs to spam or ham based on the words it contains.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q6e\n",
    "### SHORT ESSAY\n",
    "### QUESTION: See whole question above if necessary.\n",
    "#             The Enron data includes two classes of documents: spam and ham (they're actually labeled 1 and 0).\n",
    "#             In plain English, explain what P_hat(c) and P_hat(t_k|c) means in the context of this data. How would\n",
    "#             we estimate these values from a training corpus?\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "In the context of the Enron email data, 𝑃̂_hat(c) means the estimated probability of a class, either\n",
    "spam (1) or ham (0), based on the training data, which in plain English tells us how common each class\n",
    "is overall. It can be estimated by counting the number of emails in each class and dividing by the\n",
    "total number of emails in the training set. Similarly, 𝑃̂(t_k|c) means the estimated probability of a\n",
    "specific word t_k occurring given a class c, which tells us how likely a given word is to appear in\n",
    "spam emails versus ham emails. It can be estimated by counting how many times that word appears in\n",
    "all documents of class c and dividing by the total number of words in class c documents (or possibly \n",
    "adding 1 if using Laplace smoothing). These probabilities allow a Naive Bayes model to predict the\n",
    "likelihood that a new email belongs to spam or ham based on the words it contains.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "# q6f\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: How many passes over the data would we need to make to retrieve this information for all classes and all words?\n",
    "\n",
    "#   a.) We'll need three passes over the data because we need to count the words (first pass), count each word's occurrence (second pass) and then compute the probabilities (third pass)\n",
    "#   b.) We only need one pass over the data to tally up the information for these prior & conditional probabilities... however after counting each word's occurrences in each class we will need to go on to divide by the class totals which is a little extra work after completing the pass over the data.\n",
    "#   c.) We'll need two passes over the data as we need to compute the totals before we can calculate the probabilities.\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"b\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part G My work in Markdown:\n",
    "Training data:\n",
    "\n",
    "| DocID | Class | Body                     |\n",
    "| ----- | ----- | ------------------------ |\n",
    "| Doc1  | 1     | Chinese Beijing Chinese  |\n",
    "| Doc2  | 1     | Chinese Chinese Shanghai |\n",
    "| Doc3  | 1     | Chinese Macao            |\n",
    "| Doc4  | 0     | Tokyo Japan Chinese      |\n",
    "\n",
    "We will mimic the tetbook except calculate frequencies and probabilities without smoothing:\n",
    "\n",
    "* Class counts:\n",
    "\n",
    "  * Class 1 (China): 3 documents\n",
    "  * Class 0 (NOT China): 1 document\n",
    "  * Priors:\n",
    "\n",
    "    * Pr(y=1) = 3/4\n",
    "    * Pr(y=0) = 1/4\n",
    "\n",
    "* Word counts:\n",
    "\n",
    "  * Chinese: 2+2+1=5 times in Class 1, 1 time in Class 0\n",
    "  * Beijing: 1 in Class 1, 0 in Class 0\n",
    "  * Shanghai: 1 in Class 1, 0 in Class 0\n",
    "  * Macao: 1 in Class 1, 0 in Class 0\n",
    "  * Tokyo: 0 in Class 1, 1 in Class 0\n",
    "  * Japan: 0 in Class 1, 1 in Class 0\n",
    "\n",
    "* Conditional probabilities:\n",
    "\n",
    "  For each word, $Pr(word|class) = \\frac{\\text{count of word in class}}{\\text{total words in class}}$\n",
    "\n",
    "  * Class 1 total words = 2+2+2=8\n",
    "    Word counts in Class 1:\n",
    "\n",
    "    * Beijing: 1 → Pr = 1/8\n",
    "    * Chinese: 5 → Pr = 5/8\n",
    "    * Shanghai: 1 → Pr = 1/8\n",
    "    * Macao: 1 → Pr = 1/8\n",
    "    * Tokyo: 0 → Pr = 0\n",
    "    * Japan: 0 → Pr = 0\n",
    "\n",
    "  * Class 0 total words: 3\n",
    "    Word counts in Class 0:\n",
    "\n",
    "    * Tokyo: 1 →  Pr = 1/3\n",
    "    * Japan: 1 →  Pr = 1/3\n",
    "    * Chinese: 1 →  Pr = 1/3\n",
    "    * Beijing, Shanghai, Macao: 0 →  Pr = 0\n",
    "\n",
    "Table filled:\n",
    " $$\n",
    "    \\begin{matrix} \n",
    "    Word & Freq(word in China Docs) & Freq(word in NOT China Docs) & Pr(w_i|y=China) &Pr(w_i|y= NOT China)\\\\\n",
    "    beijing    & 1&0&1/8&0\\\\\n",
    "     chinese  & 5&1&5/8&1/3\\\\\n",
    "     tokyo      & 0&1&0&1/3\\\\\n",
    "     shanghai   & 1&0&1/8&0\\\\\n",
    "     japan      & 0&1&0&1/3\\\\\n",
    "     macao      & 1&0&1/8&0\\\\\n",
    "    CLASS PRIORS    & 3&1&3/4&1/4\\\\\n",
    "    \\end{matrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0.125, 0], [5, 1, 0.625, 0.3333333333333333], [0, 1, 0, 0.3333333333333333], [1, 0, 0.125, 0], [0, 1, 0, 0.3333333333333333], [1, 0, 0.125, 0], [3, 1, 0.75, 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# q6g\n",
    "### HAND CALCULATIONS / REPLACE LIST VALUES\n",
    "### QUESTION: See whole question above if necessary.\n",
    "#             Above, we've reproduced the document classification example from the textbook (we added an empty subject\n",
    "#             field to mimic the Enron data format). Remember that the classes in this \"Chinese Example\" are 1 (about China)\n",
    "#             and 0 (not about China). Calculate the class priors and the conditional probabilities for an unsmoothed Multinomial\n",
    "#             Naive Bayes model trained on this data. Fill in the answers below:\n",
    "\n",
    "#             [NOTE: Your results should NOT match those in the text -- they are trained  with +1 smoothing.\n",
    "#             You are training a model without smoothing]\n",
    "\n",
    "### ENTER ONLY THE **FRACTION**. DO NOT ENTER THE DECIMAL.\n",
    "\n",
    "\n",
    "word =    [\"freq_wordinChinaDocs\", \"freq_wordinNOTChinaDocs\", \"pr_wi_given_y_eq_China\", \"pr_wi_given_y_neq_China\"]\n",
    "beijing =      [1,                       0,                          1/8,                      0]\n",
    "chinese =      [5,                       1,                          5/8,                      1/3]\n",
    "tokyo =        [0,                       1,                          0,                      1/3]\n",
    "shanghai =     [1,                       0,                          1/8,                      0]\n",
    "japan =        [0,                       1,                          0,                      1/3]\n",
    "macao =        [1,                       0,                          1/8,                      0]\n",
    "class_priors = [3,                       1,                          3/4,                      1/4]\n",
    "\n",
    "#####################\n",
    "# DO NOT MODIFY\n",
    "\n",
    "all = [beijing, chinese, tokyo, shanghai, japan, macao, class_priors]\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SEE ANSWER BELOW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q6h\n",
    "### HAND CALCULATIONS\n",
    "### QUESTION: See whole question above if necessary.\n",
    "###           Use the model you trained to classify the following test document: Chinese Chinese Chinese Tokyo Japan.\n",
    "#             Show the calculations that lead to your result inside the print statement below\n",
    "#             or by uploading an image called 'q6h.<png|jpeg|etc.>'. For example: 'q6h.png'.\n",
    "\n",
    "### IF YOU UPLOADED AN IMAGE, ENTER THE FILENAME INSIDE THE PRINT STATEMENT. IF YOU WANT TO WRITE IN LATEX, CREATE A MARKDOWN CELL\n",
    "###     BELOW AND PUT 'SEE ANSWER BELOW' INSIDE THE PRINT STATEMENT\n",
    "print(\n",
    "\"\"\"\n",
    "SEE ANSWER BELOW\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6h: My Calculations\n",
    "\n",
    "We classify the test document:\n",
    "\n",
    "$$D_5 = \\text{Chinese Chinese Chinese Tokyo Japan}$$ using the unsmoothed Multinomial Naive Bayes model.\n",
    "\n",
    "Class priors:\n",
    "\n",
    "$$P(y = \\text{China}) = \\frac{3}{4}, \\quad P(y \\neq \\text{China}) = \\frac{1}{4}$$\n",
    "\n",
    "Conditional probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{Chinese} \\mid \\text{China}) &= \\frac{5}{8}, & P(\\text{Chinese} \\mid \\text{NOT China}) &= \\frac{1}{3} \\\\\n",
    "P(\\text{Beijing} \\mid \\text{China}) &= \\frac{1}{8}, & P(\\text{Beijing} \\mid \\text{NOT China}) &= 0 \\\\\n",
    "P(\\text{Shanghai} \\mid \\text{China}) &= \\frac{1}{8}, & P(\\text{Shanghai} \\mid \\text{NOT China}) &= 0 \\\\\n",
    "P(\\text{Macao} \\mid \\text{China}) &= \\frac{1}{8}, & P(\\text{Macao} \\mid \\text{NOT China}) &= 0 \\\\\n",
    "P(\\text{Tokyo} \\mid \\text{China}) &= 0, & P(\\text{Tokyo} \\mid \\text{NOT China}) &= \\frac{1}{3} \\\\\n",
    "P(\\text{Japan} \\mid \\text{China}) &= 0, & P(\\text{Japan} \\mid \\text{NOT China}) &= \\frac{1}{3} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now I compute the unnormalized posterior probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{China} \\mid D_5) &\\propto P(y = \\text{China}) \\cdot P(\\text{Chinese} \\mid \\text{China})^3 \\cdot P(\\text{Tokyo} \\mid \\text{China}) \\cdot P(\\text{Japan} \\mid \\text{China}) \\\\\n",
    "&= \\frac{3}{4} \\cdot \\left(\\frac{5}{8}\\right)^3 \\cdot 0 \\cdot 0 = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{NOT China} \\mid D_5) &\\propto P(y \\neq \\text{China}) \\cdot P(\\text{Chinese} \\mid \\text{NOT China})^3 \\cdot P(\\text{Tokyo} \\mid \\text{NOT China}) \\cdot P(\\text{Japan} \\mid \\text{NOT China}) \\\\\n",
    "&= \\frac{1}{4} \\cdot \\left(\\frac{1}{3}\\right)^3 \\cdot \\frac{1}{3} \\cdot \\frac{1}{3} \\\\\n",
    "&= \\frac{1}{4} \\cdot \\frac{1}{27} \\cdot \\frac{1}{9} = \\frac{1}{972}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, we do the classification:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{c \\in \\{\\text{China}, \\text{NOT China}\\}} P(c \\mid D_5) = \\text{NOT China}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I believe the question here is incorrect, we should be referencing the question written in the markdown instead.\n",
      "In our unsmoothed model (g and h above), the test document 'Chinese Chinese Chinese Tokyo Japan'\n",
      "was classified as NOT China because the words 'Tokyo' and 'Japan' had zero probability under the\n",
      "China class, which drove the entire probability for that class down to zero despite the overall large\n",
      "amount of 'Chinese'. In contrast, Example 13.1 in the textbook applies Laplace +1 smoothing, which\n",
      "makes every word in the vocabulary have at least a small probability in each class. That is, our 0\n",
      "probabilities for the unsmoothed model now have probabilities > 0 with smoothing. Thus, the China\n",
      "class does not get wiped out by the unseen words, and the strong evidence from three occurrences of\n",
      "'Chinese' in China class outweighs the two negative indicators ('Tokyo' and 'Japan'). With smoothing,\n",
      "the model now correctly labels the test document as China. Smoothing prevents zero probabilities from\n",
      "dominating the final result, leading to more balanced and realistic classifications, which is\n",
      "especially useful when the training data is small or incomplete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q6i\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Use the model you trained to classify the following test document: Chinese Chinese Chinese Tokyo Japan.\n",
    "#             Show the calculations that lead to your result inside the print statement below\n",
    "#             or by uploading an image called 'q6i.<png|jpeg|etc.>'. For example: 'q6i.png'.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "I believe the question here is incorrect, we should be referencing the question written in the markdown instead.\n",
    "In our unsmoothed model (g and h above), the test document 'Chinese Chinese Chinese Tokyo Japan'\n",
    "was classified as NOT China because the words 'Tokyo' and 'Japan' had zero probability under the\n",
    "China class, which drove the entire probability for that class down to zero despite the overall large\n",
    "amount of 'Chinese'. In contrast, Example 13.1 in the textbook applies Laplace +1 smoothing, which\n",
    "makes every word in the vocabulary have at least a small probability in each class. That is, our 0\n",
    "probabilities for the unsmoothed model now have probabilities > 0 with smoothing. Thus, the China\n",
    "class does not get wiped out by the unseen words, and the strong evidence from three occurrences of\n",
    "'Chinese' in China class outweighs the two negative indicators ('Tokyo' and 'Japan'). With smoothing,\n",
    "the model now correctly labels the test document as China. Smoothing prevents zero probabilities from\n",
    "dominating the final result, leading to more balanced and realistic classifications, which is\n",
    "especially useful when the training data is small or incomplete.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Naive Bayes Inference\n",
    "\n",
    "In the next two questions you'll write code to parallelize the Naive Bayes calculations that you performed above. We'll do this in two phases: one MapReduce job to perform training and a second MapReduce to perform inference. While in practice we'd need to train a model before we can use it to classify documents, for learning purposes we're going to develop our code in the opposite order. By first focusing on the pieces of information/format we need to perform the classification (inference) task you should find it easier to develop a solid implementation for training phase when you get to question 8 below. In both of these questions we'll continue to use the Chinese example corpus from the textbook to help us test our MapReduce code as we develop it. Below we've reproduced the corpus, test set and model in text format that matches the Enron data.\n",
    "\n",
    "### Q7 Tasks:\n",
    "\n",
    "* __a) short essay:__ run the provided cells to create the example files and load them in to HDFS. Then take a closer look at __`NBmodel.txt`__. This text file represents a Naive Bayes model trained (with Laplace +1 smoothing) on the example corpus. What are the 'keys' and 'values' in this file? Which record means something slightly different than the rest? The value field of each record includes two numbers which will be helpful for debugging but which we don't actually need to perform inference -- what are they? [`HINT`: _This file represents the model from Example 13.1 in the textbook, if you're having trouble getting oriented try comparing our file to the numbers in that example._]\n",
    "\n",
    "* __b) short essay:__ When performing Naive Bayes in practice instead of multiplying the probabilities (as in equation 13.3) we add their logs (as in equation 13.4). Why do we choose to work with log probabilities? If we had an unsmoothed model, what potential error could arise from this transformation?\n",
    "\n",
    "* __c) multiple choice:__ Documents 6 and 8 in the test set include a word that did not appear in the training corpus (and as a result does not appear in the model). What should we do at inference time when we need a class conditional probability for this word?\n",
    "\n",
    "* __d) multiple choice:__ The goal of our MapReduce job is to stream over the test set and classify each document by peforming the calculation from equation 13.4. To do this we'll load the model file (which contains the probabilities for equation 13.4) into memory on the nodes where we do our mapping. This is called an in-memory join. Does loading a model 'state' like this depart from the functional programming principles? Explain why or why not. From a scability perspective when would this kind of memory use be justified? When would it be unwise?\n",
    "\n",
    "* __e) code:__ Complete the code in __`NaiveBayes/classify_mapper.py`__. Read the docstring carefully to understand how this script should work and the format it should return. Run the provided unit tests to confirm that your script works as expected then write a Hadoop streaming job to classify the Chinese example test set. [`HINT 1:` _you shouldn't need a reducer for this one._ `HINT 2:` _Don't forget to add the model file to the_ `-files` _parameter in your Hadoop streaming job so that it gets shipped to the mapper nodes where it will be accessed by your script._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to create the example corpus and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NBmodel.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NBmodel.txt\n",
    "beijing\t0.0,1.0,0.111111111111,0.142857142857\n",
    "chinese\t1.0,5.0,0.222222222222,0.428571428571\n",
    "tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "shanghai\t0.0,1.0,0.111111111111,0.142857142857\n",
    "ClassPriors\t1.0,3.0,0.25,0.75\n",
    "japan\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "macao\t0.0,1.0,0.111111111111,0.142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/chineseTrain.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/chineseTest.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# load the data files into HDFS\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTrain.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTest.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In NBModel.txt, each line in here is a key-value pair representing part of the trained Naive Bayes\n",
      "model. I believe the keys are words from the vocabulary, such as 'beijing', 'chinese', and 'tokyo'.\n",
      "The values consist of four numbers: the first number is the frequency count of that word in documents\n",
      "labeled 0 (Not China), the second number is the frequency count in documents labeled 1 (China), the\n",
      "third number is the conditional probability P(word|class=Not China), and the fourth number is\n",
      "P(word|class=China). The third and fourth numbers are what we actually use for inference.\n",
      "Most records correspond to individual words and their conditional probabilities, while the\n",
      "'ClassPriors' record is slightly different because it represents the prior probabilities of the classes\n",
      "rather than word probabilities. The first two numbers in each record are probably for debugging and are\n",
      "not needed for the inference step.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q7a\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Run the provided cells to create the example files and load them into HDFS. Then take a\n",
    "#             closer look at NBmodel.txt. This text file represents a Naive Bayes model trained\n",
    "#             (with Laplace +1 smoothing) on the example corpus. What are the 'keys' and 'values'\n",
    "#             in this file? Which record means something slightly different than the rest? The value\n",
    "#             field of each record includes two numbers that will be helpful for debugging but which\n",
    "#             we don't actually need to perform inference -- what are they? [HINT: This file represents\n",
    "#             the model from Example 13.1 in the textbook, if you're having trouble getting oriented\n",
    "#             try comparing our file to the numbers in that example.]\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "In NBModel.txt, each line in here is a key-value pair representing part of the trained Naive Bayes\n",
    "model. I believe the keys are words from the vocabulary, such as 'beijing', 'chinese', and 'tokyo'.\n",
    "The values consist of four numbers: the first number is the frequency count of that word in documents\n",
    "labeled 0 (Not China), the second number is the frequency count in documents labeled 1 (China), the\n",
    "third number is the conditional probability P(word|class=Not China), and the fourth number is\n",
    "P(word|class=China). The third and fourth numbers are what we actually use for inference.\n",
    "Most records correspond to individual words and their conditional probabilities, while the\n",
    "'ClassPriors' record is slightly different because it represents the prior probabilities of the classes\n",
    "rather than word probabilities. The first two numbers in each record are probably for debugging and are\n",
    "not needed for the inference step.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We choose to work with log probabilities because multiplying many small probabilities can quickly\n",
      "result in the product becoming so small that it cannot be accurately represented by the computer\n",
      "(floating point underflow as the textbook says). By taking the logarithm of each probability,\n",
      "multiplication becomes addition, which is much more stable and prevents this issue. Using logs also\n",
      "makes the calculations easier to manage and allows us to compare values without computing extremely\n",
      "tiny products. However, if we have an unsmoothed model and a word occurs in the test document but never\n",
      "appeared in the training data for a given class, its probability is zero. Taking the logarithm of zero\n",
      "is undefined, which would cause a computation error and prevent us from obtaining a valid result for\n",
      "that class. Thus, smoothing is important when using log probabilities in Naive Bayes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q7b\n",
    "### SHORT ESSAY\n",
    "### QUESTION: When performing Naive Bayes in practice instead of multiplying the probabilities\n",
    "#             (as in equation 13.3) we add their logs (as in equation 13.4).\n",
    "#             Why do we choose to work with log probabilities? If we had an unsmoothed model,\n",
    "#             what potential error could arise from this transformation?\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "We choose to work with log probabilities because multiplying many small probabilities can quickly\n",
    "result in the product becoming so small that it cannot be accurately represented by the computer\n",
    "(floating point underflow as the textbook says). By taking the logarithm of each probability,\n",
    "multiplication becomes addition, which is much more stable and prevents this issue. Using logs also\n",
    "makes the calculations easier to manage and allows us to compare values without computing extremely\n",
    "tiny products. However, if we have an unsmoothed model and a word occurs in the test document but never\n",
    "appeared in the training data for a given class, its probability is zero. Taking the logarithm of zero\n",
    "is undefined, which would cause a computation error and prevent us from obtaining a valid result for\n",
    "that class. Thus, smoothing is important when using log probabilities in Naive Bayes.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "# q7c\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Documents 6 and 8 in the test set include a word that did not appear in the training\n",
    "#             corpus (and as a result does not appear in the model). What should we do at inference\n",
    "#             time when we need a class conditional probability for this word?\n",
    "\n",
    "#   a.) We should ignore such documents with previously unseen words.\n",
    "\n",
    "#   b.) We should assign the class probability of the majority class to the unseen word.\n",
    "\n",
    "#   c.) We could either assign the same conditional probability to each class (eg. 0.5 for two classes)\n",
    "#       or we could simply disregard that word since multiplying the same value for each class won't\n",
    "#       ultimately affect the argmax determination.\n",
    "\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"c\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "# q7d\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: The goal of our MapReduce job is to stream over the test set and classify each\n",
    "#             document by performing the calculation from equation 13.4 (see figure above for\n",
    "#             more details of this equation). To do this, we'll load the model file (which contains\n",
    "#             the probabilities for equation 13.4 (see figure above for more details of this equation)\n",
    "#             into memory on the nodes where we do our mapping. This is called an in-memory join.\n",
    "#             Does loading a model 'state' like this depart from the functional programming principles?\n",
    "#             Explain why or why not. From a scalability perspective, when would this kind of memory\n",
    "#             use be justified? When would it be unwise?\n",
    "\n",
    "#   a.) Loading the model into memory fundamentally breaks the functional programming model as it\n",
    "#       makes it fully stateful. This is unacceptable and it's a discouraged programming practice\n",
    "#       in a map-reduce model. We should find an alternative way of solving the problem.\n",
    "\n",
    "#   b.) Loading the model into memory has absolutely nothing to do with the functional programming model.\n",
    "#       This is a very acceptable solution that scales really well and has the benefit of being applicable\n",
    "#       to small as well as large vocabularies.\n",
    "\n",
    "#   c.) Loading the model into memory is a slight departure from statelessness since we're maintaining\n",
    "#       a model state. This is forgivable since we're not updating that state at all after it gets\n",
    "#       loaded on each mapper node so there's no risk of a race condition.\n",
    "#       Furthermore our model is small, however with a really large vocabulary the model\n",
    "#       could get too large to fit in memory and we might pursue a different solution in that case.\n",
    "\n",
    "#   d.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"c\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work for `part e` starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - do your work in NaiveBayes/classify_mapper.py first, then run this cell.\n",
    "!chmod a+x NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Mapper for Naive Bayes Inference.\n",
      "INPUT:\n",
      "    ID \\t true_class \\t subject \\t body \\n\n",
      "OUTPUT:\n",
      "    ID \\t true_class \\t logP(ham|doc) \\t logP(spam|doc) \\t predicted_class\n",
      "SUPPLEMENTAL FILE: \n",
      "    This script requires a trained Naive Bayes model stored \n",
      "    as NBmodel.txt in the current directory. The model should \n",
      "    be a tab separated file whose records look like:\n",
      "        WORD \\t ham_count,spam_count,P(word|ham),P(word|spam)\n",
      "        \n",
      "Instructions:\n",
      "    We have loaded the supplemental file and taken the log of \n",
      "    each conditional probability in the model. We also provide\n",
      "    the code to tokenize the input lines for you. Keep in mind \n",
      "    that each 'line' of this file represents a unique document \n",
      "    that we wish to classify. Fill in the missing code to get\n",
      "    the probability of each class given the words in the document.\n",
      "    Remember that you will need to handle the case where you\n",
      "    encounter a word that is not represented in the model.\n",
      "\"\"\"\n",
      "import os\n",
      "import re\n",
      "import sys\n",
      "import numpy as np\n",
      "import math\n",
      "\n",
      "# confirm that we have access to the model file\n",
      "assert 'NBmodel.txt' in os.listdir('.'), \"ERROR: can't find NBmodel.txt\"\n",
      "\n",
      "# load the model into a dictionary for easy access\n",
      "MODEL = {}\n",
      "for record in open('NBmodel.txt', 'r').readlines():\n",
      "    word, payload = record.split('\\t')\n",
      "    # extract conditional probabilities\n",
      "    ham_cProb, spam_cProb = payload.split(',')[2:]\n",
      "    # save their logs as a tuple in our model dictionary\n",
      "    take_log = lambda x: np.log(x) if x != 0 else float(\"-inf\")\n",
      "    MODEL[word] = (take_log(float(ham_cProb)),\n",
      "                   take_log(float(spam_cProb)))\n",
      "\n",
      "# read from standard input\n",
      "for line in sys.stdin:\n",
      "    # parse input and tokenize\n",
      "    docID, _class, subject, body = line.lower().split('\\t')\n",
      "    words = re.findall(r'[a-z]+', subject + ' ' + body)\n",
      "    \n",
      "    # initialize variables that student code should overwrite\n",
      "    logpHam, logpSpam, pred_class = None, None, None\n",
      "    \n",
      "    ################# YOUR CODE HERE ################\n",
      "    # TIP: try using MODEL.get(word, (0,0)) to access the tuple \n",
      "    # of log probabilities without throwing a KeyError!    \n",
      "    # Initialize with class priors (already in log space)\n",
      "    logpHam, logpSpam = MODEL['ClassPriors']  # assume this exists\n",
      "\n",
      "    # Accumulate log probabilities for each word safely\n",
      "    for word in words:\n",
      "        # use 0 for unseen words\n",
      "        ham_log, spam_log = MODEL.get(word, (0.0, 0.0))\n",
      "        logpHam += ham_log\n",
      "        logpSpam += spam_log\n",
      "\n",
      "    # Predict class\n",
      "    pred_class = 0 if logpHam > logpSpam else 1\n",
      "    ################# (END) YOUR CODE ##############\n",
      "    \n",
      "    print(f\"{docID}\\t{_class}\\t{logpHam}\\t{logpSpam}\\t{pred_class}\")"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - unit test NaiveBayes/classify_mapper.py (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob703969390542198683.jar tmpDir=null\n",
      "2025-09-21 05:57:16,789 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:57:17,078 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:57:17,591 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:57:17,592 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:57:17,828 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0029\n",
      "2025-09-21 05:57:18,581 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 05:57:19,433 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2025-09-21 05:57:19,627 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0029\n",
      "2025-09-21 05:57:19,629 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 05:57:19,853 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 05:57:19,854 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 05:57:19,923 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0029\n",
      "2025-09-21 05:57:19,966 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0029/\n",
      "2025-09-21 05:57:19,968 INFO mapreduce.Job: Running job: job_1758422346528_0029\n",
      "2025-09-21 05:57:28,204 INFO mapreduce.Job: Job job_1758422346528_0029 running in uber mode : false\n",
      "2025-09-21 05:57:28,205 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 05:57:37,464 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2025-09-21 05:57:38,491 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-09-21 05:57:45,573 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-09-21 05:57:46,579 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-09-21 05:57:52,622 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2025-09-21 05:57:54,642 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2025-09-21 05:57:57,663 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 05:58:04,719 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-09-21 05:58:06,740 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 05:58:08,756 INFO mapreduce.Job: Job job_1758422346528_0029 completed successfully\n",
      "2025-09-21 05:58:08,852 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=204\n",
      "\t\tFILE: Number of bytes written=3286791\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1555\n",
      "\t\tHDFS: Number of bytes written=178\n",
      "\t\tHDFS: Number of read operations=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=208908264\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=49394556\n",
      "\t\tTotal time spent by all map tasks (ms)=66194\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15651\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=66194\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15651\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=208908264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=49394556\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=178\n",
      "\t\tMap output materialized bytes=366\n",
      "\t\tInput split bytes=950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=366\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=1967\n",
      "\t\tCPU time spent (ms)=19440\n",
      "\t\tPhysical memory (bytes) snapshot=7043969024\n",
      "\t\tVirtual memory (bytes) snapshot=57953398784\n",
      "\t\tTotal committed heap usage (bytes)=5605163008\n",
      "\t\tPeak Map Physical memory (bytes)=620396544\n",
      "\t\tPeak Map Virtual memory (bytes)=4470661120\n",
      "\t\tPeak Reduce Physical memory (bytes)=340434944\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4459491328\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=605\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=178\n",
      "2025-09-21 05:58:08,852 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadooop streaming job here\n",
    "#NOTE: I HAD TO ADD REDUCER WITH SORT JUST TO MAP THE EXPECTED OUTCOME SHOWN BELOW:\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -files NBmodel.txt,NaiveBayes/classify_mapper.py \\\n",
    "    -mapper classify_mapper.py \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input {HDFS_DIR}/chineseTest.txt \\\n",
    "    -output {HDFS_DIR}/chinese-output \\\n",
    "    -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - retrieve test set results from HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-000* > NaiveBayes/chineseResults.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - take a look (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseResults.txt | sort -k1,1 | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th> Expected output for the test set:</th>\n",
    "<tr align=Left><td><pre>\n",
    "d5\t1\t-8.90668134\t-8.10769031\t1\n",
    "d6\t1\t-5.78074351\t-4.17950237\t1\n",
    "d7\t0\t-6.59167373\t-7.51170688\t0\n",
    "d8\t0\t-4.39444915\t-5.56579673\t0\n",
    "</pre></td><tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Naive Bayes Training\n",
    "\n",
    "In Question 7 we used a model that we had trained by hand. Next we'll develop the code to do that same training in parallel, making it suitable for use with larger corpora (like the Enron emails). The end result of the MapReduce job you write in this question should be a model text file that looks just like the example (`NBmodel.txt`) that we created by hand above.\n",
    "\n",
    "To refresh your memory about the training process take a look at  `6a` and `6b` where you described the pieces of information you'll need to collect in order to encode a Multinomial Naive Bayes model. We now want to retrieve those pieces of information while streaming over a corpus. The bulk of the task will be very similar to the word counting excercises you've already done but you may want to consider a slightly different key-value record structure to efficiently tally counts for each class. \n",
    "\n",
    "The most challenging (interesting?) design question will be how to retrieve the totals (# of documents and # of words in documents for each class). Of course, counting these numbers is easy. The hard part is the timing: you'll need to make sure you have the counts totalled up _before_ you start estimating the class conditional probabilities for each word. It would be best (i.e. most scalable) if we could find a way to do this tallying without storing the whole vocabulary in memory... Use an appropriate MapReduce design pattern to implement this efficiently! \n",
    "\n",
    "\n",
    "### Q8 Tasks:\n",
    "\n",
    "* __a) make a plan:__  Fill in the docstrings for __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ to appropriately reflect the format that each script will input/output. [`HINT:` _the input files_ (`enronemail_1h.txt` & `chineseTrain.txt`) _have a prespecified format and your output file should match_ `NBmodel.txt` _so you really only have to decide on an internal format for Hadoop_].\n",
    "\n",
    "\n",
    "* __b) short essay:__ Read the code in __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ so that together they train a Multinomial Naive Bayes model __with no smoothing__. Confirm that your trained model matches your hand calculations from Question 6. Explain the code.\n",
    "\n",
    "\n",
    "* __c) multiple choice:__ We saw in Question 6 that adding Laplace smoothing (where the smoothing parameter $k=1$) makes our classifications less sensitve to rare words. However implementing this technique requires access to one additional piece of information that we had not previously used in our Naive Bayes training. What is that extra piece of information? [`HINT:` see equation 13.7 in Manning, Raghavan and Schutze].\n",
    "\n",
    "* __d) multiple choice:__ There are a couple of approaches that we could take to handle the extra piece of information you identified in `c`: 1) if we knew this extra information beforehand, we could provide it to our reducer as a configurable parameter for the vocab size dynamically (_where would we get it in the first place?_). Or 2) we could compute it in the reducer without storing any bulky information in memory but then we'd need some postprocessing or a second MapReduce job to complete the calculation (_why?_). What is non-ideal about each of these options?\n",
    "\n",
    "* __e) code + short essay:__ Choose one of the 2 options above. State your choice & reasoning in the space below then use that strategy to complete the code in __`NaiveBayes/train_reducer_smooth.py`__. Test this alternate reducer then write and run a Hadoop streaming job to train an MNB model with smoothing on the Chinese example. Your results should match the model that we provided for you above (and the calculations in the textbook example). __IMPORTANT NOTE:__ For full credit on this question, your code must work with multiple reducers. \n",
    "\n",
    "    - [`HINT:` You will need to implement custom partitioning - [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb) in GCS bucket __`GCS/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb`__] \n",
    "    \n",
    "    - [`HINT:` To make your custom partitioning code more flexible, you can read the number of reduce tasks configuration parameter in your mapper code (instead of hard coding it). See pg 204. Hadoop Defintive Guide - Streaming environment variables]\n",
    "\n",
    "    - [`HINT:` Don't start from scratch with this one -- you can just copy over your reducer code from part `b` and make the needed modifications]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== MAPPER DOCSTRING ============\n",
      "Mapper reads in text documents and emits word counts by class.\n",
      "INPUT:                                                    \n",
      "    DocID \\t true_class \\t subject \\t body                \n",
      "OUTPUT:                                                   \n",
      "    partitionKey \\t word \\t class0_partialCount,class1_partialCount       \n",
      "    \n",
      "=========== REDUCER DOCSTRING ============\n",
      "Reducer aggregates word counts by class and emits frequencies.\n",
      "\n",
      "INPUT:\n",
      "    Each line of input comes from the mapper and has the format:\n",
      "        partitionKey \\t word \\t class0_partialCount,class1_partialCount\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part a - do your work in train_mapper.py and train_reducer.py then RUN THIS CELL AS IS\n",
    "!chmod a+x NaiveBayes/train_mapper.py\n",
    "!chmod a+x NaiveBayes/train_reducer.py\n",
    "!echo \"=========== MAPPER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_mapper.py | tail -n 6\n",
    "!echo \"=========== REDUCER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_reducer.py | tail -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part b starts here`:__ MNB _without_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The `train_mapper.py` and `train_reducer.py` scripts work together to train a Multinomial Naive Bayes\n",
      "model using Hadoop Streaming. The mapper’s job is to read each document in the training set, split it\n",
      "into words, and keep track of which class the document belongs to. For every word, the mapper emits a\n",
      "record that says whether it came from class 0 or class 1, using the format\n",
      "`[class0_count, class1_count]`. The mapper also keeps track of the total number of documents and total\n",
      "number of words per class. At the end, it sends these totals along with special keys (`docTotals` and\n",
      "`wordTotals`) so that every reducer can access the global class statistics.\n",
      "\n",
      "The reducer then takes all of these intermediate records and combines them. For words, it adds up the\n",
      "counts from different mappers so we get the total number of times each word appears in class 0 and in\n",
      "class 1. Once the totals are ready, the reducer computes the conditional probabilities for each word in\n",
      "each class: `P(word|class) = count(word, class) / total_words_in_class`. It also computes the class\n",
      "priors, which are just the fraction of documents in each class compared to the total number of\n",
      "documents. These results are formatted to match the `NBmodel.txt` structure.\n",
      "\n",
      "When we compare the output from these scripts to the hand calculations in Question 6, we see that the\n",
      "values match. For example, the word “Chinese” appears exactly the right number of times in the class 1\n",
      "documents, and the computed probability lines up with the manual fraction we calculated. The same is\n",
      "true for other words like “Tokyo,” “Beijing,” and “Macao,” and the overall priors (25% class 0,\n",
      "75% class 1) are correct. This confirms that the MapReduce implementation is reproducing\n",
      "the unsmoothed Multinomial Naive Bayes model.\n",
      "\n",
      "The design choice in these scripts is how the mapper and reducer communicate. Instead of trying to hold\n",
      "everything in memory, the mapper outputs both word counts and global totals so that reducers can\n",
      "independently compute the necessary probabilities. This design makes the workflow scalable for large\n",
      "datasets, like the Enron emails, while still matching the logic of the hand-calculated example.\n",
      "Thus, the code is a parallelized version of the exact same training process we did manually.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q8b1\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Read the code in NaiveBayes/train_mapper.py and NaiveBayes/train_reducer.py so\n",
    "#             that together they train a Multinomial Naive Bayes model with no smoothing.\n",
    "#             Confirm that your trained model matches your hand calculations from Question 6.\n",
    "#             Explain the code.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "\n",
    "The `train_mapper.py` and `train_reducer.py` scripts work together to train a Multinomial Naive Bayes\n",
    "model using Hadoop Streaming. The mapper’s job is to read each document in the training set, split it\n",
    "into words, and keep track of which class the document belongs to. For every word, the mapper emits a\n",
    "record that says whether it came from class 0 or class 1, using the format\n",
    "`[class0_count, class1_count]`. The mapper also keeps track of the total number of documents and total\n",
    "number of words per class. At the end, it sends these totals along with special keys (`docTotals` and\n",
    "`wordTotals`) so that every reducer can access the global class statistics.\n",
    "\n",
    "The reducer then takes all of these intermediate records and combines them. For words, it adds up the\n",
    "counts from different mappers so we get the total number of times each word appears in class 0 and in\n",
    "class 1. Once the totals are ready, the reducer computes the conditional probabilities for each word in\n",
    "each class: `P(word|class) = count(word, class) / total_words_in_class`. It also computes the class\n",
    "priors, which are just the fraction of documents in each class compared to the total number of\n",
    "documents. These results are formatted to match the `NBmodel.txt` structure.\n",
    "\n",
    "When we compare the output from these scripts to the hand calculations in Question 6, we see that the\n",
    "values match. For example, the word “Chinese” appears exactly the right number of times in the class 1\n",
    "documents, and the computed probability lines up with the manual fraction we calculated. The same is\n",
    "true for other words like “Tokyo,” “Beijing,” and “Macao,” and the overall priors (25% class 0,\n",
    "75% class 1) are correct. This confirms that the MapReduce implementation is reproducing\n",
    "the unsmoothed Multinomial Naive Bayes model.\n",
    "\n",
    "The design choice in these scripts is how the mapper and reducer communicate. Instead of trying to hold\n",
    "everything in memory, the mapper outputs both word counts and global totals so that reducers can\n",
    "independently compute the necessary probabilities. This design makes the workflow scalable for large\n",
    "datasets, like the Enron emails, while still matching the logic of the hand-calculated example.\n",
    "Thus, the code is a parallelized version of the exact same training process we did manually.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tchinese\t0,1\n",
      "A\tbeijing\t0,1\n",
      "A\tchinese\t0,1\n",
      "A\tchinese\t0,1\n",
      "A\tchinese\t0,1\n",
      "A\tshanghai\t0,1\n",
      "A\tchinese\t0,1\n",
      "A\tmacao\t0,1\n",
      "A\ttokyo\t1,0\n",
      "A\tjapan\t1,0\n",
      "A\tchinese\t1,0\n",
      "A\t*docTotals\t1,3\n",
      "A\t*wordTotals\t3,8\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your mapper here - RUN CELL AS IS\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing      0,1,0.0,0.125\n",
      "chinese      1,5,0.3333333333333333,0.625\n",
      "japan        1,0,0.3333333333333333,0.0\n",
      "macao        0,1,0.0,0.125\n",
      "shanghai     0,1,0.0,0.125\n",
      "tokyo        1,0,0.3333333333333333,0.0\n",
      "ClassPriors  1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your reducer here - RUN CELL AS IS\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k2,2 > mapper_test_output.txt\n",
    "!cat mapper_test_output.txt | NaiveBayes/train_reducer.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing      0,1,0.0,0.125\n",
      "chinese      1,5,0.3333333333333333,0.625\n",
      "japan        1,0,0.3333333333333333,0.0\n",
      "macao        0,1,0.0,0.125\n",
      "shanghai     0,1,0.0,0.125\n",
      "tokyo        1,0,0.3333333333333333,0.0\n",
      "ClassPriors  1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here - RUN CELL AS IS\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort | NaiveBayes/train_reducer.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-train-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear (and name) an output directory in HDFS for your unsmoothed chinese NB model - RUN CELL AS IS\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-train-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob2085878467307239662.jar tmpDir=null\n",
      "2025-09-21 05:58:55,860 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:58:56,164 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:58:56,669 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 05:58:56,670 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 05:58:56,888 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0030\n",
      "2025-09-21 05:58:58,037 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 05:58:58,096 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2025-09-21 05:58:58,326 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0030\n",
      "2025-09-21 05:58:58,328 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 05:58:58,527 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 05:58:58,527 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 05:58:58,588 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0030\n",
      "2025-09-21 05:58:58,624 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0030/\n",
      "2025-09-21 05:58:58,626 INFO mapreduce.Job: Running job: job_1758422346528_0030\n",
      "2025-09-21 05:59:07,727 INFO mapreduce.Job: Job job_1758422346528_0030 running in uber mode : false\n",
      "2025-09-21 05:59:07,728 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 05:59:16,851 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-09-21 05:59:22,903 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-09-21 05:59:24,921 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-09-21 05:59:29,958 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2025-09-21 05:59:32,980 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2025-09-21 05:59:35,997 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 05:59:43,037 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-09-21 05:59:44,044 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 05:59:45,056 INFO mapreduce.Job: Job job_1758422346528_0030 completed successfully\n",
      "2025-09-21 05:59:45,155 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=963\n",
      "\t\tFILE: Number of bytes written=3046432\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1535\n",
      "\t\tHDFS: Number of bytes written=228\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=207762636\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27605532\n",
      "\t\tTotal time spent by all map tasks (ms)=65831\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8747\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=65831\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8747\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=207762636\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27605532\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=51\n",
      "\t\tMap output bytes=849\n",
      "\t\tMap output materialized bytes=1071\n",
      "\t\tInput split bytes=960\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1071\n",
      "\t\tReduce input records=51\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=102\n",
      "\t\tShuffled Maps =20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tGC time elapsed (ms)=1641\n",
      "\t\tCPU time spent (ms)=18190\n",
      "\t\tPhysical memory (bytes) snapshot=6807633920\n",
      "\t\tVirtual memory (bytes) snapshot=53569122304\n",
      "\t\tTotal committed heap usage (bytes)=5766119424\n",
      "\t\tPeak Map Physical memory (bytes)=640737280\n",
      "\t\tPeak Map Virtual memory (bytes)=4482732032\n",
      "\t\tPeak Reduce Physical memory (bytes)=338788352\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4466524160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=575\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=228\n",
      "2025-09-21 05:59:45,155 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-train-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your hadoop streaming job - RUN CELL AS IS\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-train-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 2 # <-- feel free to modify the number of reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - extract your results (i.e. model) to a local file - RUN CELL AS IS\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-train-output/part-0000* > NaiveBayes/chineseModelUnsmoothed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing      0,1,0.0,0.125\n",
      "japan        1,0,0.3333333333333333,0.0\n",
      "tokyo        1,0,0.3333333333333333,0.0\n",
      "ClassPriors  1.0,3.0,0.25,0.75\n",
      "chinese      1,5,0.3333333333333333,0.625\n",
      "macao        0,1,0.0,0.125\n",
      "shanghai     0,1,0.0,0.125\n",
      "ClassPriors  1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - print your model so that we can confirm that it matches expected results - RUN CELL AS IS\n",
    "!cat NaiveBayes/chineseModelUnsmoothed.txt | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q8c\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: What is that extra piece of information you will need in order to smooth\n",
    "#             the word class conditional probabilities?\n",
    "\n",
    "#   a.) vocabulary size\n",
    "#   b.) the number of occurrences of t in training documents from class c\n",
    "#   c.) the number of documents in class c\n",
    "#   d.) the total number of documents (including class c and not class c)\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# q8d\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: There are a couple of approaches that we could take to handle the extra piece of information\n",
    "#             you identified in Q8.c)\n",
    "\n",
    "#             1) if we knew this extra information beforehand, we could provide it to our reducer as a\n",
    "#                configurable parameter for the vocab size dynamically (where would we get it in \n",
    "#                the first place?).\n",
    "#             2) we could compute it in the reducer without storing any bulky information in memory\n",
    "#                but then we'd need some postprocessing or a second MapReduce job to complete the\n",
    "#                calculation (why?). \n",
    "\n",
    "#             What is non-ideal about each of these options?\n",
    "\n",
    "#   a.) For option 1, we need to do some EDA in advance, but the data might be too big for us to pass through.\n",
    "#       For option 2, it is incompatible with using multiple reducers.\n",
    "\n",
    "#   b.) For option 1, the information we got might not be accurate because it is changing dynamically.\n",
    "#       For option 2, we might get out of memory issue.\n",
    "\n",
    "#   c.) For option 1, there is no way to have the information in advance.\n",
    "#       For option 2, it will take a long time to process.\n",
    "\n",
    "#   d.) For option 1, we can not pass it as a configurable parameter.\n",
    "#       For option 2, we wouldn't be able to compute the correct conditional probability (estimates)\n",
    "#       until the reducer has already parsed all of the records.\n",
    "\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"d\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part e starts here`:__ MNB _with_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I would choose Option 2, computing the vocabulary size in the reducer, because of scalability. \n",
      "If we choose Option 1, we’d have to run an additional preprocessing pass or do some EDA just to know \n",
      "the vocabulary size, which becomes inefficient when dealing with very large datasets. In contrast, \n",
      "Option 2 allows us to compute the vocabulary size as part of the reducer’s workflow, even though it\n",
      "means we’ll need either a second MapReduce job or a postprocessing step before finalizing probabilities.\n",
      "While this adds some overhead, it avoids assumptions or prior knowledge about the data and is more\n",
      "true to the distributed nature of MapReduce. In practice, needing an extra pass for smoothing is more\n",
      "acceptable than requiring complete knowledge of the dataset ahead of time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q8e1\n",
    "### SHORT ESSAY\n",
    "### QUESTION: Choose one of the 2 options above. State your choice & reasoning in the space below.\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "I would choose Option 2, computing the vocabulary size in the reducer, because of scalability. \n",
    "If we choose Option 1, we’d have to run an additional preprocessing pass or do some EDA just to know \n",
    "the vocabulary size, which becomes inefficient when dealing with very large datasets. In contrast, \n",
    "Option 2 allows us to compute the vocabulary size as part of the reducer’s workflow, even though it\n",
    "means we’ll need either a second MapReduce job or a postprocessing step before finalizing probabilities.\n",
    "While this adds some overhead, it avoids assumptions or prior knowledge about the data and is more\n",
    "true to the distributed nature of MapReduce. In practice, needing an extra pass for smoothing is more\n",
    "acceptable than requiring complete knowledge of the dataset ahead of time.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "import os\n",
      "import sys                                                  \n",
      "import numpy as np  \n",
      "\n",
      "#################### YOUR CODE HERE ###################\n",
      "\"\"\"\n",
      "Multi-reducer for Naive Bayes with Laplace smoothing, aggregates word counts by class and emits smoothed conditional probabilities\n",
      "\n",
      "INPUT:\n",
      "    partitionKey \\t word \\t class0_partialCount,class1_partialCount\n",
      "    where:\n",
      "      - word can be a token, or special keys:\n",
      "            *docTotals, *wordTotals, *vocabWord\n",
      "\n",
      "OUTPUT:\n",
      "    word \\t class0_count,class1_count,P(word|class0),P(word|class1)\n",
      "    OR:\n",
      "    ClassPriors \\t doc0_count,doc1_count,P(class0),P(class1)\n",
      "\n",
      "This reducer works correctly with multiple reducers.\n",
      "Vocabulary size is inferred dynamically as the number of unique words seen\n",
      "(excluding docTotals and wordTotals).\n",
      "\"\"\"\n",
      "\n",
      "def EMIT(word, c0, c1, p0, p1):\n",
      "    print(f\"{word}\\t{c0},{c1},{p0},{p1}\")\n",
      "\n",
      "# trackers\n",
      "docTotals = np.array([0.0, 0.0])\n",
      "wordTotals = np.array([0.0, 0.0])\n",
      "vocab = set()\n",
      "word_counts = {}\n",
      "\n",
      "for line in sys.stdin:\n",
      "    try:\n",
      "        part, wrd, counts = line.strip().split()\n",
      "    except ValueError:\n",
      "        continue\n",
      "\n",
      "    counts = np.array([int(c) for c in counts.split(\",\")])\n",
      "\n",
      "    if wrd == \"*docTotals\":\n",
      "        docTotals += counts\n",
      "    elif wrd == \"*wordTotals\":\n",
      "        wordTotals += counts\n",
      "    elif wrd == \"*vocabWord\":\n",
      "        # only add marker word to vocab\n",
      "        vocab.add(tuple(counts))  # use counts as a unique dummy\n",
      "    else:\n",
      "        # accumulate counts per word\n",
      "        if wrd in word_counts:\n",
      "            word_counts[wrd] += counts\n",
      "        else:\n",
      "            word_counts[wrd] = counts\n",
      "\n",
      "# compute global vocabulary size\n",
      "V = len(vocab)\n",
      "\n",
      "# emit word-level probabilities\n",
      "for wrd, counts in word_counts.items():\n",
      "    probs = (counts + 1.0) / (wordTotals + V)\n",
      "    EMIT(wrd, counts[0], counts[1], probs[0], probs[1])\n",
      "\n",
      "# emit priors\n",
      "priors = docTotals / docTotals.sum() if docTotals.sum() > 0 else [0.0, 0.0]\n",
      "EMIT(\"ClassPriors\", int(docTotals[0]), int(docTotals[1]), priors[0], priors[1])\n",
      "#################### (END) YOUR CODE ###################"
     ]
    }
   ],
   "source": [
    "# part e\n",
    "!cat NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing      0,1,0.3333333333333333,0.25\n",
      "chinese      1,5,0.6666666666666666,0.75\n",
      "japan        1,0,0.6666666666666666,0.125\n",
      "macao        0,1,0.3333333333333333,0.25\n",
      "shanghai     0,1,0.3333333333333333,0.25\n",
      "tokyo        1,0,0.6666666666666666,0.125\n",
      "ClassPriors  1,3,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part e - write a unit test for your NEW reducer here\n",
    "!chmod +x NaiveBayes/train_reducer_smooth.py\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k2,2 > mapper_test_output.txt\n",
    "!cat mapper_test_output.txt | NaiveBayes/train_reducer_smooth.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing      0,1,0.3333333333333333,0.25\n",
      "chinese      1,5,0.6666666666666666,0.75\n",
      "japan        1,0,0.6666666666666666,0.125\n",
      "macao        0,1,0.3333333333333333,0.25\n",
      "shanghai     0,1,0.3333333333333333,0.25\n",
      "tokyo        1,0,0.6666666666666666,0.125\n",
      "ClassPriors  1,3,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part e - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort | NaiveBayes/train_reducer_smooth.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-train-smooth-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear (and name) an output directory in HDFS for your SMOOTHED chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-train-smooth-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob8662185012174384812.jar tmpDir=null\n",
      "2025-09-21 07:07:25,864 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:07:26,166 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:07:26,686 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:07:26,687 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:07:26,907 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0048\n",
      "2025-09-21 07:07:27,276 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 07:07:27,336 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2025-09-21 07:07:27,550 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0048\n",
      "2025-09-21 07:07:27,551 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 07:07:27,764 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 07:07:27,764 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 07:07:27,841 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0048\n",
      "2025-09-21 07:07:27,880 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0048/\n",
      "2025-09-21 07:07:27,881 INFO mapreduce.Job: Running job: job_1758422346528_0048\n",
      "2025-09-21 07:07:36,994 INFO mapreduce.Job: Job job_1758422346528_0048 running in uber mode : false\n",
      "2025-09-21 07:07:36,995 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 07:07:46,138 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-09-21 07:07:53,200 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-09-21 07:07:54,207 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-09-21 07:08:01,261 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "2025-09-21 07:08:02,266 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2025-09-21 07:08:06,291 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 07:08:13,336 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-09-21 07:08:14,341 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 07:08:16,357 INFO mapreduce.Job: Job job_1758422346528_0048 completed successfully\n",
      "2025-09-21 07:08:16,451 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=963\n",
      "\t\tFILE: Number of bytes written=3046852\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1535\n",
      "\t\tHDFS: Number of bytes written=265\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=205970028\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27605532\n",
      "\t\tTotal time spent by all map tasks (ms)=65263\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8747\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=65263\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8747\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=205970028\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27605532\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=51\n",
      "\t\tMap output bytes=849\n",
      "\t\tMap output materialized bytes=1071\n",
      "\t\tInput split bytes=960\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1071\n",
      "\t\tReduce input records=51\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=102\n",
      "\t\tShuffled Maps =20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tGC time elapsed (ms)=1726\n",
      "\t\tCPU time spent (ms)=18340\n",
      "\t\tPhysical memory (bytes) snapshot=6751588352\n",
      "\t\tVirtual memory (bytes) snapshot=53600485376\n",
      "\t\tTotal committed heap usage (bytes)=5544345600\n",
      "\t\tPeak Map Physical memory (bytes)=632635392\n",
      "\t\tPeak Map Virtual memory (bytes)=4521316352\n",
      "\t\tPeak Reduce Physical memory (bytes)=337420288\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4469878784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=575\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=265\n",
      "2025-09-21 07:08:16,451 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-train-smooth-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-train-smooth-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - extract your results (i.e. model) to a local file called \"chineseModelSmoothed.txt\" in the NaiveBayes folder.\n",
    "!hdfs dfs -getmerge {HDFS_DIR}/chinese-train-smooth-output NaiveBayes/chineseModelSmoothed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing      0,1,0.3333333333333333,0.25\n",
      "japan        1,0,0.6666666666666666,0.125\n",
      "tokyo        1,0,0.6666666666666666,0.125\n",
      "ClassPriors  1,3,0.25,0.75\n",
      "chinese      1,5,0.6666666666666666,0.75\n",
      "macao        0,1,0.3333333333333333,0.25\n",
      "shanghai     0,1,0.3333333333333333,0.25\n",
      "ClassPriors  1,3,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part e - RUN CELL AS IS\n",
    "!cat NaiveBayes/chineseModelSmoothed.txt | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Enron Ham/Spam NB Classifier & Results\n",
    "\n",
    "Fantastic work. We're finally ready to perform Spam Classification on the Enron Corpus. In this question you'll run the analysis you've developed, report its performance.\n",
    "\n",
    "### Q9 Tasks:\n",
    "* __a) train/test split:__ Run the provided code to split our Enron file into a training set and testing set then load them into HDFS. [`NOTE:` _Make sure you re calculate the vocab size for just the training set!_]\n",
    "\n",
    "* __b) code:__ Write Hadoop Streaming jobs to train MNB Models on the training set with smoothing (without smoothing is provided for your reference). Save your models to local files at __`NaiveBayes/Unsmoothed/NBmodel.txt`__ and __`NaiveBayes/Smoothed/NBmodel.txt`__. [`NOTE:` _This naming is important because we wrote our classification task so that it expects a file of that name... if this inelegance frustrates you there is an alternative that would involve a few adjustments to your code [read more about it here](http://www.tnoda.com/blog/2013-11-23)._] Finally run the checks that we provide to confirm that your results are correct.\n",
    "\n",
    "\n",
    "* __c) code:__ Recall that we designed our classification job with just a mapper. An efficient way to report the performance of our models would be to simply add a reducer phase to this job and compute precision and recall right there. Complete the code in __`NaiveBayes/evaluation_reducer.py`__ and then write Hadoop jobs to evaluate your two models on the test set. Report their performance side by side. [`NOTE:` if you need a refresher on precision, recall and F1-score [Wikipedia](https://en.wikipedia.org/wiki/F1_score) is a good resource.]\n",
    "\n",
    "\n",
    "* __d) short essay:__ Compare the performance of your two models. What do you notice about the unsmoothed model's predictions? Can you guess why this is happening? Which evaluation measure do you think is most relevant in our use case? [`NOTE:` _Feel free to answer using your common sense but if you want more information on evaluating the classification task checkout_ [this blogpost](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/) or [here](https://web.archive.org/web/20141112020055/https://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf)\n",
    "\n",
    "* __e.1) multiple answers:__ What is the reason behind the different performance of two models? (Select 2)\n",
    "\n",
    "* __e.2) multiple choide:__ Which evaluation measure do you think is least relevant in our use case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test/Train split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/enron_train.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/enron_test.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# part a - test/train split (RUN THIS CELL AS IS)\n",
    "!head -n 80 data/enronemail_1h.txt > data/enron_train.txt\n",
    "!tail -n 20 data/enronemail_1h.txt > data/enron_test.txt\n",
    "!hdfs dfs -copyFromLocal data/enron_train.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal data/enron_test.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4557\n"
     ]
    }
   ],
   "source": [
    "# Get vocab size from the training set only\n",
    "!cat data/enron_train.txt | NaiveBayes/train_mapper.py | cut -f2 | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _without smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob3186959711895925512.jar tmpDir=null\n",
      "2025-09-21 07:08:30,930 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:08:31,235 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:08:31,766 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:08:31,766 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:08:31,967 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0049\n",
      "2025-09-21 07:08:32,288 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 07:08:32,352 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 07:08:32,645 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0049\n",
      "2025-09-21 07:08:32,647 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 07:08:32,841 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 07:08:32,841 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 07:08:32,919 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0049\n",
      "2025-09-21 07:08:32,961 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0049/\n",
      "2025-09-21 07:08:32,963 INFO mapreduce.Job: Running job: job_1758422346528_0049\n",
      "2025-09-21 07:08:42,063 INFO mapreduce.Job: Job job_1758422346528_0049 running in uber mode : false\n",
      "2025-09-21 07:08:42,064 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 07:08:50,255 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2025-09-21 07:08:51,262 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2025-09-21 07:08:52,270 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 07:08:58,339 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2025-09-21 07:08:59,347 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 07:09:05,406 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2025-09-21 07:09:06,416 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2025-09-21 07:09:07,421 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 07:09:15,471 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-09-21 07:09:17,487 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2025-09-21 07:09:18,492 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 07:09:19,504 INFO mapreduce.Job: Job job_1758422346528_0049 completed successfully\n",
      "2025-09-21 07:09:19,595 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=344668\n",
      "\t\tFILE: Number of bytes written=3734399\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=213073\n",
      "\t\tHDFS: Number of bytes written=186999\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=188356392\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=57196188\n",
      "\t\tTotal time spent by all map tasks (ms)=59682\n",
      "\t\tTotal time spent by all reduce tasks (ms)=18123\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=59682\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=18123\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=188356392\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=57196188\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25121\n",
      "\t\tMap output bytes=294408\n",
      "\t\tMap output materialized bytes=344812\n",
      "\t\tInput split bytes=855\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4561\n",
      "\t\tReduce shuffle bytes=344812\n",
      "\t\tReduce input records=25121\n",
      "\t\tReduce output records=4558\n",
      "\t\tSpilled Records=50242\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=1775\n",
      "\t\tCPU time spent (ms)=21450\n",
      "\t\tPhysical memory (bytes) snapshot=6589108224\n",
      "\t\tVirtual memory (bytes) snapshot=53512183808\n",
      "\t\tTotal committed heap usage (bytes)=5596774400\n",
      "\t\tPeak Map Physical memory (bytes)=634986496\n",
      "\t\tPeak Map Virtual memory (bytes)=4463157248\n",
      "\t\tPeak Reduce Physical memory (bytes)=339673088\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4473970688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=212218\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=186999\n",
      "2025-09-21 07:09:19,596 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-model\n"
     ]
    }
   ],
   "source": [
    "# part b -  Unsmoothed model (RUN CELL AS IS)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/enron-model \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘NaiveBayes/Unsmoothed’: File exists\n"
     ]
    }
   ],
   "source": [
    "# save the model locally - RUN CELL AS IS\n",
    "!mkdir NaiveBayes/Unsmoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/enron-model/part-000* > NaiveBayes/Unsmoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0001725476662928134,0.00029682398337785694\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000172547666293,0.000296823983378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,8.62738331464067e-05,0.001632531908578213\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,8.62738331464e-05,0.00163253190858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _with Laplace +1 smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob8285227735498364300.jar tmpDir=null\n",
      "2025-09-21 07:09:28,181 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:09:28,485 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:09:29,023 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:09:29,023 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:09:29,248 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0050\n",
      "2025-09-21 07:09:29,635 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 07:09:29,689 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 07:09:29,919 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0050\n",
      "2025-09-21 07:09:29,921 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 07:09:30,110 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 07:09:30,110 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 07:09:30,179 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0050\n",
      "2025-09-21 07:09:30,219 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0050/\n",
      "2025-09-21 07:09:30,221 INFO mapreduce.Job: Running job: job_1758422346528_0050\n",
      "2025-09-21 07:09:39,325 INFO mapreduce.Job: Job job_1758422346528_0050 running in uber mode : false\n",
      "2025-09-21 07:09:39,327 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 07:09:48,474 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2025-09-21 07:09:49,491 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 07:09:56,570 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2025-09-21 07:09:57,577 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 07:10:04,645 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2025-09-21 07:10:05,650 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 07:10:13,701 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-09-21 07:10:15,718 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 07:10:17,736 INFO mapreduce.Job: Job job_1758422346528_0050 completed successfully\n",
      "2025-09-21 07:10:17,836 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=344668\n",
      "\t\tFILE: Number of bytes written=3734747\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=213073\n",
      "\t\tHDFS: Number of bytes written=253561\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=203571468\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=54936492\n",
      "\t\tTotal time spent by all map tasks (ms)=64503\n",
      "\t\tTotal time spent by all reduce tasks (ms)=17407\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=64503\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=17407\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=203571468\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=54936492\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25121\n",
      "\t\tMap output bytes=294408\n",
      "\t\tMap output materialized bytes=344812\n",
      "\t\tInput split bytes=855\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4561\n",
      "\t\tReduce shuffle bytes=344812\n",
      "\t\tReduce input records=25121\n",
      "\t\tReduce output records=4558\n",
      "\t\tSpilled Records=50242\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=1849\n",
      "\t\tCPU time spent (ms)=23070\n",
      "\t\tPhysical memory (bytes) snapshot=6603149312\n",
      "\t\tVirtual memory (bytes) snapshot=53631991808\n",
      "\t\tTotal committed heap usage (bytes)=5355601920\n",
      "\t\tPeak Map Physical memory (bytes)=633466880\n",
      "\t\tPeak Map Virtual memory (bytes)=4525268992\n",
      "\t\tPeak Reduce Physical memory (bytes)=347840512\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4471541760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=212218\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=253561\n",
      "2025-09-21 07:10:17,837 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model\n",
      "mkdir: cannot create directory ‘NaiveBayes/Smoothed’: File exists\n"
     ]
    }
   ],
   "source": [
    "# part b -  Smoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/smooth-model \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!mkdir NaiveBayes/Smoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model/part-000* > NaiveBayes/Smoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0002588214994392201,0.00037102997922232116\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.0001858045336306206,0.00027730020520215184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,0.0001725476662928134,0.0017067379044226774\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,0.0001238696890870804,0.0012755809439298986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - Copy to new files.\n",
    "# IMPORTANT: Use this for the autograder!\n",
    "!cp NaiveBayes/Unsmoothed/NBmodel.txt Unsmoothed_NBmodel.txt\n",
    "!cp NaiveBayes/Smoothed/NBmodel.txt Smoothed_NBmodel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - write your code in NaiveBayes/evaluation_reducer.py then RUN THIS\n",
    "!chmod a+x NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x NaiveBayes/classify_mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t1\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t1\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t0\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t0\n",
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t True\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t True\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t True\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t True\n",
      "# Documents:\t4\n",
      "True Positives:\t2\n",
      "True Negatives:\t2\n",
      "False Positives:\t0\n",
      "False Negatives:\t0\n",
      "Accuracy\t1.0000\n",
      "Precision\t1.0000\n",
      "Recall\t1.0000\n",
      "F-Score\t1.0000\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test your evaluation job on the chinese model (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py \n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-unsmoothed-eval\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob4052303460531890707.jar tmpDir=null\n",
      "2025-09-21 07:10:27,767 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:10:28,071 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:10:28,580 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:10:28,580 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:10:28,783 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0051\n",
      "2025-09-21 07:10:29,155 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 07:10:29,215 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 07:10:29,419 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0051\n",
      "2025-09-21 07:10:29,420 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 07:10:29,605 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 07:10:29,605 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 07:10:29,669 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0051\n",
      "2025-09-21 07:10:29,713 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0051/\n",
      "2025-09-21 07:10:29,715 INFO mapreduce.Job: Running job: job_1758422346528_0051\n",
      "2025-09-21 07:10:38,822 INFO mapreduce.Job: Job job_1758422346528_0051 running in uber mode : false\n",
      "2025-09-21 07:10:38,823 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 07:10:47,945 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2025-09-21 07:10:48,958 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 07:10:55,025 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2025-09-21 07:10:56,041 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 07:11:02,096 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2025-09-21 07:11:03,107 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2025-09-21 07:11:04,112 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 07:11:11,172 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-09-21 07:11:13,188 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2025-09-21 07:11:14,195 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 07:11:15,207 INFO mapreduce.Job: Job job_1758422346528_0051 completed successfully\n",
      "2025-09-21 07:11:15,307 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=830\n",
      "\t\tFILE: Number of bytes written=3039979\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=91491\n",
      "\t\tHDFS: Number of bytes written=1312\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=189183264\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=48059568\n",
      "\t\tTotal time spent by all map tasks (ms)=59944\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15228\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=59944\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15228\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=189183264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=48059568\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=772\n",
      "\t\tMap output materialized bytes=974\n",
      "\t\tInput split bytes=846\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=974\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=47\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=1807\n",
      "\t\tCPU time spent (ms)=18800\n",
      "\t\tPhysical memory (bytes) snapshot=6488141824\n",
      "\t\tVirtual memory (bytes) snapshot=53508407296\n",
      "\t\tTotal committed heap usage (bytes)=5356650496\n",
      "\t\tPeak Map Physical memory (bytes)=629948416\n",
      "\t\tPeak Map Virtual memory (bytes)=4478070784\n",
      "\t\tPeak Reduce Physical memory (bytes)=320815104\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4465991680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1312\n",
      "2025-09-21 07:11:15,307 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-unsmoothed-eval\n",
      "0015.2001-07-05.sa_and_hp\t1\t-inf\t-inf\t True\n",
      "0016.2001-02-12.kitchen\t0\t-inf\t-inf\t False\n",
      "0016.2004-08-01.bg\t1\t-inf\t-inf\t True\n",
      "0017.1999-12-14.kaminski\t0\t-inf\t-inf\t False\n",
      "0017.2000-01-17.beck\t0\t-2607.2310518590175\t-inf\t True\n",
      "0017.2003-12-18.gp\t1\t-inf\t-inf\t True\n",
      "0017.2004-08-01.bg\t1\t-inf\t-inf\t True\n",
      "# Documents:\t7\n",
      "True Positives:\t4\n",
      "True Negatives:\t1\n",
      "False Positives:\t2\n",
      "False Negatives:\t0\n",
      "Accuracy\t0.7143\n",
      "Precision\t0.6667\n",
      "Recall\t1.0000\n",
      "F-Score\t0.8000\n",
      "0015.2003-12-19.gp\t1\t-inf\t-inf\t True\n",
      "0016.2001-07-05.sa_and_hp\t1\t-inf\t-inf\t True\n",
      "0016.2003-12-19.gp\t1\t-inf\t-749.4645716867724\t True\n",
      "0017.2001-04-03.williams\t0\t-inf\t-inf\t False\n",
      "0017.2004-08-02.bg\t1\t-inf\t-inf\t True\n",
      "0018.1999-12-14.kaminski\t0\t-inf\t-inf\t False\n",
      "0018.2001-07-13.sa_and_hp\t1\t-inf\t-inf\t True\n",
      "# Documents:\t7\n",
      "True Positives:\t5\n",
      "True Negatives:\t0\n",
      "False Positives:\t2\n",
      "False Negatives:\t0\n",
      "Accuracy\t0.7143\n",
      "Precision\t0.7143\n",
      "Recall\t1.0000\n",
      "F-Score\t0.8333\n",
      "0015.1999-12-15.farmer\t0\t-580.1213669207012\t-inf\t True\n",
      "0015.2000-06-09.lokay\t0\t-inf\t-inf\t False\n",
      "0015.2001-02-12.kitchen\t0\t-inf\t-inf\t False\n",
      "0016.1999-12-15.farmer\t0\t-inf\t-inf\t False\n",
      "0016.2001-07-06.sa_and_hp\t1\t-inf\t-inf\t True\n",
      "0018.2003-12-18.gp\t1\t-inf\t-inf\t True\n",
      "# Documents:\t6\n",
      "True Positives:\t2\n",
      "True Negatives:\t1\n",
      "False Positives:\t3\n",
      "False Negatives:\t0\n",
      "Accuracy\t0.5000\n",
      "Precision\t0.4000\n",
      "Recall\t1.0000\n",
      "F-Score\t0.5714\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the UNSMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-unsmoothed-eval\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Unsmoothed/NBmodel.txt \\\n",
    "  -mapper \"classify_mapper.py Unsmoothed/NBmodel.txt\" \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/enron-unsmoothed-eval \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-unsmoothed-eval/part-000* > NaiveBayes/Unsmoothed/eval_results.txt\n",
    "!cat NaiveBayes/Unsmoothed/eval_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-smoothed-eval\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob6827331058676559847.jar tmpDir=null\n",
      "2025-09-21 07:11:23,451 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:11:23,774 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:11:24,294 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:11:24,294 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:11:24,507 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0052\n",
      "2025-09-21 07:11:25,302 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 07:11:25,352 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 07:11:25,586 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0052\n",
      "2025-09-21 07:11:25,587 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 07:11:25,789 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 07:11:25,789 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 07:11:25,856 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0052\n",
      "2025-09-21 07:11:25,892 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0052/\n",
      "2025-09-21 07:11:25,894 INFO mapreduce.Job: Running job: job_1758422346528_0052\n",
      "2025-09-21 07:11:34,055 INFO mapreduce.Job: Job job_1758422346528_0052 running in uber mode : false\n",
      "2025-09-21 07:11:34,056 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 07:11:43,203 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2025-09-21 07:11:44,209 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 07:11:50,271 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2025-09-21 07:11:52,290 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 07:11:57,323 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2025-09-21 07:11:59,344 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2025-09-21 07:12:00,349 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 07:12:07,396 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-09-21 07:12:09,414 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 07:12:11,431 INFO mapreduce.Job: Job job_1758422346528_0052 completed successfully\n",
      "2025-09-21 07:12:11,526 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1353\n",
      "\t\tFILE: Number of bytes written=3040953\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=91491\n",
      "\t\tHDFS: Number of bytes written=1831\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=192844224\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=50218272\n",
      "\t\tTotal time spent by all map tasks (ms)=61104\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15912\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=61104\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15912\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=192844224\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=50218272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=1295\n",
      "\t\tMap output materialized bytes=1497\n",
      "\t\tInput split bytes=846\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1497\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=47\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=1840\n",
      "\t\tCPU time spent (ms)=19270\n",
      "\t\tPhysical memory (bytes) snapshot=6477770752\n",
      "\t\tVirtual memory (bytes) snapshot=53503766528\n",
      "\t\tTotal committed heap usage (bytes)=5283250176\n",
      "\t\tPeak Map Physical memory (bytes)=630489088\n",
      "\t\tPeak Map Virtual memory (bytes)=4476682240\n",
      "\t\tPeak Reduce Physical memory (bytes)=328458240\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4461928448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1831\n",
      "2025-09-21 07:12:11,527 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-smoothed-eval\n",
      "0015.2001-07-05.sa_and_hp\t1\t-718.074550570112\t-685.9199010095051\t True\n",
      "0016.2001-02-12.kitchen\t0\t-875.4189633910552\t-987.7645996354338\t True\n",
      "0016.2004-08-01.bg\t1\t-526.6779903687783\t-492.3420942688456\t True\n",
      "0017.1999-12-14.kaminski\t0\t-323.7569575512076\t-323.66696576611446\t False\n",
      "0017.2000-01-17.beck\t0\t-2558.0460740502617\t-3099.551121277458\t True\n",
      "0017.2003-12-18.gp\t1\t-178.45256542349912\t-170.35239658841286\t True\n",
      "0017.2004-08-01.bg\t1\t-135.78677982156506\t-129.942638962945\t True\n",
      "# Documents:\t7\n",
      "True Positives:\t4\n",
      "True Negatives:\t2\n",
      "False Positives:\t1\n",
      "False Negatives:\t0\n",
      "Accuracy\t0.8571\n",
      "Precision\t0.8000\n",
      "Recall\t1.0000\n",
      "F-Score\t0.8889\n",
      "0015.2003-12-19.gp\t1\t-818.5546343276645\t-768.136163826235\t True\n",
      "0016.2001-07-05.sa_and_hp\t1\t-718.074550570112\t-685.9199010095051\t True\n",
      "0016.2003-12-19.gp\t1\t-755.1470376260291\t-711.4724001041774\t True\n",
      "0017.2001-04-03.williams\t0\t-376.4551262442174\t-375.5717164345936\t False\n",
      "0017.2004-08-02.bg\t1\t-1819.9609810654495\t-1798.708463919598\t True\n",
      "0018.1999-12-14.kaminski\t0\t-806.0616791673333\t-796.643586996273\t False\n",
      "0018.2001-07-13.sa_and_hp\t1\t-2739.2924585901756\t-2642.45801598889\t True\n",
      "# Documents:\t7\n",
      "True Positives:\t5\n",
      "True Negatives:\t0\n",
      "False Positives:\t2\n",
      "False Negatives:\t0\n",
      "Accuracy\t0.7143\n",
      "Precision\t0.7143\n",
      "Recall\t1.0000\n",
      "F-Score\t0.8333\n",
      "0015.1999-12-15.farmer\t0\t-559.9466668007929\t-663.883032251278\t True\n",
      "0015.2000-06-09.lokay\t0\t-100.61650200847703\t-104.93850050911041\t True\n",
      "0015.2001-02-12.kitchen\t0\t-4171.932628980197\t-4408.903438490671\t True\n",
      "0016.1999-12-15.farmer\t0\t-628.6010796725175\t-656.6929707101618\t True\n",
      "0016.2001-07-06.sa_and_hp\t1\t-16269.303071708116\t-14820.684401824823\t True\n",
      "0018.2003-12-18.gp\t1\t-2614.3117371903304\t-2564.7492652902256\t True\n",
      "# Documents:\t6\n",
      "True Positives:\t2\n",
      "True Negatives:\t4\n",
      "False Positives:\t0\n",
      "False Negatives:\t0\n",
      "Accuracy\t1.0000\n",
      "Precision\t1.0000\n",
      "Recall\t1.0000\n",
      "F-Score\t1.0000\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the SMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-smoothed-eval\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Smoothed/NBmodel.txt \\\n",
    "  -mapper \"classify_mapper.py Smoothed/NBmodel.txt\" \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/enron-smoothed-eval \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-smoothed-eval/part-000* > NaiveBayes/Smoothed/eval_results.txt\n",
    "!cat NaiveBayes/Smoothed/eval_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== UNSMOOTHED MODEL ============\n",
      "# Documents:\t6\n",
      "True Positives:\t2\n",
      "True Negatives:\t1\n",
      "False Positives:\t3\n",
      "False Negatives:\t0\n",
      "Accuracy\t0.5000\n",
      "Precision\t0.4000\n",
      "Recall\t1.0000\n",
      "F-Score\t0.5714\n",
      "=========== SMOOTHED MODEL ============\n",
      "# Documents:\t6\n",
      "True Positives:\t2\n",
      "True Negatives:\t4\n",
      "False Positives:\t0\n",
      "False Negatives:\t0\n",
      "Accuracy\t1.0000\n",
      "Precision\t1.0000\n",
      "Recall\t1.0000\n",
      "F-Score\t1.0000\n"
     ]
    }
   ],
   "source": [
    "# part c - display results \n",
    "# NOTE: feel free to modify the tail commands to match the format of your results file\n",
    "print('=========== UNSMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Unsmoothed/eval_results.txt\n",
    "print('=========== SMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Smoothed/eval_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - Copy to new files.\n",
    "# IMPORTANT: Use this for the autograder!\n",
    "!cp NaiveBayes/Unsmoothed/eval_results.txt Unsmoothed_results.txt\n",
    "!cp NaiveBayes/Smoothed/eval_results.txt Smoothed_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`EXPECTED RESULTS:`__ \n",
    "<table>\n",
    "<th>Unsmoothed Model</th>\n",
    "<th>Smoothed Model</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t1\n",
    "True Negatives:\t9\n",
    "False Positives:\t0\n",
    "False Negatives:\t10\n",
    "Accuracy\t0.5\n",
    "Precision\t1.0\n",
    "Recall\t0.0909\n",
    "F-Score\t0.1667\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t11\n",
    "True Negatives:\t6\n",
    "False Positives:\t3\n",
    "False Negatives:\t0\n",
    "Accuracy\t0.85\n",
    "Precision\t0.7857\n",
    "Recall\t1.0\n",
    "F-Score\t0.88\n",
    "</pre></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "__`NOTE:`__ _Don't be too disappointed if these seem low to you. We've trained and tested on a very very small corpus... bigger datasets coming soon!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking at the (expected) results, the unsmoothed model falls apart. Even though it gets a precision of 1.0,\n",
      "it almost never actually predicts spam correctly – the recall is only 0.09. That means it is too conservative\n",
      "and only calls something spam in very rare cases. The reason for this is that without Laplace smoothing,\n",
      "any word that never appeared in training for a certain class gets probability zero. Once you multiply (or add\n",
      "log-probabilities), that one zero wipes out the whole score for that class. So the model ends up defaulting\n",
      "to the safer class (ham) almost all the time, which explains why recall is so terrible.\n",
      "\n",
      "Once we add smoothing, the model starts performing much more reasonably. It no longer collapses whenever\n",
      "it sees a new or rare word, and we can see that recall shoots up to 1.0 while precision stays at a pretty good\n",
      "0.79. The F-score also jumps from 0.17 to 0.88, which shows a huge gain in overall balance. Smoothing\n",
      "fixes the zero-probability problem and lets the classifier generalize better.\n",
      "\n",
      "For spam filtering specifically, recall is the metric that matters the most, since missing spam (false negatives)\n",
      "is worse than occasionally flagging a ham email. But precision is still important because if too many good\n",
      "emails get flagged, users won’t trust the filter. Because of that, the F-score ends up being the most useful\n",
      "measure here, since it balances both precision and recall and gives a clearer picture of real-world performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q9d\n",
    "### ESSAY\n",
    "### QUESTION: Compare the performance of your two models. What do you notice about the unsmoothed model's\n",
    "#             predictions? Can you guess why this is happening? Which evaluation measure do you think is\n",
    "#             most relevant in our use case?\n",
    "\n",
    "### ENTER ANSWER IN BETWEEN THE \"\"\" \"\"\" INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Looking at the (expected) results, the unsmoothed model falls apart. Even though it gets a precision of 1.0,\n",
    "it almost never actually predicts spam correctly – the recall is only 0.09. That means it is too conservative\n",
    "and only calls something spam in very rare cases. The reason for this is that without Laplace smoothing,\n",
    "any word that never appeared in training for a certain class gets probability zero. Once you multiply (or add\n",
    "log-probabilities), that one zero wipes out the whole score for that class. So the model ends up defaulting\n",
    "to the safer class (ham) almost all the time, which explains why recall is so terrible.\n",
    "\n",
    "Once we add smoothing, the model starts performing much more reasonably. It no longer collapses whenever\n",
    "it sees a new or rare word, and we can see that recall shoots up to 1.0 while precision stays at a pretty good\n",
    "0.79. The F-score also jumps from 0.17 to 0.88, which shows a huge gain in overall balance. Smoothing\n",
    "fixes the zero-probability problem and lets the classifier generalize better.\n",
    "\n",
    "For spam filtering specifically, recall is the metric that matters the most, since missing spam (false negatives)\n",
    "is worse than occasionally flagging a ham email. But precision is still important because if too many good\n",
    "emails get flagged, users won’t trust the filter. Because of that, the F-score ends up being the most useful\n",
    "measure here, since it balances both precision and recall and gives a clearer picture of real-world performance.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac\n"
     ]
    }
   ],
   "source": [
    "# q9e1\n",
    "### MULTIPLE ANSWERS\n",
    "### QUESTION: What is the reason behind the different performances of the two models? (Select 2)\n",
    "\n",
    "#   a.) There are a lot of words with 0 probability in one or the other class.\n",
    "#   b.) The Class Prior for the negative class is larger than the Class Prior for the positive class.\n",
    "#   c.) The number of words in negative class is different from positive class.\n",
    "#   d.) An unsmoothed Naive Bayes model will always yield non-zero posterior class probabilities.\n",
    "\n",
    "### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), y.), and z.), enter \"xyz\")\n",
    "answer = \"ac\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# q9e2\n",
    "### MULTIPLE CHOICE\n",
    "### QUESTION: Which evaluation measure do you think is least relevant in our use case?\n",
    "#             [Hint: Think about the class imbalance between Ham/Spam and the meaning\n",
    "#             of each measure in relations to spam emails.]\n",
    "\n",
    "#   a.) Precision\n",
    "#   b.) Recall\n",
    "#   c.) F1 Score\n",
    "#   d.) Accuracy\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"d\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: Custom Partitioning and Secondary Sort - EXTRA CREDIT (Optional)\n",
    "\n",
    "Now that we have our model, we can analyse the results and think about future improvements.\n",
    "\n",
    "### Q10 Tasks:\n",
    "\n",
    "* __a.1) code + multiple choice:__ Let's look at the top ten words with the highest conditional probability in `Spam` and in `Ham`. We'll do this by writing a Hadoop job that sorts the model file (`NaiveBayes/Smoothed/NBmodel.txt`). Normally we'd have to run two jobs -- one that sorts on $P(word|ham)$ and another that sorts on $P(word|spam)$. However if we slighly modify the data format in the model file then we can get the top words in each class with just one job. We've written a mapper that will do just this for you. Read through __`NaiveBayes/model_sort_mapper.py`__. How will this mapper allow us to partition and sort our model file? \n",
    "\n",
    "\n",
    "* __a.2) code:__ Write a Hadoop job that uses our mapper and `/bin/cat` for a reducer to partition and sort. Print out the top 10 words in each class (where 'top' == highest conditional probability).[`HINT:` _this should remind you a lot of what we did in Question 6._]\n",
    "\n",
    "* __b)__ Print top words in each class. What do you notice about the 'top words' we printed? [`NOTE:` _you do not need to code anything for this task, but if you are struggling with it you could try changing 'k' and see what happens to the test set. We don't recommend doing this exploration with the Enron data because it will be harder to see the impact with such a big vocabulary_]\n",
    "\n",
    "* __b.1) multiple choice:__ How does the smoothing parameter 'k' affect the bias and the variance of our model?\n",
    "\n",
    "* __b.2) True/False:__ : Increasing the smoothing parameter 'k' would mostly affect the probabilities of words that occur much more in one class than another, and would have little effect on words whose probabilities were similar in each class?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q10a1\n",
    "### MULTIPLE CHOICE (Extra Credit)\n",
    "### QUESTION: How will this mapper allow us to partition and sort model file?\n",
    "\n",
    "#   a.) This mapper output two new fields: name of the class, and conditional probability\n",
    "#       of the corresponding class. We can use them to partition our file and sort on both\n",
    "#       classes simultaneously.\n",
    "\n",
    "#   b.) This mapper output two new fields: name of the class, and payload.\n",
    "#       We can use them to sort based on the payload.\n",
    "\n",
    "#   c.) This mapper output two new fields: name of the class, and Class Prior.\n",
    "#       We can use them to sort based on the Class Prior.\n",
    "\n",
    "#   d.) This mapper output two new fields: name of the class, and conditional\n",
    "#       probability of the both classes. We can use them to partition our file and\n",
    "#       sort on both classes simultaneously.\n",
    "\n",
    "#   e.) None of the provided responses are correct.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model-sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.4.jar] /tmp/streamjob16796280175296395.jar tmpDir=null\n",
      "2025-09-21 07:42:02,185 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:42:02,464 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:42:02,995 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.142.0.14:8032\n",
      "2025-09-21 07:42:02,996 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.142.0.14:10200\n",
      "2025-09-21 07:42:03,228 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1758422346528_0055\n",
      "2025-09-21 07:42:03,599 WARN concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2025-09-21 07:42:03,601 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2025-09-21 07:42:03,665 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2025-09-21 07:42:03,986 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1758422346528_0055\n",
      "2025-09-21 07:42:03,988 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 07:42:04,192 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-09-21 07:42:04,192 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-09-21 07:42:04,259 INFO impl.YarnClientImpl: Submitted application application_1758422346528_0055\n",
      "2025-09-21 07:42:04,298 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1758422346528_0055/\n",
      "2025-09-21 07:42:04,300 INFO mapreduce.Job: Running job: job_1758422346528_0055\n",
      "2025-09-21 07:42:13,402 INFO mapreduce.Job: Job job_1758422346528_0055 running in uber mode : false\n",
      "2025-09-21 07:42:13,403 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-09-21 07:42:21,521 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2025-09-21 07:42:22,529 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-09-21 07:42:29,602 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-09-21 07:42:35,652 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2025-09-21 07:42:37,668 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 07:42:44,717 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-09-21 07:42:47,746 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 07:42:48,758 INFO mapreduce.Job: Job job_1758422346528_0055 completed successfully\n",
      "2025-09-21 07:42:48,855 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=657263\n",
      "\t\tFILE: Number of bytes written=4349665\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=279064\n",
      "\t\tHDFS: Number of bytes written=639025\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=183893808\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=53620440\n",
      "\t\tTotal time spent by all map tasks (ms)=58268\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16990\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=58268\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16990\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=183893808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=53620440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4558\n",
      "\t\tMap output records=9110\n",
      "\t\tMap output bytes=639025\n",
      "\t\tMap output materialized bytes=657407\n",
      "\t\tInput split bytes=927\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9110\n",
      "\t\tReduce shuffle bytes=657407\n",
      "\t\tReduce input records=9110\n",
      "\t\tReduce output records=9110\n",
      "\t\tSpilled Records=18220\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=1840\n",
      "\t\tCPU time spent (ms)=18160\n",
      "\t\tPhysical memory (bytes) snapshot=6421381120\n",
      "\t\tVirtual memory (bytes) snapshot=53483823104\n",
      "\t\tTotal committed heap usage (bytes)=5336203264\n",
      "\t\tPeak Map Physical memory (bytes)=617738240\n",
      "\t\tPeak Map Virtual memory (bytes)=4468592640\n",
      "\t\tPeak Reduce Physical memory (bytes)=343453696\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4474736640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=278137\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=639025\n",
      "2025-09-21 07:42:48,856 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model-sorted\n"
     ]
    }
   ],
   "source": [
    "# part a - write your Hadoop job here (sort smoothed model on P(word|class))\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r -f {HDFS_DIR}/smooth-model-sorted\n",
    "\n",
    "# Hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k4,4nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/model_sort_mapper.py \\\n",
    "  -mapper model_sort_mapper.py \\\n",
    "  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "  -input {HDFS_DIR}/smooth-model \\\n",
    "  -output {HDFS_DIR}/smooth-model-sorted \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== PART-00000===============\n",
      "the\t453,535,0.03916832024846864,0.03977441377263283\tspam\t0.039774\t\n",
      "the\t453,535,0.03916832024846864,0.03977441377263283\tham\t0.039168\t\n",
      "of\t188,252,0.016305754464670866,0.01877411694864945\tspam\t0.018774\t\n",
      "of\t188,252,0.016305754464670866,0.01877411694864945\tham\t0.016306\t\n",
      "for\t148,153,0.012854801138814598,0.011427723360047493\tham\t0.012855\t\n",
      "for\t148,153,0.012854801138814598,0.011427723360047493\tspam\t0.011428\t\n",
      "enron\t116,0,0.010094038478129584,7.420599584446424e-05\tham\t0.010094\t\n",
      "i\t113,106,0.009835216978690364,0.007940041555357673\tham\t0.009835\t\n",
      "i\t113,106,0.009835216978690364,0.007940041555357673\tspam\t0.007940\t\n",
      "or\t41,88,0.003623500992149081,0.0066043336301573165\tspam\t0.006604\t\n",
      "cat: Unable to write to output stream.\n",
      "============== PART-00001===============\n",
      "ect\t378,0,0.03269778276248814,7.420599584446424e-05\tham\t0.032698\t\n",
      "and\t258,277,0.022344922784919334,0.020629266844761057\tham\t0.022345\t\n",
      "and\t258,277,0.022344922784919334,0.020629266844761057\tspam\t0.020629\t\n",
      "a\t168,274,0.014580277801742732,0.020406648857227663\tspam\t0.020407\t\n",
      "your\t35,271,0.003105857993270641,0.02018403086969427\tspam\t0.020184\t\n",
      "you\t80,252,0.006988180484858942,0.01877411694864945\tspam\t0.018774\t\n",
      "a\t168,274,0.014580277801742732,0.020406648857227663\tham\t0.014580\t\n",
      "com\t74,108,0.0064705374859805025,0.008088453547046601\tspam\t0.008088\t\n",
      "be\t90,84,0.00785091881632301,0.00630750964677946\tham\t0.007851\t\n",
      "is\t86,86,0.007505823483737383,0.006455921638468388\tham\t0.007506\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part b - print top words in each class\n",
    "for idx in range(2):\n",
    "    print(f\"============== PART-0000{idx}===============\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/smooth-model-sorted/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results:\n",
    "============== PART-00000===============\n",
    "the\t453,535,0.02811841942276725,0.029726581997670677\tham\t0.028118\t\n",
    "ect\t378,0,0.023473306082001735,5.546004104043037e-05\tham\t0.023473\t\n",
    "to\t350,420,0.021739130434782608,0.023348677278021184\tham\t0.021739\t\n",
    "and\t258,277,0.01604112473677691,0.015417891409239643\tham\t0.016041\t\n",
    "hou\t203,0,0.0126347082868822,5.546004104043037e-05\tham\t0.012635\t\n",
    "of\t188,252,0.011705685618729096,0.014031390383228884\tham\t0.011706\t\n",
    "a\t168,274,0.010466988727858293,0.015251511286118352\tham\t0.010467\t\n",
    "in\t160,157,0.009971509971509971,0.008762686484387999\tham\t0.009972\t\n",
    "for\t148,153,0.00922829183698749,0.008540846320226277\tham\t0.009228\t\n",
    "on\t122,95,0.007617985878855444,0.005324163939881316\tham\t0.007618\t\n",
    "cat: Unable to write to output stream.\n",
    "============== PART-00001===============\n",
    "the\t453,535,0.02811841942276725,0.029726581997670677\tspam\t0.029727\t\n",
    "to\t350,420,0.021739130434782608,0.023348677278021184\tspam\t0.023349\t\n",
    "and\t258,277,0.01604112473677691,0.015417891409239643\tspam\t0.015418\t\n",
    "a\t168,274,0.010466988727858293,0.015251511286118352\tspam\t0.015252\t\n",
    "your\t35,271,0.002229654403567447,0.01508513116299706\tspam\t0.015085\t\n",
    "of\t188,252,0.011705685618729096,0.014031390383228884\tspam\t0.014031\t\n",
    "you\t80,252,0.005016722408026756,0.014031390383228884\tspam\t0.014031\t\n",
    "in\t160,157,0.009971509971509971,0.008762686484387999\tspam\t0.008763\t\n",
    "for\t148,153,0.00922829183698749,0.008540846320226277\tspam\t0.008541\t\n",
    "it\t30,119,0.0019199801808497462,0.0066552049248516446\tspam\t0.006655\t\n",
    "cat: Unable to write to output stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q10b1\n",
    "### MULTIPLE CHOICE (Extra Credit)\n",
    "### QUESTION: How does the smoothing parameter 'k' affect the bias and the variance of our model?\n",
    "\n",
    "#   a.) Increasing k reduces the variance of our model, and increases the bias of our model.\n",
    "#   b.) Increasing k reduces the variance of our model, and reduces the bias of our model.\n",
    "#   c.) Increasing k increases the variance of our model, and reduces the bias of our model.\n",
    "#   d.) Increasing k increases the variance of our model, and increases the bias of our model.\n",
    "\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "# q10b2\n",
    "### TRUE OR FALSE (Extra Credit)\n",
    "### QUESTION: Increasing the smoothing parameter 'k' would mostly affect the probabilities of\n",
    "#             words that occur much more in one class than another, and would have little\n",
    "#             effect on words whose probabilities were similar in each class?\n",
    "\n",
    "### ENTER \"t\" for True or \"f\" for False.\n",
    "answer = \"f\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have completed HW2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE FOLLOWING FILES ARE REQUIRED TO BE UPLOADED FOR GRADESCOPE\n",
    "\n",
    "#### Please make sure that the filenames exactly match:\n",
    "- HW2.ipynb\n",
    "- mapper.py\n",
    "- reducer.py\n",
    "- chineseResults.txt\n",
    "- chineseModelUnsmoothed.txt\n",
    "- chineseModelSmoothed.txt\n",
    "- Unsmoothed_results.txt\n",
    "- Smoothed_results.txt\n",
    "- Unsmoothed_NBmodel.txt\n",
    "- Smoothed_NBmodel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
