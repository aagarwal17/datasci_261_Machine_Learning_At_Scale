{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Intro to the Map Reduce Paradigm  \n",
    "\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Summer 2025`__\n",
    "\n",
    "Welcome to Machine Learning at Scale! This first homework assignment introduces one of the core strategies in distributed processing: divide and conquer. We'll use the simplest of tasks, word counting, to illustrate the difference between a scalable and non-scalable algorithm. You will be working with the text of _Alice in Wonderland_ to put these ideas into practice using Python and Bash scripting. By the end of this week you should be able to:\n",
    "* ... __describe__ the Bias-Variance tradeoff as it applies to Machine Learning.\n",
    "* ... __explain__ why we consider word counting to be an \"Embarrassingly Parallel\" task.\n",
    "* ... __estimate__ the runtime of embarrassingly parallel tasks using \"back of the envelope\" calculations.\n",
    "* ... __implement__ a Map Reduce algorithm using the Command Line.\n",
    "* ... __set-up__ a Docker container and know why we use them for this course.\n",
    "\n",
    "You will also  become familiar (if you aren't already) with `defaultdict`, `re` and `time` in Python, linux piping and sorting, and Jupyter magic commands `%%writefile` and `%%timeit`. \n",
    "\n",
    "\n",
    "__IMPORTANT:__ If you're not familiar with linux, you should read the following tutorial reagrding **piping** and **redirecting**: https://ryanstutorials.net/linuxtutorial/piping.php You will need to understand the differences to answer some of the later questions.\n",
    "\n",
    "__Please refer to the HW1 Assignment in bCourses for detailed submission instructions__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/HW1\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to your local media notebook location \n",
    "%cd /media/notebooks/Assignments/HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=8, micro=15, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm you are running Python 3:\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder for any data you download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜dataâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "# NOTE: the contents of this directory will be ignored by git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Introductions\n",
    "\n",
    "`The Caterpillar and Alice looked at each other for some time in silence: at last the Caterpillar took the hookah out of its mouth, and addressed her in a languid, sleepy voice. \"Who are you?\" said the Caterpillar.`\n",
    "\n",
    "<div style=\"text-align: right\"> -- Lewis Carroll, <i>Alice's Adventures in Wonderland</i>, Chapter 4 </div>\n",
    "\n",
    "\n",
    "__a) Short Essay:__ Tell us about yourself! Briefly describe where you live, how far along you are in MIDS, what other classes you are taking and what you want to get out of w261."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, my name is Arun Agarwal. I am from West Chester, Pennsylvania, a little bit outside of Philadelphia. I have been here my whole life. I am two courses away from finishing MIDS, having already taken all of the foundational courses and 266. I want MIDS to help me accelerate my career as a Data Scientist. I currently work as a Machine Learning Engineer at a large finance company, and my team's work revolves heavily around Generative AI, Machine Learning Models, and NLP; thus, the three advanced courses I am taking/have taken revolve around these topics!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Hello, my name is Arun Agarwal. I am from West Chester, Pennsylvania, a little bit outside of Philadelphia. I have been here my whole life. I am two courses away from finishing MIDS, having already taken all of the foundational courses and 266. I want MIDS to help me accelerate my career as a Data Scientist. I currently work as a Machine Learning Engineer at a large finance company, and my team's work revolves heavily around Generative AI, Machine Learning Models, and NLP; thus, the three advanced courses I am taking/have taken revolve around these topics!\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Bias - Variance\n",
    "__a) Short Essay:__ In 1-2 sentences (~200 andÂ absolutely no moreÂ than 300 words!), explain the bias-variance trade off. Describe what it means to \"decompose\" sources of error. How is this used in machine learning? Please also cite any sources that informed your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As we mentioned in class, the bias-variance trade-off can be explained in multiple ways. I will reference it as a balance between error from simplifying assumptions/underfitting data (bias) and error from sensitivity to changes in the training data that overfit the data (variance). The goal with this trade-off is to find the best spot between underfitting and overfitting the data, minimizing total prediction error. Decomposing sources of error means to break down the model's error into their parts, the bias, the variance, and irreducible noise. This allows us to see where the error stems from, either the assumptions made, model sensitivity, or possible unavoidable randomness. We use this in machine learning in multiple areas, but generally, it is to guide what kind of model we select, the type of regularization, and controlling complexity to achieve generalization rather than memorization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "As we mentioned in class, the bias-variance trade-off can be explained in multiple ways. I will reference it as a balance between error from simplifying assumptions/underfitting data (bias) and error from sensitivity to changes in the training data that overfit the data (variance). The goal with this trade-off is to find the best spot between underfitting and overfitting the data, minimizing total prediction error. Decomposing sources of error means to break down the model's error into their parts, the bias, the variance, and irreducible noise. This allows us to see where the error stems from, either the assumptions made, model sensitivity, or possible unavoidable randomness. We use this in machine learning in multiple areas, but generally, it is to guide what kind of model we select, the type of regularization, and controlling complexity to achieve generalization rather than memorization.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Tokenizing\n",
    "A number of our assignments this term will involve extracting information from text. A common preprocessing step when working with raw files is to 'tokenize' (i.e. extract words from) the text. Within the field of Natural Language Processing a lot of thought goes into what specific tokenizing makes most sense for a given task. For example, you might choose to remove punctuation or to consider punctuation symbols  'tokens' in their own right. __In this question you'll use the Python `re` module to create a tokenizer to use when you perform Word Count on the _Alice In Wonderland_ text.__\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) Short Essay:__ In the Naive Bayes algorithm (which we'll implement next week), we'll estimate the _likelihood_ of a word by counting the number of times it appears and dividing by the size of the vocabulary (total number of unique words). Using the text: *\"Alice had an adventure that took alice to wonderland\"*, give a concrete example of how two different tokenizers could cause us to get two different results on this calculation. [`HINT`: _you should not need to read up on Naive Bayes to answer this question._]  \n",
    "  \n",
    "\n",
    "* __b) Multiple Choice:__ When tokenizing in this assignment we'll remove punctuation and discard numerical digits by making everything lowercase and then capturing only consecutive letters a to z. Suppose __`tokenize(x)`__ is a Python function that performs the desired tokenization. What would __`tokenize(\"By-the-bye, what became of Alice's 12 hats?!\")`__ output?\n",
    "\n",
    "\n",
    "* __c) code:__  Fill in the regular expression pattern in the cell labeled `part c` so that the subsequent call to `re.findall(RE_PATTERN, ...)` returns the tokenization described above. [`HINT`: _we've taken care of the lowercase part for you. If regex is new to you, check out the [`re`  documentation](https://docs.python.org/3/library/re.html) or [this PyMOTW tutorial](https://pymotw.com/2/re/)._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "\n",
    "> __a)__ Type your answer below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Different tokenizers could cause us to get two different results on the calculation based on how words are counted or the vocabulary size. For example if one of the tokenizers is case-sensitive, it will count \"Alice\" and \"alice\" as separate words, making the vocab size 9 and each probability 1/9. If, instead, our tokenizer was case-insensitive, \"alice\" would now appear twice, creating a vocab size of 8 and a probability of 2/8 or 1/4. Thus, the type of tokenizer can cause differences in the word probabilities, changing classification outcomes potentially.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3a\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Different tokenizers could cause us to get two different results on the calculation based on how words are counted or the vocabulary size. For example if one of the tokenizers is case-sensitive, it will count \"Alice\" and \"alice\" as separate words, making the vocab size 9 and each probability 1/9. If, instead, our tokenizer was case-insensitive, \"alice\" would now appear twice, creating a vocab size of 8 and a probability of 2/8 or 1/4. Thus, the type of tokenizer can cause differences in the word probabilities, changing classification outcomes potentially.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q3b\n",
    "### MULTIPLE CHOICE\n",
    "#   a.) ['by', 'the', 'bye', 'what', 'became', 'of', 'alice', 's', 'hats']\n",
    "#   b.) ['by', 'the', 'bye', 'what', 'became', 'of', 'alices', '12', 'hats']\n",
    "#   c.) ['by', 'the', 'bye', 'what', 'became', 'of', 'alices', 'hats']\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter 'f')\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3c\n",
    "def regex_tokenizer(text=\"By-the-bye, what became of Alice's 12 hats?!\"):\n",
    "    # BEGIN SOLUTION\n",
    "    regex = r\"[a-z]+\"\n",
    "    # END SOLUTION\n",
    "\n",
    "    RE_PATTERN = re.compile(regex)\n",
    "    return re.findall(RE_PATTERN, text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "`\"Please would you tell me\", said Alice, a little timidly, for she was not quite sure whether it was good manners for her to speak first, \"why your cat grins like that?\"`  \n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, <i>Alice's Adventures in Wonderland</i>, Chapter 4</div>\n",
    "\n",
    "For the main part of this assignment we'll be working with the free plain text version of _Alice's Adventures in Wonderland_ available from Project Gutenberg. __Use the first two cells below to download this text and preview the first few lines.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://261-hw-data/main/Assignments/HW1/data/alice.txt...\n",
      "/ [1 files][166.6 KiB/166.6 KiB]                                                \n",
      "Operation completed over 1 objects/166.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Download Full text \n",
    "!mkdir -p data/\n",
    "!gsutil cp gs://261-hw-data/main/Assignments/HW1/data/alice.txt data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of Aliceâ€™s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the first few lines:\n",
    "\n",
    "# NOTE: If you are working in JupyterLab on Docker you may not see the output \n",
    "# below due to an encoding issue... in that case, use a terminal on Docker to \n",
    "# execute this head command and confirm that the file has downloaded properly, \n",
    "# this encoding issue should not affect your work on subsequent HW items.\n",
    "\n",
    "!head -n 6 data/alice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like you to develop a habit of creating small files with simulated data for use in developing, debugging, and testing your code. The Jupyter magic command `%%writefile` is a convenient way to do this. __Run the following cells to create a test data file for use in our word counting task.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "# confirm the file was created in the data directory using a grep command:\n",
    "!ls data | grep test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Word Count in Python\n",
    "\n",
    "Over the course of this term you will also become very familiar with writing Python programs that read from standard input and using Linux piping commands to run these programs and save their output to file. __In this question you will write a short Python script to perform the Word Count task and then run your script on the _Alice in Wonderland_ text__. You can think of this like a \"baseline implementation\" that we'll later compare to the parallelized version of the same task.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) Code in `wordCount.py` and Submit on bCourses:__ Complete the Python script in the file __`wordCount.py`__. Read the docstrings carefully to be sure you understand the expected behavior of this function. Please do not code outside of the marked location.\n",
    "\n",
    "\n",
    "* __b) testing:__ Run the cell marked `part b` to call your script on the test file we created above. Confirm that your script returns the correct counts for each word by visually comparing the output to the test file. \n",
    "\n",
    "\n",
    "* __c) results:__ When you are confident in your implementation, run the cell marked `part c` to count the number of occurrences of each word in _Alice's Adventures in Wonderland_. In the same cell we'll pipe the output to file. Then use the provided `grep` commands to check your answers.\n",
    "\n",
    "\n",
    "* __d) Short Essay on bCourses:__ Suppose you decide that you'd really like  a word and its plural (e.g. 'hatter' and 'hatters' or 'person' and 'people') to be counted as the same word. After we have run the wordcount would it be more efficient to post-process your output file or discard your output file and start the analysis over with a new tokenizer? Explain your reasoning briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a-c)__ _Complete the coding portions of this question before answering 'd'._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part a - DO YOUR WORK IN wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "This script reads lines from STDIN and returns a list of\n",
      "all words and the count of how many times they occurred.\n",
      "\n",
      "INPUT:\n",
      "    a text file\n",
      "OUTPUT FORMAT:\n",
      "    word \\t count\n",
      "USAGE:\n",
      "    python wordCount.py < yourTextFile.txt\n",
      "\n",
      "Instructions:\n",
      "    Fill in the missing code below so that the script\n",
      "    prints tab separated word counts to Standard Output.\n",
      "    NOTE: we have performed the tokenizing for you, please\n",
      "    don't modify the provided code or you may fail unit tests.\n",
      "\"\"\"\n",
      "\n",
      "# imports\n",
      "import sys\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "counts = defaultdict(int)\n",
      "\n",
      "# stream over lines from Standard Input\n",
      "for line in sys.stdin:\n",
      "\n",
      "    # tokenize\n",
      "    line = line.strip()\n",
      "    words = re.findall(r'[a-z]+', line.lower())\n",
      "\n",
      "############ YOUR CODE HERE #########\n",
      "    # count each word\n",
      "    for word in words:\n",
      "        counts[word] += 1\n",
      "        \n",
      "for word, count in counts.items():\n",
      "    print(f\"{word}\\t{count}\")\n",
      "\n",
      "\n",
      "############ (END) YOUR CODE #########\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL as IS.\n",
    "!cat wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\t3\n",
      "is\t2\n",
      "a\t2\n",
      "small\t3\n",
      "test\t3\n",
      "file\t3\n",
      "for\t1\n",
      "has\t1\n",
      "two\t1\n",
      "lines\t1\n"
     ]
    }
   ],
   "source": [
    "# part b - DO NOT MODIFY THIS CELL, just run it as is to test your script\n",
    "!python wordCount.py < data/alice_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part c - DO NOT MODIFY THIS CELL, just run it as is to perform the word count.\n",
    "!python wordCount.py < data/alice.txt > data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 words & their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1839\n",
      "project\t88\n",
      "gutenberg\t98\n",
      "ebook\t13\n",
      "of\t638\n",
      "alice\t403\n",
      "s\t222\n",
      "adventures\t11\n",
      "in\t435\n",
      "wonderland\t7\n"
     ]
    }
   ],
   "source": [
    "!head data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"alice\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "!grep alice data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "# q4c1\n",
    "##############################\n",
    "\n",
    "# ENTER ANSWER HERE\n",
    "num_alice_counts = 403\n",
    "\n",
    "##############################\n",
    "# DON'T TOUCH\n",
    "print(num_alice_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"hatter\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatter\t56\n",
      "hatters\t1\n"
     ]
    }
   ],
   "source": [
    "# q4c2\n",
    "!grep hatter data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "# q4c2\n",
    "##############################\n",
    "\n",
    "# ENTER ANSWER HERE\n",
    "num_hatter_counts = 56\n",
    "\n",
    "##############################\n",
    "# DON'T TOUCH\n",
    "print(num_hatter_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"queen\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\t76\n",
      "queens\t1\n"
     ]
    }
   ],
   "source": [
    "!grep queen data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "# q4c3\n",
    "##############################\n",
    "\n",
    "# ENTER ANSWER HERE\n",
    "num_queen_counts = 76\n",
    "\n",
    "##############################\n",
    "\n",
    "# DON'T TOUCH\n",
    "print(num_queen_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It would be better to post-process the output. After running our wordcount, we have reduced the corpus token amount to the vocab size. Collapsing plurals will now become an O(V) aggregation, where we read each word and its count, map the word to its lemma (will need a dictionary for irregular words like person to people), and sum the counts. We then do not need to rescan or retokenize the whole text. A new tokenizer would mean redoing all the operations done with the original tokenizer, making it more expensive. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q4d\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "It would be better to post-process the output. After running our wordcount, we have reduced the corpus token amount to the vocab size. Collapsing plurals will now become an O(V) aggregation, where we read each word and its count, map the word to its lemma (will need a dictionary for irregular words like person to people), and sum the counts. We then do not need to rescan or retokenize the whole text. A new tokenizer would mean redoing all the operations done with the original tokenizer, making it more expensive. \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Unix Sorting Practice\n",
    "\n",
    "Another common task in this course's assignments will be to make strategic use of sorting.     \n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) Multiple Choice:__ What is the Big O complexity of the fastest comparison based sorting algorithms? [*`HINT`: If you need a Big O notation refresher, here's a [blog post](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/), a [cheatsheet](http://bigocheatsheet.com), and a [thorough explanation](http://pages.cs.wisc.edu/~vernon/cs367/notes/3.COMPLEXITY.html).*]\n",
    "\n",
    "* __b) Multiple Choice:__ What is the default sorting algorithm in MapReduce? What is the Big O complexity of this algorithm? Why do you think this algorithm was chosen? [*`HINT`: Julius Ceasar! (week 1 slides)*]\n",
    "\n",
    "* __c) Code in notebook:__ Write a unix command to check how many records are in your word count file. How many records are there?\n",
    "\n",
    "* __d) Code in notebook:__ Write a unix command to sort your word count file alphabetically. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) the results to `data/alice_counts_A-Z.txt`. [*`HINT`: if Unix sort commands are new to you, start with [this biowize blogpost](https://biowize.wordpress.com/2015/03/13/unix-sort-sorting-with-both-numeric-and-non-numeric-keys/) or [this unixschool tutorial](http://www.theunixschool.com/2012/08/linux-sort-command-examples.html)*]\n",
    "\n",
    "* __e) Code in notebook:__ Write a unix command to sort your word count file from highest to lowest count. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) your results to `data/alice_counts_sorted.txt`; then run the provided cell to print the top ten words. Compare your output to the expected output we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q5a\n",
    "### MULTIPLE CHOICE\n",
    "#   a.) O(n log n)\n",
    "#   b.) O(log n)\n",
    "#   c.) O(n)\n",
    "#   d.) O(2n)\n",
    "#   e.) O(n!)\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter 'f')\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "# q5b\n",
    "### MULTIPLE CHOICE\n",
    "#   a.) Bubble sort. This is due to the simplicity of the algorithm.\n",
    "#       The worst case time complexity of bubble sort is  ð‘‚(ð‘›2)\n",
    "\n",
    "#   b.) Quick sort. This is due to the partitioning nature of the algorithm\n",
    "#       making it a perfect fit for parallelization. The worst case time complexity of bubble sort is  ð‘‚(ð‘›â‹…ð‘™ð‘œð‘”ð‘›)\n",
    "\n",
    "#   c.) Mergesort. This is due to the divide and conquer nature of the algorithm making it\n",
    "#       a perfect fit for an embarrassingly parallel framework. The worst case time complexity of mergesort is  ð‘‚(ð‘›â‹…ð‘™ð‘œð‘”ð‘›)\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter 'f')\n",
    "answer = \"c\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3006 data/alice_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# part c - write a unix command to check how many records are in your word count file\n",
    "#### WRITE BELOW\n",
    "!wc -l data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3006\n"
     ]
    }
   ],
   "source": [
    "# q5c\n",
    "##############################\n",
    "\n",
    "# ENTER ANSWER HERE\n",
    "num_records = 3006\n",
    "\n",
    "##############################\n",
    "\n",
    "# DON'T TOUCH\n",
    "print(num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part d - unix command to sort your word counts alphabetically \n",
    "!sort data/alice_counts.txt > data/alice_counts_A-Z.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t695\n",
      "abide\t2\n",
      "able\t1\n",
      "about\t102\n",
      "above\t3\n",
      "absence\t1\n",
      "absurd\t2\n",
      "accept\t1\n",
      "acceptance\t1\n",
      "accepted\t2\n"
     ]
    }
   ],
   "source": [
    "# part d - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_A-Z.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t695\n",
      "abide\t2\n",
      "able\t1\n",
      "about\t102\n",
      "above\t3\n",
      "absence\t1\n",
      "absurd\t2\n",
      "accept\t1\n",
      "acceptance\t1\n",
      "accepted\t2\n"
     ]
    }
   ],
   "source": [
    "# q5d\n",
    "# DON'T MODIFY - Autograder Only\n",
    "with open(\"data/alice_counts_A-Z.txt\", \"r\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part e - unix command to sort your word counts from highest to lowest count\n",
    "!sort -k2,2nr data/alice_counts.txt > data/alice_counts_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1839\n",
      "and\t942\n",
      "to\t811\n",
      "a\t695\n",
      "of\t638\n",
      "it\t610\n",
      "she\t553\n",
      "i\t546\n",
      "you\t486\n",
      "said\t462\n"
     ]
    }
   ],
   "source": [
    "# part e - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1839\n",
      "and\t942\n",
      "to\t811\n",
      "a\t695\n",
      "of\t638\n",
      "it\t610\n",
      "she\t553\n",
      "i\t546\n",
      "you\t486\n",
      "said\t462\n"
     ]
    }
   ],
   "source": [
    "# q5e\n",
    "# DON'T MODIFY - Autograder Only\n",
    "with open(\"data/alice_counts_sorted.txt\", \"r\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>expected output for (d):</th>\n",
    "<th>expected output for (e):</th>\n",
    "<tr><td><pre>\n",
    "a\t695\n",
    "abide\t2\n",
    "able\t1\n",
    "about\t102\n",
    "above\t3\n",
    "absence\t1\n",
    "absurd\t2\n",
    "accept\t1\n",
    "acceptance\t1\n",
    "accepted\t2\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1839\n",
    "and\t942\n",
    "to\t811\n",
    "a\t695\n",
    "of\t638\n",
    "it\t610\n",
    "she\t553\n",
    "i\t546\n",
    "you\t486\n",
    "said\t462\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Parallel Word Count (part 1)\n",
    "What would happen if we tried to run our script on a much larger dataset? For one thing, it would take longer to run. However there is a second concern. The Python object that aggregates our counts (`defaultdict`) exists in memory on the machine running this notebook. If the vocabulary is too large for the memory space available we would crash the notebook. The solution? Divide and Conquer! Instead of running the script on the whole dataset at once, we could split our text up in to smaller 'chunks' and process them independently of each other. __In this question you'll use a bash script to \"parallelize\" your Word Count.__\n",
    "\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) Read provided code:__ The bash script `pWordCount_v1.sh` takes an input file, splits it into a specified number of 'chunks', and then applies an executable of your choice to each chunk. Read through this code and make sure you understand each line before you proceed. [*`HINT:` For now, ignore the 'student code' section -- you'll use that in part c.*]\n",
    "\n",
    "\n",
    "* __b) Short Essay:__ Below we've provided the command to use this script to apply your analysis (`wordCount.py`) to the _Alice_ text in 4 parallel processes. We'll redirect the results into a file called `alice_pCounts.txt.` Run this analysis and compare the count for the word 'alice' to your answer from Question 4. Explain what went wrong and describe what we have to add to `pWordCount_v1.sh` to fix the problem.\n",
    "\n",
    "\n",
    "* __c) Code in notebook:__ We've provided a python script, `aggregateCounts_v1.py`, which reads word counts from standard input and combines any duplicates it encounters. Read through this script to be sure you understand how it is written. Then follow the instructions in `pWordCount_v1.sh` to make a one-line modification so that it accepts `aggregateCounts_v1.py` as a 4th argument and uses this script to combine the chunk-ed word counts. Run the cell below to confirm that you now get the correct results for your 'alice' count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# pWordCount.sh\n",
      "# Usage: pWordCount.sh m testFile.txt mapper.py [reducer.py]\n",
      "# Input:\n",
      "#   m = number of processes (maps), e.g., 4\n",
      "#   inputFile = a text input file\n",
      "#   mapper = an executable that reads from STDIN and prints to STDOUT\n",
      "#   reducer = (optional) an executable that reads from STDIN and prints \n",
      "#             to STDOUT, if no reducer is provided, the framework will\n",
      "#             simply stream the mapper output.\n",
      "#\n",
      "# Instructions:\n",
      "#    For Q6a - Read this script and its comments closely. Ignore the\n",
      "#              part marked \"Otherwise\" in STEP 3, you'll use that later.\n",
      "#    For Q6c - Add a single line of code under '#Q6c' in STEP 3 so that\n",
      "#              the script pipes the output of each chunk's word countfiles\n",
      "#              into the second executable script provided as an argument,\n",
      "#              Note that we saved the script name (which was the 4th arg)\n",
      "#              to the variable $reducer. It can be executed by piping the\n",
      "#              counts to './$reducer' -- don't forget to redirect the output\n",
      "#              of this second script into $data.output\n",
      "\n",
      "# --------------------------------------------------------------------\n",
      "\n",
      "usage()\n",
      "{\n",
      "    echo ERROR: No arguments supplied\n",
      "    echo\n",
      "    echo To run use\n",
      "    echo \"pWordCount.sh m inputFile mapper.py [reducer.py]\"\n",
      "    echo Input:\n",
      "    echo \"number of processes/maps, EG, 4\"\n",
      "    echo \"mapper.py = an executable script to apply to each chunk in parallel\"\n",
      "    echo \"reducer.py = an executable script after the parallel processes are complete.\"\n",
      "    echo NOTE: if no reducer is supplied, we will simply combine the output files.\n",
      "}\n",
      "\n",
      "\n",
      "# print the usage message if this script is called without required args\n",
      "if [ $# -lt 3 ]\n",
      "  then\n",
      "    usage\n",
      "    exit 1\n",
      "fi\n",
      "\n",
      "# collect the arguments\n",
      "m=$1\n",
      "data=$2\n",
      "mapper=$3\n",
      "\n",
      "\n",
      "################# STEP 1: Split up the data #################\n",
      "# 'wc' determines the number of lines in the data\n",
      "# 'perl -pe' regex strips the piped wc output to a number\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
      "\n",
      "# determine the lines per chunk for the desired number of processes\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
      "\n",
      "# split the original file into chunks by line\n",
      "split -l $linesinchunk $data $data.chunk.\n",
      "\n",
      "\n",
      "############## STEP 2: Process in \"Parallel\" #################\n",
      "for datachunk in $data.chunk.*; do\n",
      "    # redirect the lines of text into the user supplied executable (mapper)\n",
      "    # and redirect STDOUT to a temporary file on disk\n",
      "    ./$mapper  < $datachunk > $datachunk.counts &\n",
      "done\n",
      "# wait for the mappers to finish their work\n",
      "wait\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############## STEP 3: Collect the results #################\n",
      "# 'ls' makes a list of the temporary count files\n",
      "# 'perl -pe' regex replaces line breaks with spaces\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
      "\n",
      "# If no 'reducer' executable was provided ...\n",
      "if [ $# -eq 3 ]\n",
      "  then\n",
      "    # combine all the count files into one\n",
      "    cat $countfiles > $data.output\n",
      "fi\n",
      "\n",
      "# Otherwise...\n",
      "if [ $# -eq 4 ]\n",
      "  then\n",
      "    reducer=$4\n",
      "    ################ YOUR CODE HERE #############\n",
      "    #Q6c\n",
      "\n",
      "\n",
      "    ################# (END YOUR CODE)###########\n",
      "fi\n",
      "\n",
      "\n",
      "############## STEP 4: Final Output #################\n",
      "# clean up the data chunks and temporary count files\n",
      "\\rm $data.chunk.*\n",
      "# display the content of the output file:\n",
      "cat $data.output\n",
      "\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# part b - RUN THIS CELL AS IS\n",
    "!cat pWordCount_v1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part b - make sure your scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x pWordCount_v1.sh\n",
    "!chmod a+x wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part b - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' 'wordCount.py' > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t113\n",
      "alice\t126\n",
      "alice\t122\n",
      "alice\t42\n"
     ]
    }
   ],
   "source": [
    "# part b - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When we ran the parallel version, the word â€œaliceâ€ showed up with four different counts because each chunk was processed separately and the results were never combined. Whatâ€™s missing is a reducer step that adds the counts together across all chunks so each word has one final total. To fix this, we need to update `pWordCount_v1.sh` so that if a reducer is given, all the mapper outputs are sent into it. This lets the reducer merge duplicate words and give one correct overall count.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q6b\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "When we ran the parallel version, the word â€œaliceâ€ showed up with four different counts because each chunk was processed separately and the results were never combined. Whatâ€™s missing is a reducer step that adds the counts together across all chunks so each word has one final total. To fix this, we need to update `pWordCount_v1.sh` so that if a reducer is given, all the mapper outputs are sent into it. This lets the reducer merge duplicate words and give one correct overall count.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part c - make sure the aggregateCounts script is executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x aggregateCounts_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part c - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v1.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "# part c - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Parallel Word Count (part 2)\n",
    "\n",
    "Congratulations, you've just implemented a Map-Reduce algorithm! From here on out, we'll refer to the two Python scripts you passed to `pWordCount_v1.sh` as '_mapper_' and '_reducer_'. The bash script itself served as our '_framework_' -- it split up the original input, then ___mapped___ our word counting script on to each chunk, then ___aggregated (a.k.a. reduced)___ the resulting files by piping them into our collation script. Unfortunately, as you may have realized already, there is a major scalability concern with this particular implementation. __In this question you'll fix our implementation of parallel word count so that it will be scalable.__\n",
    "\n",
    "__HINT:__ MapReduce uses the Merge-Sort algorithm under the hood. Linux `sort` command has a merge option which you can use to simulate the MapReduce framework. Use the `man sort` command to find more information on this option. \n",
    "\n",
    "### Q7 Tasks:\n",
    "\n",
    "* __a) Multiple Choice:__ What is the potential scalability problem with the provided implementation of `aggregateCounts_v1.py`? Why would this supposedly 'parallelized' Word Count potentially not work on a really large input corpus. [*`HINT:` See the intro to Q6*]\n",
    "\n",
    "\n",
    "* __b) Code in `pWordCount_v2.sh`:__ Fortunately, a 'strategic sort' can solve this problem. Read the instructions at the top of `pWordCount_v2.sh` carefully then make your changes that alphabetically sort the output from the mappers (`countfiles`) before piping them into the reducer script.\n",
    "\n",
    "\n",
    "* __c) Code in `aggregateCounts_v2.py`:__ Write the main part of `aggregateCounts_v2.py` so that it takes advantage of the sorted input to add duplicate counts without storing the whole vocabulary in memory. Refer to the file docstring for more detailed instructions. Confirm that your implementation works by running it on both the test and true data files.\n",
    "\n",
    "\n",
    "* __d) Short Essay:__ If you are paying close attention, this rewritten reducer sets us up for a truly scalable solution, but doesn't get us all the way there. In particular, while we chunked our data so it can be processed by multiple mappers, we're still streaming the entire dataset through one reduce script. If the vocabulary is too large to fit on a single computer, we might split the word counts in half after sorting them, then perform the reducing on two separate machines. Explain what could go wrong with this approach. (For now, ignore the question of how we'd sort a dataset that is too large to fit on a single machine and just focus on what might be wrong about the result of this split-in-half reducing).\n",
    "\n",
    "\n",
    "* __e) Short Essay:__ Can you come up with a different way of splitting up the data that would allow us to perform the reducing on separate machines without needing any postprocessing? This is a theoretical question -- don't worry if you don't know how to implement your idea in a bash script, just describe how you'd want to split the sorted counts into different files to be reduced separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "# q7a\n",
    "### MULTIPLE CHOICE\n",
    "#   a.) The implementation stores every line from sys.stdin in memory while accumulating counts.\n",
    "#       For a really large corpus and/or a cluster of machines with memory constraints\n",
    "#       this could become too large to run on a single node.\n",
    "#   b.) The implementation requires Python imports which may not be available on every\n",
    "#       node of the cluster for the mappers to use.\n",
    "#   c.) The implementation stores the entire vocabulary in a dictionary while accumulating counts.\n",
    "#       For a really large corpus and/or a cluster of machines with memory constraints this dictionary \n",
    "#       could become too large to run on a single node. In fact, from a scalability perspective this\n",
    "#       implementation is essentially equivalent to our original python word counter.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter 'f')\n",
    "answer = \"c\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# pWordCount.sh\n",
      "# Usage: pWordCount.sh m testFile.txt mapper.py [reducer.py]\n",
      "# Input:\n",
      "#   m = number of processes (maps), e.g., 4\n",
      "#   inputFile = a text input file\n",
      "#   mapper = an executable that reads from STDIN and prints to STDOUT\n",
      "#   reducer = (optional) an executable that reads from STDIN and prints \n",
      "#             to STDOUT, if no reducer is provided, the framework will\n",
      "#             simply stream the mapper output.\n",
      "#\n",
      "# Instructions:\n",
      "#    For Q7b - Ammend this script in STEP 2 and STEP 3 to \n",
      "#              alphabtetically sort the contents of each chunk before\n",
      "#              piping them into the reducer script and redirecting on to \n",
      "# .            $data.output.\n",
      "# --------------------------------------------------------------------\n",
      "\n",
      "usage()\n",
      "{\n",
      "    echo ERROR: No arguments supplied\n",
      "    echo\n",
      "    echo To run use\n",
      "    echo \"pWordCount.sh m inputFile mapper.py [reducer.py]\"\n",
      "    echo Input:\n",
      "    echo \"number of processes/maps, EG, 4\"\n",
      "    echo \"mapper.py = an executable script to apply to each chunk in parallel\"\n",
      "    echo \"reducer.py = an executable script after the parallel processes are complete.\"\n",
      "    echo NOTE: if no reducer is supplied, we will simply combine the output files.\n",
      "}\n",
      "\n",
      "\n",
      "# print the usage message if this script is called without required args\n",
      "if [ $# -lt 3 ]\n",
      "  then\n",
      "    usage\n",
      "    exit 1\n",
      "fi\n",
      "\n",
      "# collect the arguments\n",
      "m=$1\n",
      "data=$2\n",
      "mapper=$3\n",
      "\n",
      "\n",
      "################# STEP 1: Split up the data #################\n",
      "# 'wc' determines the number of lines in the data\n",
      "# 'perl -pe' regex strips the piped wc output to a number\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
      "\n",
      "# determine the lines per chunk for the desired number of processes\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
      "\n",
      "# split the original file into chunks by line\n",
      "split -l $linesinchunk $data $data.chunk.\n",
      "\n",
      "\n",
      "############## STEP 2: Process in \"Parallel\" #################\n",
      "for datachunk in $data.chunk.*; do\n",
      "    # redirect the lines of text into the user supplied executable (mapper)\n",
      "    # and redirect STDOUT to a temporary file on disk\n",
      "    \n",
      "    \n",
      "    ################ YOUR CODE HERE #############\n",
      "    # Modify the line from pWordCount_v1.sh, so that  \n",
      "    # each chunk is individually sorted. \n",
      "    cat $datachunk | ./$mapper | sort > $datachunk.counts &\n",
      "    ################# (END YOUR CODE)###########\n",
      "    \n",
      "done\n",
      "# wait for the mappers to finish their work\n",
      "wait\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############## STEP 3: Collect the results #################\n",
      "# 'ls' makes a list of the temporary count files\n",
      "# 'perl -pe' regex replaces line breaks with spaces\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
      "\n",
      "# If no 'reducer' executable was provided ...\n",
      "if [ $# -eq 3 ]\n",
      "  then\n",
      "    # combine all the count files into one\n",
      "    cat $countfiles > $data.output\n",
      "fi\n",
      "\n",
      "# Otherwise...\n",
      "if [ $# -eq 4 ]\n",
      "  then\n",
      "    reducer=$4\n",
      "    ################ YOUR CODE HERE #############\n",
      "    # write one line to merge the individual chunks,\n",
      "    # pipe to the reducer script, and redirect to \n",
      "    # output file\n",
      "    sort -m $countfiles | ./$reducer > $data.output\n",
      "    ################# (END YOUR CODE)###########\n",
      "fi\n",
      "\n",
      "\n",
      "############## STEP 4: Final Output #################\n",
      "# clean up the data chunks and temporary count files\n",
      "\\rm $data.chunk.*\n",
      "# display the content of the output file:\n",
      "cat $data.output\n",
      "\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# Run CELL AS IS\n",
    "!cat pWordCount_v2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "This script reads word counts from STDIN and aggregates\n",
      "the counts for any duplicated words.\n",
      "\n",
      "INPUT & OUTPUT FORMAT:\n",
      "    word \\t count\n",
      "USAGE (standalone):\n",
      "    python aggregateCounts_v2.py < yourCountsFile.txt\n",
      "\n",
      "Instructions:\n",
      "    For Q7 - Your solution should not use a dictionary or store anything   \n",
      "             other than a single total count - just print them as soon as  \n",
      "             you've added them. HINT: you've modified the framework script \n",
      "             to ensure that the input is alphabetized; how can you \n",
      "             use that to your advantage?\n",
      "\"\"\"\n",
      "\n",
      "# imports\n",
      "import sys\n",
      "\n",
      "\n",
      "################# YOUR CODE HERE #################\n",
      "current_word = None\n",
      "current_total = 0\n",
      "\n",
      "for line in sys.stdin:\n",
      "    line = line.strip()\n",
      "    if not line:\n",
      "        continue\n",
      "    try:\n",
      "        word, count = line.split(\"\\t\", 1)\n",
      "        count = int(count)\n",
      "    except ValueError:\n",
      "        # skip malformed lines\n",
      "        continue\n",
      "\n",
      "    if current_word == word:\n",
      "        current_total += count\n",
      "    else:\n",
      "        # output previous word before moving on\n",
      "        if current_word is not None:\n",
      "            print(f\"{current_word}\\t{current_total}\")\n",
      "        current_word = word\n",
      "        current_total = count\n",
      "# remove the last word\n",
      "if current_word is not None:\n",
      "    print(f\"{current_word}\\t{current_total}\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "################ (END) YOUR CODE #################\n"
     ]
    }
   ],
   "source": [
    "# Run CELL AS IS\n",
    "!cat aggregateCounts_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add permissions to your new files (RUN THIS CELL AS IS)\n",
    "!chmod a+x pWordCount_v2.sh\n",
    "!chmod a+x aggregateCounts_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t2\n",
      "file\t3\n",
      "for\t1\n",
      "has\t1\n",
      "is\t2\n",
      "lines\t1\n",
      "small\t3\n",
      "test\t3\n",
      "this\t3\n",
      "two\t1\n"
     ]
    }
   ],
   "source": [
    "# part c - test your code on the test file (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v2.sh 4 'data/alice_test.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part c - run your code on the Alice file (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "# part c - confirm that your 'alice' count is correct (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If we simply split the sorted list of counts into two pieces and run each piece through its own reducer, some words might be cut across the split. Parts of the same wordâ€™s counts would end up in both reducers, so each would only total what it sees. That would leave two different counts for the same word instead of one correct total. To avoid this, weâ€™d need to make sure all copies of a word go to the same reducer before summing.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q7d\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "If we simply split the sorted list of counts into two pieces and run each piece through its own reducer, some words might be cut across the split. Parts of the same wordâ€™s counts would end up in both reducers, so each would only total what it sees. That would leave two different counts for the same word instead of one correct total. To avoid this, weâ€™d need to make sure all copies of a word go to the same reducer before summing.\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instead of just cutting the file in half, we could break it up by the words themselves. For example, one reducer could get everything starting with â€œaâ€“mâ€ and another gets â€œnâ€“z.â€ Because every copy of the same word always goes to the same reducer, each one can total its words completely on its own, so thereâ€™s no need to merge results later.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q7e\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Instead of just cutting the file in half, we could break it up by the words themselves. For example, one reducer could get everything starting with â€œaâ€“mâ€ and another gets â€œnâ€“z.â€ Because every copy of the same word always goes to the same reducer, each one can total its words completely on its own, so thereâ€™s no need to merge results later.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Scalability Considerations\n",
    "\n",
    "In your reading for Week 2's live session, [Chapter1, section 2](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf) of _Data Intensive Text Processing with MapReduce_, Lin and Dyer discuss a number of \"Big Ideas\" that underlie large scale processing: __scale \"out,\" not \"up\"; assume failures are common; move processing to the data; process data sequentially and avoid random access; hide system-level details from the application developer; and seamless scalability.__ Part of our work this semester will be to interact with these ideas in a practical way, not just a conceptual one.\n",
    "\n",
    "### Q8 Tasks:\n",
    "\n",
    "* __a) Short Essay:__ What do Lin and Dyer consider the two features of an \"ideal algorithm\" from a scalability perspective?\n",
    "\n",
    "\n",
    "* __b) Multiple Choice:__ The mapper script below (created on the fly using a little Jupyter magic) will help us illustrate the concept of scalability. Run the provided code which passes this mapper script to our parallel computation 'framework' and runs the 'analysis' on the _Alice_ text file. Note that we've omitted a reducer for simplicity. What do you observe about the time it takes to run this \"algorithm\" when we use 1, 2 and 4 partitions? Does it meet Lin and Dyer's criteria?\n",
    "\n",
    "* __c) Short Essay:__ Let's try something similar with your Word Count analysis. Run the provided code to time your implementation with 1, 2, 4 and 8 partitions. What do you observe about the runtimes? Does this match your expectation? Speculate about why we might be seeing these times. What conclusions should we draw about the scalability of our implementation? [*`HINT:` Consider the limitations of both your machine and our implementation... there are some competing forces at work, what are they?*]\n",
    "\n",
    "\n",
    "* __d) Multiple Choice:__ Which components of your Map-Reduce algorithm are affected by a change in the number of partitions? Does increasing the number of partitions increase or decrease the total time spent on each of these portions of the task? What tradeoff does this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lin and Dyer say a truly scalable algorithm should break the work into many small pieces that can run at the same time without depending much on each other, and it should keep communication between machines as low as possible so results are easy to merge. This makes it simple to spread the job across many servers and keep speeding things up just by adding more machines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q8a\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Lin and Dyer say a truly scalable algorithm should break the work into many small pieces that can run at the same time without depending much on each other, and it should keep communication between machines as low as possible so results are easy to merge. This makes it simple to spread the job across many servers and keep speeding things up just by adding more machines.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to create the mapper referenced in `part b`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜demoâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This mapper reads from STDIN and waits 0.001 seconds per line.\n",
    "Its only purpose is to demonstrate one of the scalability ideas.\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "for line in sys.stdin:\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure the mapper is executable\n",
    "!chmod a+x demo/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the next three cells to apply our demo mapper with 1, 2 and 4 partitions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31 s Â± 9.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.27 s Â± 9.01 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 s Â± 5.67 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to repeat this process with your word count algorithm refereced in `part c`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 ms Â± 2.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 ms Â± 13.6 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 ms Â± 4.87 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 ms Â± 4.61 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 8 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605 ms Â± 6.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 16 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965 ms Â± 10.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 32 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# q8b\n",
    "### MULTIPLE CHOICE\n",
    "#   a.) Yes, doubling the number of map tasks (approximately) halves the runtime.\n",
    "#   b.) No, doubling the number of map tasks the runtime stays the same or even increases.\n",
    "\n",
    "### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is f.), enter 'f')\n",
    "answer = \"a\"\n",
    "\n",
    "\n",
    "#####################\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When we run our Word Count with 1, 2, 4, and 8 partitions, the runtime doesnâ€™t shrink perfectly with more partitions. It gets faster at first, but after a point, adding more partitions barely helps or can even make it a little slower. This happens because splitting the data and managing multiple processes adds extra work, and the reducer still has to handle everything on one machine. Plus, our computer only has a limited number of cores and memory. The takeaway is that parallelizing helps, but our implementation isnâ€™t fully scalable, more partitions donâ€™t always mean faster results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q8c\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "When we run our Word Count with 1, 2, 4, and 8 partitions, the runtime doesnâ€™t shrink perfectly with more partitions. It gets faster at first, but after a point, adding more partitions barely helps or can even make it a little slower. This happens because splitting the data and managing multiple processes adds extra work, and the reducer still has to handle everything on one machine. Plus, our computer only has a limited number of cores and memory. The takeaway is that parallelizing helps, but our implementation isnâ€™t fully scalable, more partitions donâ€™t always mean faster results.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Increasing the number of partitions in a MapReduce job mainly affects the map phase and the overhead of managing multiple chunks. Each mapper has less data to process, so it can finish faster, but having more partitions also means more processes to start, more temporary files to handle, and more work to coordinate everything. This creates a tradeoff: while smaller chunks can speed up individual mappers, the extra overhead can reduce the overall benefit, so making too many partitions can actually slow things down instead of helping.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q8d\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "Increasing the number of partitions in a MapReduce job mainly affects the map phase and the overhead of managing multiple chunks. Each mapper has less data to process, so it can finish faster, but having more partitions also means more processes to start, more temporary files to handle, and more work to coordinate everything. This creates a tradeoff: while smaller chunks can speed up individual mappers, the extra overhead can reduce the overall benefit, so making too many partitions can actually slow things down instead of helping.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Embarrassingly Parallel\n",
    "`\"If any one of them can explain it,\" said Alice, (she had grown so large in the last few minutes that she wasnâ€™t a bit afraid of interrupting him,) \"Iâ€™ll give him sixpence. I donâ€™t believe thereâ€™s an atom of meaning in it.\"`\n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, <i>Alice's Adventures in Wonderland</i>, Chapter 12</div>\n",
    "\n",
    "### Q9 Tasks:\n",
    "\n",
    "* __a) Short Essay:__ Describe what we mean by 'Embarrassingly Parallel' in terms of word counting. Does this term describe a 'task'? An 'implementation of a task'? \n",
    "\n",
    "* __b) Short Essay:__ Define this concept in terms of 'associative' and 'commutative' operations. [*`HINT:` Refer to Chapter 2 in DITP*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "\n",
    "> __a)__ Type your answer here!\n",
    "â€œEmbarrassingly parallelâ€ means a task that can be divided into completely independent parts, so each part can be done at the same time without needing to talk to the others. In word counting, this is like splitting a text into chunksâ€”each chunk can be counted separately, and nothing depends on the counts from the other chunks. The term really describes the task itself, not the way you implement it, because it tells you that the work can naturally be done in parallel with almost no coordination needed.\n",
    "\n",
    "\n",
    "> __b)__ Type your answer here!\n",
    "An operation is associative if it doesnâ€™t matter how you group the values when combining them, and commutative if the order of the values doesnâ€™t change the result. For word counting, adding up counts is both associative and commutative: you can combine partial counts in any order or grouping, and the final total stays the same. This makes word counting â€œembarrassingly parallelâ€, bc you can split the text into chunks, count words separately, and then merge the results without worrying about the order affecting the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â€œEmbarrassingly parallelâ€ means a task that can be divided into completely independent parts, so each part can be done at the same time without needing to talk to the others. In word counting, this is like splitting a text into chunksâ€”each chunk can be counted separately, and nothing depends on the counts from the other chunks. The term really describes the task itself, not the way you implement it, because it tells you that the work can naturally be done in parallel with almost no coordination needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q9a\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "â€œEmbarrassingly parallelâ€ means a task that can be divided into completely independent parts, so each part can be done at the same time without needing to talk to the others. In word counting, this is like splitting a text into chunksâ€”each chunk can be counted separately, and nothing depends on the counts from the other chunks. The term really describes the task itself, not the way you implement it, because it tells you that the work can naturally be done in parallel with almost no coordination needed.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An operation is associative if it doesnâ€™t matter how you group the values when combining them, and commutative if the order of the values doesnâ€™t change the result. For word counting, adding up counts is both associative and commutative: you can combine partial counts in any order or grouping, and the final total stays the same. This makes word counting â€œembarrassingly parallelâ€, bc you can split the text into chunks, count words separately, and then merge the results without worrying about the order affecting the outcome.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q9b\n",
    "### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n",
    "print(\n",
    "\"\"\"\n",
    "An operation is associative if it doesnâ€™t matter how you group the values when combining them, and commutative if the order of the values doesnâ€™t change the result. For word counting, adding up counts is both associative and commutative: you can combine partial counts in any order or grouping, and the final total stays the same. This makes word counting â€œembarrassingly parallelâ€, bc you can split the text into chunks, count words separately, and then merge the results without worrying about the order affecting the outcome.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have completed HW1! Please refer to the HW1 Assignment in bCourses for detailed submission instructions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "297px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "951px",
    "left": "0px",
    "right": "1561px",
    "top": "106px",
    "width": "600px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
