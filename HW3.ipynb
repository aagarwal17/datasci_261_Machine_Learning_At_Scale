{"cells": [{"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "c052a4ea-9f23-4194-afc4-ef6472f60a3b", "showTitle": false, "title": ""}}, "source": "# HW 3 - Synonym Detection In Spark\n\n\n__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Summer 2025`__\n\nYou made it through Hadoop! \ud83d\udc4f This will be your first assignment working in Spark so that's \ud83d\udd25.\u00a0 In the last homework assignment you performed Naive Bayes to classify documents as 'ham' or 'spam.' In doing so, we relied on the implicit assumption that the list of words in a document can tell us something about the nature of that document's content. We'll rely on a similar intuition this week: the idea that, if we analyze a large enough corpus of text, the list of words that appear in small window before or after a vocabulary term can tell us something about that term's meaning. This is similar to the intuition behind the word2vec algorithm. For HW3, you'll perform Synonym Detection by repurposing an algorithm commonly used in Natural Language Processing to perform document similarity analysis. In doing so you'll also become familiar with important datatypes for efficiently processing sparse vectors and a number of set similarity metrics (e.g. Cosine, Jaccard, Dice). By the end of this homework you should be able to:\n\n* ... __define__ the terms `one-hot encoding`, `co-occurrence matrix`, `stripe`, `inverted index`, `postings`, and `basis vocabulary` in the context of both synonym detection and document similarity analysis.\n* ... __explain__ the reasoning behind using a word stripe to compare word meanings.\n* ... __identify__ what makes set-similarity calculations computationally challenging.\n* ... __implement__ stateless algorithms in Spark to build stripes, inverted index and compute similarity metrics.\n* ... __identify__ when it makes sense to take a stripe approach and when to use pairs\n* ... __apply__ appropriate metrics to assess the performance of your synonym detection algorithm. \n\nAs always, your work will be graded both on the correctness of your output and on the clarity and design of your code.\n\nImportantly, you will no longer be working in `Local Disk/media`, but in `GCS` using your personal storage bucket. Your assignment is located at `GCS/Assignments/HW3/HW3.ipynb`.In fact, assignments 3, and 4 will all utilize storage buckets rather than `Local Disk/media`.\n\n\n__RECOMMENDED READING FOR HW3__:\t\nYour reading assignment for weeks 4 and 5 were fairly heavy and you may have glossed over the papers on dimension independent similarity metrics by [Zadeh et al](http://stanford.edu/~rezab/papers/disco.pdf) and pairwise document similarity by [Elsayed et al](https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf). If you haven't already, this would be a good time to review those readings, especially when it comes to the similarity formulas -- they are directly relevant to this assignment.\n\nDITP Chapter 4 - Inverted Indexing for Text Retrieval. While this text is specific to Hadoop, the Map/Reduce concepts still apply."}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "516b0fa3-8a8e-4279-a9a9-d0e2610edca0", "showTitle": false, "title": ""}}, "source": "## Notebook Set-Up\nBefore starting your homework run review the following  and run the code cells  to confirm your setup."}, {"cell_type": "markdown", "metadata": {}, "source": "### Data and HW output vs notebook Locations: private data bucket versus dataproc staging bucket\n\nWhen you create a DataProc cluster, HDFS is used as the default filesystem. In this course, we override this behavior by setting the defaultFS as a Cloud Storage bucket. The benefits of using Cloud Storage buckets are that your job results get to persist beyond the lifetime of the cluster (and btw latency on these cloud buckets is super low).\n\nIn this HW, you use your personal cloud bucket (and folders on them), known as DATA_BUCKET, as:\n* input and output for the Spark Apps that you will develop as part of your submission.  \n* you will save your jupyter notebooks and data on your personal cloud bucket also!\n\nThe datasets for this homework are preloaded into your `private Data Bucket` on Google Cloud. Recall that you created a private data bucket during the first week of semester. You may have called it w261-<your initials>. Jimi's bucket name is `w261-jgs`. To facilitate ease of access, we have set up location variables for the course-related data buckets. Your private data bucket  can be accessed via: \n\n* ```python\n    import os\n    DATA_BUCKET = os.getenv('DATA_BUCKET','')[:-1] # our private storage bucket location\n```\n\nThis DATA_BUCKET will be used for hosting the input data for this assignment and also to store output from your HW Assignment apps.\nAssociated with each DataProc cluster is a persistant storage bucket that we refer to as the DataProc Staging Bucket. You will be using this staging bucket to store notebooks and other files associated with your HW assignments, and  live sessions. The location of the staging bucket  is made available via `os.getenv(\"STAGING_BUCKET\")`. Since this bucket is persistent, we will no longer need to snapshot your student workspaces. \n    \nFor more background on Dataproc staging  buckets please see:\n* [https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Personal Data bucket:  gs://arun-fall-2025\n991.38 KiB  2025-10-04T21:54:27Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/261-Homework-3-Synonym-Detection-Using-Apache Spark.html\n181.55 KiB  2025-10-05T20:49:58Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/HW3.ipynb\n147.75 KiB  2025-09-14T23:17:19Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/HW3_summer.ipynb\n174.54 KiB  2025-10-04T21:57:53Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/HW_3_Fall_2018_2.ipynb\n158.08 KiB  2025-10-04T21:57:52Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/hw3_Fall_2018.ipynb\n107.83 KiB  2025-10-04T21:52:55Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/hw3_Workbook.ipynb\n     205 B  2025-10-04T22:12:32Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/readme.md\n     145 B  2025-10-05T17:49:06Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/sample_docs.txt\n489.21 KiB  2025-08-23T18:33:53Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/scaling-up-similarity-search.pdf\n     494 B  2025-10-05T18:17:51Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/systems_test.txt\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/.ipynb_checkpoints/\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/deprecated/\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/docker/\nTOTAL: 10 objects, 2305204 bytes (2.2 MiB)\n"}], "source": "# Access your personal data bucket and see whats there\n# (RUN THIS CELL AS IS)\nimport os\nDATA_BUCKET = os.getenv('DATA_BUCKET','')[:-1] # our private storage bucket location\nHW3_FOLDER = f\"{DATA_BUCKET}/notebooks/jupyter/Assignments/HW3\"\nprint(f\"Personal Data bucket:  {DATA_BUCKET}\")\n!gsutil ls -lh  {HW3_FOLDER}  #lets have a look the HW3 folder on our private storage bucket"}, {"cell_type": "code", "execution_count": 2, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "6f5abc60-dcd4-468a-b0f2-250552a36386", "showTitle": false, "title": ""}}, "outputs": [], "source": "import re\nimport ast\nimport time\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Spark is NOT currently running\n"}], "source": "# Autograder Skip\n\n# to shutdown Spark. (RUN THIS CELL AS IS)\n# if Spark is not running no problem\n# if Spark is running no problem also it will be shutdown\n#  \n\ntry:\n    spark\n    print(f\"{sc.master} appName: {sc.appName}\")\n    spark.sparkContext.stop()\n    del spark; del sc\n    print(f\"Just Shutdown Spark.\")\nexcept NameError:\n    print(\"Spark is NOT currently running\")"}, {"cell_type": "code", "execution_count": 4, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "6f5abc60-dcd4-468a-b0f2-250552a36386", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "starting Spark\n"}, {"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/06 04:47:40 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n25/10/06 04:47:41 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n25/10/06 04:47:41 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n25/10/06 04:47:41 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "# Autograder Skip\n\n# start Spark Session (RUN THIS CELL AS IS)\n#Step A: Start Spark by running the following cell\n\nfrom pyspark.sql import SparkSession\n\ntry:\n    spark\n    print(\"Spark is already running\")\n    print(f\"{sc.master} appName: {sc.appName}\")\nexcept NameError:\n    print('starting Spark')\n    app_name = 'hw3_notebook'\n    master = \"local[*]\"\n    spark = SparkSession\\\n            .builder\\\n            .appName(app_name)\\\n            .master(master)\\\n            .getOrCreate()\nsc = spark.sparkContext\n\n# Don't worry about messages shown below"}, {"cell_type": "markdown", "metadata": {}, "source": "## OPTIONAL: Setup Spark UI (jobs and stages)\n\nWeb UI (aka Application UI or webUI or Spark UI) is the web interface of a running Spark application to monitor and inspect Spark job executions in a web browser. The following is a screenshot of the Spark UI. Feel free to skip this section.  \n\nNote: these steps need to be repeated if the cloud shell is restarted\n\n<img src='https://www.dropbox.com/s/rithkjbboymr0ey/SparkUI_screenshot.png?raw=true' style='width:80%'>\n\n\nGetting access to Spark UI, you will need to create a tunnel from DataProc via the CloudShell. Please follow all steps depicted below :\n\n* Launch Spark first (as described above)!\n* The run the following command (make sure your specify the correct zone!) \n   \n``` bash\ngcloud compute ssh w261-m --zone us-central1-a --ssh-flag \"-L 8080:localhost:4040\"\n```\n"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "us-east1-c\nCopy the following command (and swap out the ZONE for your cluster zone) to cloud shell\nand run as shown in the screenshot below. \nAnd click on the PREVIEW ON PORT 8080 menu option in the web preview menu (look for the icon <>).\n\n\n"}, {"data": {"text/plain": "'   gcloud compute ssh w261-m --zone us-east1-c --ssh-flag \"-L 8080:localhost:4040\"    '"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "import os\nZONE = os.getenv('ZONE','') # zone where the DataProc cluster is running\nprint(ZONE)\nprint(\"Copy the following command (and swap out the ZONE for your cluster zone) to cloud shell\\n\"+ \n      \"and run as shown in the screenshot below. \\n\"+\n      \"And click on the PREVIEW ON PORT 8080 menu option in the web preview menu (look for the icon <>).\\n\\n\")\nf'   gcloud compute ssh w261-m --zone {ZONE} --ssh-flag \"-L 8080:localhost:4040\"    ' \n"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# gcloud compute ssh w261-m --zone us-east1-c --ssh-flag \"-L 8080:localhost:4040\""}, {"cell_type": "markdown", "metadata": {}, "source": "\n<img src='https://www.dropbox.com/s/tlb4uiakj2bx7qg/Three_steps_launch_SparkUI.png?raw=true' style='width:100%'>"}, {"cell_type": "code", "execution_count": 7, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "7909b057-ba61-490a-95e7-e60c7cb06c04", "showTitle": false, "title": ""}}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://w261-m.us-east1-c.c.w261-student-2208016264.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>hw3_notebook</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f3942320280>"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n# get Spark Session info (RUN THIS CELL AS IS)\nspark #NOTE the Spark UI link provided below as output from this command does not work. See next section on how to get that working."}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "5cf6e6b5-5993-4e10-9da2-47429244cfda", "showTitle": false, "title": ""}}, "source": "# Question 1: Spark Basics.\nIn your readings and live session demos for weeks 4 and 5 you got a crash course in working with Spark. We also talked about how Spark RDDs fit into the broader picture of distributed algorithm design. The questions below cover key points from these discussions. For short responses, please answer each one very briefly - 2 to 3 sentences.\n\n### Q1 Tasks:\n\n* __a) Matching:__ Match each statement to the answer choice that best fits the statement about Hadoop and Spark\n\n* __b) Multiple Answer:__ Spark generally follows the principles of statelessness (a.k.a. functional programming). In what ways does the framework allow the programmer to depart from this principle? (Select 2 answers)\n\n* __c) Short Response:__ In the context of Spark what is a 'DAG' and how does it relate to the difference between an 'action' and a 'transformation'? Why is it useful to pay attention to the DAG that underlies your Spark implementation?"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "aba\n"}], "source": "# q1a\n### MATCHING/FILL IN THE BLANKS USING THE CHOICES BELOW\n### QUESTIONS: Match each statement to the answer choice that best fits the statement about Hadoop and Spark\n# \n# 1.) _____   An open source parallel computation framework that facilitates iterative and multi-stage jobs.\n#             Its core abstraction is the idea of the Resilient Distributed Datasets (RDDs), which are lazily\n#             evaluated collections of records. RDDs can include collections of Key-Value pairs and\n#             transformations comprise a wide range of 'map' and 'reduce' like higher order functions.\n\n# 2.) _____   An open source parallel computation framework that includes its own distributed storage system.\n#             It could only perform a single-stage MapReduce job.\n\n# 3.) _____   It does not include its own distributed storage system & instead can be connected to existing resources\n#             like HDFS, Cassandra or S3 (for distributed storage) and YARN or Mesos (for cluster manager).\n\n\n#   a.) Spark\n#   b.) Hadoop\n\n### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT.\n#       Example: Suppose your answers for #1 is x.), y for #2, and z for #3.\n#           enter \"xyz\" as your answer.\nanswer = \"aba\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ab\n"}], "source": "# q1b\n### MULTIPLE ANSWERS\n### QUESTION: Spark generally follows the principles of statelessness (a.k.a. functional programming).\n#             In what ways does the framework allow the programmer to depart from this principle? (Select 2 answers)\n\n#   a.) Broadcast variables\n#   b.) Accumulator variables\n#   c.) Mapper\n#   d.) Reducer\n\n### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), y.), and z.), enter \"xyz\")\nanswer = \"ab\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nA DAG (Directed Acyclic Graph) in Spark is the execution plan representing the sequence of transformations \nneeded to compute an RDD. It consists of vertices representing RDDs and edges representing operations, where\nedges direct from earlier to later in the sequence. Transformations like map, filter, and reduceByKey are lazy\noperations that build the DAG but don't execute immediately. They simply add steps to the graph. Actions like\ncollect, count, and saveAsTextFile trigger execution by forcing Spark to evaluate the entire DAG and return\nresults to the driver or write to storage. Since transformations are lazily evaluated and only triggered by an\naction, Spark can optimize the execution plan before running any computation. Paying attention to the DAG is\nuseful because it reveals how many stages and shuffles your job requires, where wide transformations create\nexpensive stage boundaries. It helps identify optimization opportunities like eliminating unnecessary operations\nor reordering transformations. The DAG shows where data is being shuffled across the network, which is the major\nperformance bottleneck. Understanding the DAG helps debug failures by showing the lineage of transformations that\nled to an error. The DAG Scheduler uses this graph to split work into stages of tasks for execution.\n\n"}], "source": "# q1c\n### SHORT RESPONSE\n### QUESTION: In the context of Spark what is a 'DAG' and how does it relate to the difference between an 'action'\n#             and a 'transformation'? Why is it useful to pay attention to the DAG that underlies your Spark implementation?\n\n### ENTER ANSWER INSIDE THE PRINT STATEMENT.\nprint(\n\"\"\"\nA DAG (Directed Acyclic Graph) in Spark is the execution plan representing the sequence of transformations \nneeded to compute an RDD. It consists of vertices representing RDDs and edges representing operations, where\nedges direct from earlier to later in the sequence. Transformations like map, filter, and reduceByKey are lazy\noperations that build the DAG but don't execute immediately. They simply add steps to the graph. Actions like\ncollect, count, and saveAsTextFile trigger execution by forcing Spark to evaluate the entire DAG and return\nresults to the driver or write to storage. Since transformations are lazily evaluated and only triggered by an\naction, Spark can optimize the execution plan before running any computation. Paying attention to the DAG is\nuseful because it reveals how many stages and shuffles your job requires, where wide transformations create\nexpensive stage boundaries. It helps identify optimization opportunities like eliminating unnecessary operations\nor reordering transformations. The DAG shows where data is being shuffled across the network, which is the major\nperformance bottleneck. Understanding the DAG helps debug failures by showing the lineage of transformations that\nled to an error. The DAG Scheduler uses this graph to split work into stages of tasks for execution.\n\"\"\"\n)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "c2542400-9725-4996-94c2-313025d682ba", "showTitle": false, "title": ""}}, "source": "# Question 2: Similarity Metrics\n\n## Q2(a)  Cosine Similarity\nIn data analysis, cosine similarity is a measure of similarity between two sequences of numbers. For defining it, the sequences are viewed as vectors in an inner product space, and the cosine similarity is defined as the cosine of the angle between them, that is, the dot product of the vectors divided by the product of their lengths. More formally, cosine similarity between two non-zero vectors A and B builds on the Euclidean dot product formula and is defined as follows:\u00a0\n\n\n$${\\text{cosine similarity}}=S_{COS}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B} \u00a0\\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}= {\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{\\sqrt{\\sum_{i=1}^{n}{A_{i}^{2}}} \\cdot \\sqrt{\\sum_{i=1}^{n}{B_{i}^{2}}}}}$$\n\nWhere $A_i$ and $B_i$ are components of vector A and B in space respectively.\u00a0 For example, give A=[1,2], and B=[3,4],\n\n$$S_{COS}(A=[1,2],B=[3,4]):=\\cos(\\theta)={{\\mathbf{[1,2]}\\cdot\\mathbf{[3,4]}\\over \\|\\mathbf{[1,2]}\\|\\|\\mathbf{[3,4]}\\|}}={\\frac{11}{2.236 \\cdot 5.0}}=0.984$$\n\nLooking more closely at cosine similarity:\u00a0\n\n* It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle.\n* The cosine similarity always belongs to the interval [-1,1].\n* For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1.\n* The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].\n\nWith this as background, please complete the following code to calculate the cosine similarity between two generated vectors A and B, and report the similarity to three decimal places of these two vectors."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "A: [8 8 6 2 8 7 2 1 5 4]\nA: [4 5 7 3 6 4 3 7 6 1]\nA.B: 243\nA.B: 243\ncosine_sim(A, B): 0.857\n"}], "source": "# part a\nimport numpy as np\n# The points below have been selected to demonstrate the case for Cosine similarity\n\n# Cosine similarity\nnp.random.seed(2)\nA = np.random.randint(low=0, high=10, size=10, dtype=int)\nB = np.random.randint(low=0, high=10, size=10, dtype=int)\nprint(f\"A: {A}\")\nprint(f\"A: {B}\")\nprint(f\"A.B: {np.dot(A, B)}\") \nprint(f\"A.B: {A @ B}\") \n\n# Placeholder\ncosine_sim = 0\n\n# FIX CODE BELOW\n#######################\ncosine_sim = A @ B /(np.linalg.norm(A) * np.linalg.norm(B))\nprint(f\"cosine_sim(A, B): {cosine_sim:.3}\")  "}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0.857\n"}], "source": "# q2a\n### NUMERICAL INPUT\n### QUESTION: Using the previous cell, fix the code to calculate the cosine similarity between two\n#             generated vectors A and B, and report the similarity to three decimal places of\n#             these two vectors.\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. USE THE DECIMAL, NOT THE FRACTION. (i.e. \"0.062\", NOT \"1/16\")\n#       Please submit your response as a DECIMAL with at least 3 decimal places (as shown above)\n\nanswer = \"0.857\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Q2(b) \u00a0Jaccard Similarity\n\nJaccard similarity is another measure of similarity between two sequences of numbers. It is defined by the intersection divided by the union of two sets of numbers. Between two non-zero sets\u00a0$\\mathbf {A}$ and $\\mathbf {B}$, jaccard similarity can be defined by:\n$${\\text{jaccard similarity}}={|\\mathbf {A} \\cap \\mathbf {B}| \u00a0\\over |\\mathbf {A} \\cup \\mathbf {B}| } = {|\\mathbf {A} \\cap \\mathbf {B}| \\over |\\mathbf {A}| +|\\mathbf {B}| - |\\mathbf {A} \\cap \\mathbf {B}|}$$\n\nAs a result:\u00a0\n\n* Jaccard similarity is the ratio of shared elements to unshared elements.\u00a0 I.e., Jaccard Similarity\u00a0= (number of non-zero elements in both sets) / (number non-zero elements in either set)\n* The Jaccard similarity always belongs to the interval [0,1].\n\nWith this as background, please complete the following code to calculate the Jaccard similarity between two generated vectors A and B, and report similarity to three decimal places."}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "A: [0 1 1 0 0 1 0 1 0 1]\nB: [0 1 1 1 1 1 1 1 0 0]\nA intersection B: 4\njaccard_sim(A, B): 0.5\n"}], "source": "# part b\nimport numpy as np\n# The points below have been selected to demonstrate the case for Jaccard similarity\n#Jaccard similarity\nnp.random.seed(2)\nA = np.random.randint(low=0, high=2, size=10, dtype=int)\nB = np.random.randint(low=0, high=2, size=10, dtype=int)\nprint(f\"A: {A}\")\nprint(f\"B: {B}\")\nprint(f\"A intersection B: {np.count_nonzero(np.logical_and(A, B))}\")\n\n# Placeholder\njaccard_sim = 0\n\n# FIX CODE BELOW\n#######################\njaccard_sim = np.count_nonzero(np.logical_and(A, B)) /np.count_nonzero(np.logical_or(A, B))  # hint: np.logical_or\nprint(f\"jaccard_sim(A, B): {jaccard_sim:.3}\")"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0.5\n"}], "source": "# q2b\n### NUMERICAL INPUT\n### QUESTION: Using the previous cell, fix the code to calculate the jaccard similarity between two\n#             generated vectors A and B, and report the similarity to three decimal places of\n#             these two vectors.\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. USE THE DECIMAL, NOT THE FRACTION. (i.e. \"0.062\", NOT \"1/16\")\n#       Please submit your response as a DECIMAL with at least 3 decimal places (as shown above)\n\nanswer = \"0.5\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Q2(c)\u00a0 Overlap Similarity\nThe overlap coefficient is another measure of similarity between two sequences of numbers. It measures the overlap between two finite sets by using the size of the intersection divided by the smaller of the size of the two sets. Between two non-zero sets\u00a0$A$ and $B$, It can be defined by:\n\n$${\\displaystyle \\text{overlap coefficient}={\\frac {|A\\cap B|}{\\min(|A|,|B|)}}}$$\n\nAs a result,\n\n* In comparing two documents, overlap coefficient can be thought of as the percent of words in the document with fewer unique words that are also in the other document.\n* If set A is a subset of B or the converse, then the overlap coefficient is equal to 1.\n\nWith this as background, please complete the following code to calculate the overlap metric between two generated vectors A and B, and report similarity to three decimal places."}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "A: [0 1 1 0 0 1 0 1 0 1]\nB: [0 1 1 1 1 1 1 1 0 0]\nA intersection B: 4\noverlap_sim(A, B): 0.8\n"}], "source": "# part c\nimport numpy as np\n# The points below have been selected to demonstrate the case for Overlap similarity\n# Overlap similarity\nnp.random.seed(2)\nA = np.random.randint(low=0, high=2, size=10, dtype=int)\nB = np.random.randint(low=0, high=2, size=10, dtype=int)\nprint(f\"A: {A}\")\nprint(f\"B: {B}\")\nprint(f\"A intersection B: {np.count_nonzero(np.logical_and(A, B))}\")\n\n# Placeholder\noverlap_sim = 0\n\n# FIX CODE BELOW\n#######################\noverlap_sim = np.count_nonzero(np.logical_and(A, B)) / min(np.count_nonzero(A), np.count_nonzero(B)) # hint: min length\nprint(f\"overlap_sim(A, B): {overlap_sim:.3}\")"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0.8\n"}], "source": "# q2c\n### NUMERICAL INPUT\n### QUESTION: Using the previous cell, fix the code to calculate the overlap metric between two\n#             generated vectors A and B, and report the similarity to three decimal places of\n#             these two vectors.\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. USE THE DECIMAL, NOT THE FRACTION. (i.e. \"0.062\", NOT \"1/16\")\n#       Please submit your response as a DECIMAL with at least 3 decimal places (as shown above)\n\nanswer = \"0.8\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Q2(d)\u00a0 Dice Similarity\n\nDice similarity is another measure of similarity between two sequences of numbers. The metric equals twice the number of elements common to both sets divided by the sum of the number of elements in each set. Between two non-zero sets\u00a0A and B, dice similarity can be defined by:\n\n$${\\displaystyle \\text{dice coefficient}={\\frac {2|A\\cap B|}{|A|+|B|}}}$$\n\nAs a result,\n\n* In comparing two documents, dice coefficient represents the shared word rate across both documents.\n\nWith this as background, please complete the following code to calculate the Dice similarity between two generated vectors A and B, and report similarity to three decimal places.\u00a0"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "A: [0 1 1 0 0 1 0 1 0 1]\nB: [0 1 1 1 1 1 1 1 0 0]\nA intersection B: 4\ndice_sim(A, B): 0.667\n"}], "source": "# part d\nimport numpy as np\n# The points below have been selected to demonstrate the case for Dice similarity\n# Dice similarity\nnp.random.seed(2)\nA = np.random.randint(low=0, high=2, size=10, dtype=int)\nB = np.random.randint(low=0, high=2, size=10, dtype=int)\nprint(f\"A: {A}\")\nprint(f\"B: {B}\")\nprint(f\"A intersection B: {np.count_nonzero(np.logical_and(A, B))}\")\n\n# Placeholder\ndice_sim = 0\n\n# FIX CODE BELOW\n#######################\ndice_sim = (2 * np.count_nonzero(np.logical_and(A, B))) / (np.count_nonzero(A) + np.count_nonzero(B)) # hint: total non zero elements between vectors\nprint(f\"dice_sim(A, B): {dice_sim:.3}\")"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0.667\n"}], "source": "# q2d\n### NUMERICAL INPUT\n### QUESTION: Using the previous cell, fix the code to calculate the dice similarity between two\n#             generated vectors A and B, and report the similarity to three decimal places of\n#             these two vectors.\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. USE THE DECIMAL, NOT THE FRACTION. (i.e. \"0.062\", NOT \"1/16\")\n#       Please submit your response as a DECIMAL with at least 3 decimal places (as shown above)\n\nanswer = \"0.667\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "9b5fd240-e9d2-44a2-8fa8-9c6adfa5df05", "showTitle": false, "title": ""}}, "source": "# Question 3: Synonym Detection Strategy\n\nIn the Synonym Detection task we want to compare the meaning of words, not documents. For clarity, lets call the words whose meaning we want to compare `terms`. If only we had a 'meaning document' (or an embedding vector) for each `term` then we could easily use the document similarity strategy from Question 2 to figure out which `terms` have similar meaning (i.e. are 'synonyms'). Of course in order for that to work we'd have to reasonably believe that the words in these 'meaning documents' really do reflect the meaning of the `term`. For a good analysis we'd also need these 'meaning documents' to be fairly long -- the one or two sentence dictionary definition of a term isn't going to provide enough signal to distinguish between thousands and thousands of `term` meanings. In another way, the length of an embedding vector has to be big enough to represent the `term`.\n\nThis is where the idea of co-occurrence comes in. Just like DocSim makes the assumption that words in a document tell us about the document's meaning, we're going to assume that the set of words that 'co-occur' within a small window around our term can tell us some thing about the meaning of that `term`. Remember that we're going to make this 'co-words' list (a.k.a. 'stripe') by looking at a large body of text. This stripe is our 'meaning document' in that it reflects all the kinds of situations in which our `term` gets used in real language. So another way to phrase our assumption is: we think `terms` that get used to complete lots of the same phrases probably have related meanings. This may seem like an odd assumption but computational linguists have found that it works surprisingly well in practice, and was used in one of the famous Word2Vec embedding technique in NLP. Let's look at a toy example to build your intuition for why and how.\n\nConsider the opening line of Charles Dickens' _A Tale of Two Cities_:"}, {"cell_type": "code", "execution_count": 19, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "b7504e66-ef09-472a-909b-bea5b1f5e393", "showTitle": false, "title": ""}}, "outputs": [], "source": "corpus = \"\"\"It was the best of times, it was the worst of times, \nit was the age of wisdom it was the age of foolishness\"\"\""}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "7676fba9-0e45-4f63-876d-89132d7b25c1", "showTitle": false, "title": ""}}, "source": "There are a total of 10 unique words in this short 'corpus':"}, {"cell_type": "code", "execution_count": 20, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "520d6d4c-336a-4df2-b518-a1e48c00bc98", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['the', 'was', 'times', 'foolishness', 'best', 'worst', 'of', 'wisdom', 'it', 'age']\n"}], "source": "import re\n\nwords = list(set(re.findall(r'\\w+', corpus.lower())))\nprint(words)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "b095aa02-95a8-4ac1-9a1c-b7b6c985916a", "showTitle": false, "title": ""}}, "source": "But of these 10 words, 4 are so common that they probably don't tell us very much about meaning."}, {"cell_type": "code", "execution_count": 21, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "fec9f761-d5db-4c48-90cc-85303e7e3335", "showTitle": false, "title": ""}}, "outputs": [], "source": "stopwords = [\"it\", \"the\", \"was\", \"of\"]"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "d5a85035-6a79-476a-807b-cbcbdb71579e", "showTitle": false, "title": ""}}, "source": "So we'll ignore these 'stop words' and we're left with a 6 word vocabulary:"}, {"cell_type": "code", "execution_count": 22, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "117ecf80-a449-4cc5-9fa6-c5f5128a4daa", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"}], "source": "vocab = sorted([w for w in words if w not in stopwords])\nprint(vocab)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "b833d4bf-fe53-4ca1-bf82-733995aed493", "showTitle": false, "title": ""}, "tags": []}, "source": "Your goal in the tasks below is to assess which of these six words are most related to each other in meaning -- based solely on this short two line body of text.\n\n### Q3 Tasks:\n\n* __a) Multiple Choice:__ Given this six word vocabulary, how many `pairs` of words do we want to compare? \n\n* __b) Short Response:__ More generally for a vocabulary with $n$ unique words, how many `pairs` of words are there to make? Feel free to enter an expression using Latex/Equation in a cell below your answer.\n\n* __c) Code in Notebook:__ In the space provided below, create a `stripe` for each `term` in the vocabulary. This `stripe` should be the list of all other __other words__ (not including itself) that co-occur within a __5 word window__ or 5-gram (two words on either side) of the `term`'s position in the original text. In this exercise, use `['it', 'was', 'the','of']` as stopwords, just __ignore__ them from your 5 word vectors. When you are done, take a screenshot of the `stripes` dictionary and upload it to Digital Campus.\n\n* __d) Multiple Choice:__ Run the provided code to turn your stripes into a 1-hot encoded co-occurrence matrix. For our 6 word vocabulary how many entries are in this matrix? How many entries are zeroes? \n\n* __e) Code in Notebook:__ Complete the provided code to loop over all pairs and compute their cosine similarity. Please do not modify the existing code, just add your own in the section marked ## YOUR CODE HERE ## \n\n* __f) Multiple Choice:__ Which pairs of words have the highest 'similarity' scores? \n\n* __g) Short Response (BONUS):__ Are these words 'synonyms' in the traditional sense? In what sense are their meanings 'similar'? Explain how our results are contingent on the input text. What would change if we had a much larger corpus?"}, {"cell_type": "code", "execution_count": 23, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "429c15b1-d1bd-4b04-a6e7-175e336a1907", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CORPUS:\nIt was the best of times, it was the worst of times, \nit was the age of wisdom it was the age of foolishness\n\nVOCAB:\n['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"}], "source": "# for convenience, here are the corpus & vocab list again (RUN THIS CELL AS IS)\nprint(\"CORPUS:\")\nprint(corpus)\n\nprint('\\nVOCAB:')\nprint(vocab)"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "b\n"}], "source": "# q3a\n### MULTIPLE CHOICE\n### QUESTION: Given this six-word vocabulary, how many 'pairs' of words do we want to compare?\n\n#   a.) 6\n#   b.) 15\n#   c.) 10\n#   d.) 21\n\n### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\nanswer = \"b\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nGenerally for an n-word vocavulary, this is C(n,2) = n! / (2! (n \u2013 2)!)\nFor a vocabulary with n unique words, the number of pairs is:\n\nC(n,2) = n(n-1)/2 = n!/[2!(n-2)!]\n\nThis is because we're selecting 2 words from n words where order doesn't matter \n(the pair (word1, word2) is the same as (word2, word1)).\n\n"}], "source": "# q3b\n### SHORT RESPONSE\n### QUESTION: More generally for a vocabulary with 'n' unique words, how many `pairs` of words are there to make?\n#             Feel free to enter an expression using Latex/Equation in a cell below your answer.\n\n### IF YOU WANT TO WRITE IN LATEX, CREATE A MARKDOWN CELL BELOW AND PUT 'SEE ANSWER BELOW' INSIDE THE PRINT STATEMENT\nprint(\n\"\"\"\nGenerally for an n-word vocavulary, this is C(n,2) = n! / (2! (n \u2013 2)!)\nFor a vocabulary with n unique words, the number of pairs is:\n\nC(n,2) = n(n-1)/2 = n!/[2!(n-2)!]\n\nThis is because we're selecting 2 words from n words where order doesn't matter \n(the pair (word1, word2) is the same as (word2, word1)).\n\"\"\"\n)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "482cae9e-bee6-48ab-97e4-175f993e0834", "showTitle": false, "title": ""}}, "source": "* 5-gram demonstration for the `term` age:\n\n<img src='https://github.com/kyleiwaniec/w261_assets/blob/master/images/best-of-times.png?raw=true' style='width:80%'>"}, {"cell_type": "code", "execution_count": 26, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "72c12700-cffd-4ef4-b408-f5809acc6671", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'age': ['wisdom', 'foolishness'], 'best': ['times'], 'foolishness': ['age'], 'times': ['best', 'worst'], 'wisdom': ['age'], 'worst': ['times']}\n"}], "source": "# q3c\n# PROGRAMMING\n# QUESTION: Create a stripe for each term in the vocabulary. This stripe\u00a0should be the list of all other\n#           other words (not including itself) that co-occur within a 5 word window or 5-gram (two words\n#           on either side) of the term's position in the original text. In this exercise, use ['it', 'was', 'the','of']\n#           as stopwords, just ignore them from your 5 word vectors.\n\n# part c - USE THE TEXT ABOVE TO COMPLETE EACH STRIPE\n# Stopwords: \n#     ['it', 'was', 'the', 'of'] \n# Hint:\n#     In provided sentence, 'age' appears in two 5 word vectors: ['was', 'the', 'age', 'of', 'wisdom'] and ['was', 'the', 'age', 'of', 'foolishness']\n#     After removing stopwords, the remaining words are ['wisdom'] and ['foolishness']\n#\n#     You finish the rest of the non-stopwords below. \n\nstripes = {'age':['wisdom','foolishness'], # example\n           'best':['times'], # YOU FILL IN THE REST\n           'foolishness':['age'],\n           'times': ['best', 'worst'],\n           'wisdom':['age'],\n           'worst':['times']}\n\n\n###############################\nprint(stripes)"}, {"cell_type": "code", "execution_count": 27, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "38f4b1a4-3e91-4976-89d3-07b4942215bd", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part d - initializing an empty co-occurrence matrix (RUN THIS CELL AS IS)\nco_matrix = pd.DataFrame({term: [0]*len(vocab) for term in vocab}, index = vocab, dtype=int)"}, {"cell_type": "code", "execution_count": 28, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "bac747a0-a1e3-4795-aa2c-c010b10ab979", "showTitle": false, "title": ""}}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>best</th>\n      <th>foolishness</th>\n      <th>times</th>\n      <th>wisdom</th>\n      <th>worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>age</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>best</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>foolishness</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>times</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>wisdom</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>worst</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "             age  best  foolishness  times  wisdom  worst\nage            0     0            1      0       1      0\nbest           0     0            0      1       0      0\nfoolishness    1     0            0      0       0      0\ntimes          0     1            0      0       0      1\nwisdom         1     0            0      0       0      0\nworst          0     0            0      1       0      0"}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": "# part d - this cell 1-hot encodes the co-occurrence matrix (RUN THIS CELL AS IS) \nfor term, nbrs in stripes.items():\n    pass\n    for nbr in nbrs:\n        co_matrix.loc[term, nbr] = 1\nco_matrix"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\n"}], "source": "# q3d\n### MULTIPLE CHOICE\n### QUESTION: For our 6 word vocabulary how many entries are in this matrix? How many entries are zeroes?\n\n#   a.) 36 entries, of which 28 are zeroes\n#   b.) 36 entries, of which 30 are zeroes\n#   c.) 30 entries, of which 28 are zeroes\n#   d.) 30 entries, of which 18 are zeroes\n\n### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\nanswer = \"a\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 30, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "9b6cf0e3-9003-4d88-aa65-68d0a7b693e5", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "age-best: 0.0\nage-foolishness: 0.0\nage-times: 0.0\nage-wisdom: 0.0\nage-worst: 0.0\nbest-foolishness: 0.0\nbest-times: 0.0\nbest-wisdom: 0.0\nbest-worst: 1.0\nfoolishness-times: 0.0\nfoolishness-wisdom: 1.0\nfoolishness-worst: 0.0\ntimes-wisdom: 0.0\ntimes-worst: 0.0\nwisdom-worst: 0.0\n"}], "source": "# q3e\n### PROGRAMMING\n### QUESTION: For our 6 word vocabulary how many entries are in this matrix? How many entries are zeroes?\n\n# part e - FILL IN THE MISSING LINES to compute the cosine similarity between each pair of terms\nfor term1, term2 in itertools.combinations(vocab, 2):\n    # one hot-encoded vectors\n    v1 = co_matrix[term1]\n    v2 = co_matrix[term2]\n\n    # cosine similarity\n    ############# YOUR CODE HERE #################\n    csim = v1 @ v2 /(np.linalg.norm(v1) * np.linalg.norm(v2)) # Delete None and put in your own code   \n    ############# (END) YOUR CODE #################    \n\n    print(f\"{term1}-{term2}: {csim}\")"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\n"}], "source": "# q3f\n### MULTIPLE CHOICE\n### QUESTION: Which pairs of words have the highest 'similarity' scores?\n\n#   a.) foolishness-wisdom, best-worst\n#   b.) best-worst, best-times\n#   c.) foolishness-wisdom, age-times\n#   d.) best-worst, age-times\n\n### ENTER ONLY THE LETTER INSIDE THE ANSWER VARIABLE. (i.e. if your answer is f.), enter \"f\")\nanswer = \"a\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nNo, these words are not synonyms in the traditional sense. Best and worst are antonyms with opposite meanings,\nas are foolishness and wisdom. Their similarity score of 1.0 reflects that they appear in identical contexts in\nthis corpus, not that they have similar meanings. These antonym pairs commonly occur in similar sentence\nstructures and document types, particularly in literary devices like parallelism and contrast. They both\nco-occur with the same words because Dickens uses parallel structure in the opening line to contrast these\nopposing concepts. The results are based on our tiny two-sentence corpus. With a much larger\ncorpus, we would expect these patterns to change significantly. In a larger corpus, there are many more\nopportunities for true synonyms to appear in similar contexts repeatedly, while antonyms would diverge as they\nappear in contrasting rather than identical contexts. 'Best' would occur with positive contexts like excellent,\ngood, and superior, while worst would co-occur with negative contexts like terrible, poor, and inferior. A\nlarger corpus provides more diverse contexts that distinguish between words based on actual semantic similarity\nrather than structural accidents, making antonym similarity scores much lower and true synonym scores higher.\n\n"}], "source": "# q3g\n### SHORT RESPONSE (Extra Credit)\n### QUESTION: Are these words 'synonyms' in the traditional sense? In what sense are their meanings 'similar'?\n#             Explain how our results are contingent on the input text. What would change if we had a much larger corpus?\n\n### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n\nprint(\n\"\"\"\nNo, these words are not synonyms in the traditional sense. Best and worst are antonyms with opposite meanings,\nas are foolishness and wisdom. Their similarity score of 1.0 reflects that they appear in identical contexts in\nthis corpus, not that they have similar meanings. These antonym pairs commonly occur in similar sentence\nstructures and document types, particularly in literary devices like parallelism and contrast. They both\nco-occur with the same words because Dickens uses parallel structure in the opening line to contrast these\nopposing concepts. The results are based on our tiny two-sentence corpus. With a much larger\ncorpus, we would expect these patterns to change significantly. In a larger corpus, there are many more\nopportunities for true synonyms to appear in similar contexts repeatedly, while antonyms would diverge as they\nappear in contrasting rather than identical contexts. 'Best' would occur with positive contexts like excellent,\ngood, and superior, while worst would co-occur with negative contexts like terrible, poor, and inferior. A\nlarger corpus provides more diverse contexts that distinguish between words based on actual semantic similarity\nrather than structural accidents, making antonym similarity scores much lower and true synonym scores higher.\n\"\"\"\n)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "3e757c6f-10ef-487c-bd29-4f0bd0c93aa7", "showTitle": false, "title": ""}}, "source": "# Question 4: Pairs and Stripes at Scale\n\nAs you read in the paper by [Zadeh et al](http://stanford.edu/~rezab/papers/disco.pdf), the advantage of metrics like Cosine, Dice, Overlap and Jaccard is that they are dimension independent -- that is to say, if we implement them in a smart way the computational complexity of performing these computations is independent of the number of documents we want to compare (or in our case, the number of terms that are potential synonyms). One component of a 'smart implementation' involves thinking carefully both about how you define the \"basis vocabulary\" that forms your feature set (removing stopwords, etc). Another key idea is to use a data structure that facilitates distributed calculations. The DISCO (Dimension Independent Similarity Computation) implementation further uses a sampling strategy, but that is beyond the scope of this assignment. \n\nIn this question we'll take a closer look at the computational complexity of the synonym detection approach we took in question 3 and then revisit the document similarity example as a way to explore a more efficient approach to parallelizing this analysis.\n\n### Q4 Tasks:\n\n* __a) Multiple Answer (choose 2):__ In question 3 you calculated the cosine similarity of pairs of words using the vector representation of their co-occurrences in a corpus. In the async videos about \"Pairs and Stripes\" you were introduced to an alternative strategy. Choose two ways that using these data structures are more efficient than 1-hot encoded vectors when it comes to distributed similarity calculations [__`HINT:`__ _Consider memory constraints, amount of information being shuffled, amount of information being transfered over the network, and level of parallelization._]\n\n* __b) Read Provided Code:__ The code below provides a streamlined implementation of document similarity analysis in Spark. Read through this code carefully. Once you are confident you understand how it works, answer the remaining questions. [__`TIP:`__ _to see the output of each transformation try commenting out the subsequent lines and adding an early `collect()` action_.]\n\n* __c) Multiple Answer:__ The second mapper function, `splitWords`, emits 'postings'. The list of all 'postings' for a word is also refered to as an 'inverted index'. Choose __three__ true statements regarding 'postings' and an 'inverted index' based on your reading of the provided code. (*DITP by Lin and Dyer also contains a chapter on the Inverted Index although in the context of Hadoop rather than Spark. You may find the illustration in Chapter 4 helpful in answering this question*).\n\n* __d) Multiple Dropdowns:__ The third mapper, `makeCompositeKeys`, loops over the inverted index to emit 'pairs' of what? Explain what information is included in the composite key created at this stage and why it makes sense to synchronize around that information in the context of performing document similarity calculations. In addition to the information included in these new keys, what other piece of information will we need to compute Jaccard or Cosine similarity?\n\n* __e) Multiple Choice:__ Out of all the Spark transformations we make in this analysis, which are 'wide' transformations and which are 'narrow' transformations, and why?"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "02bbeeeb-2c83-40ad-b648-79b76f2603cb", "showTitle": false, "title": ""}}, "source": "A small test file that we put in a folder on your private data bucket : __`sample_docs.txt`__ "}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ab\n"}], "source": "# q4a\n### MULTIPLE ANSWERS - CHOOSE 2\n### QUESTION: In question 3 you calculated the cosine similarity of pairs of words using the vector representation of their co-occurrences\n#             in a corpus. In the async videos about \"Pairs and Stripes\" you were introduced to an alternative strategy. Choose\n#             two ways that using these data structures are more efficient than 1-hot encoded vectors when it comes to\n#             distributed similarity calculations [HINT:\u00a0Consider memory constraints, amount of information being shuffled,\n#             amount of information being transferred over the network, and level of parallelization.]\n\n#   a.) Stripes allow us to track the same information as full vectors/matrices with less memory overhead because we don't encode zeros.\n#   b.) Pairs allow us to compute counts needed for the DISCO metrics without doing an explicit vector computation.\n#   c.) Stripes are efficient because they can be stored as Python dicts, which are very efficient data structures.\n#   d.) Pairs are efficient because they can be stored as Python tuples, which are very efficient data structures.\n#   e.) Stripes are efficient because they are used directly to do vector multiplication with other stripes to compute the DISCO metrics.\n\n### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.) and y.), enter \"xy\")\nanswer = \"ab\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 34, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "c4aa7016-5fff-4b11-b615-4cff0007f244", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying from <STDIN>...\n/ [1 files][    0.0 B/    0.0 B]                                                \nOperation completed over 1 objects.                                              \n"}], "source": "sample_doc=\"\"\"docA\tbright blue butterfly forget\ndocB\tbest forget bright sky\ndocC\tblue sky bright sun\ndocD\tunder butterfly sky hangs\ndocE\tforget blue butterfly\"\"\"\n\nsample_docs_loc = f'{HW3_FOLDER}/sample_docs.txt'\n\n!echo \"{sample_doc}\" | gsutil cp - {sample_docs_loc}"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "   1015176  2025-10-04T21:54:27Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/261-Homework-3-Synonym-Detection-Using-Apache Spark.html\n    185904  2025-10-05T20:49:58Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/HW3.ipynb\n    151301  2025-09-14T23:17:19Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/HW3_summer.ipynb\n    178730  2025-10-04T21:57:53Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/HW_3_Fall_2018_2.ipynb\n    161879  2025-10-04T21:57:52Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/hw3_Fall_2018.ipynb\n    110416  2025-10-04T21:52:55Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/hw3_Workbook.ipynb\n       205  2025-10-04T22:12:32Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/readme.md\n       145  2025-10-06T04:47:51Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/sample_docs.txt\n    500954  2025-08-23T18:33:53Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/scaling-up-similarity-search.pdf\n       494  2025-10-05T18:17:51Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/systems_test.txt\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/.ipynb_checkpoints/\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/deprecated/\n                                 gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/docker/\nTOTAL: 10 objects, 2305204 bytes (2.2 MiB)\n"}], "source": "!gsutil ls -l {HW3_FOLDER}"}, {"cell_type": "code", "execution_count": 36, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "c738aa14-92f0-4727-bd60-0c0f5326ad37", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "docA\tbright blue butterfly forget\ndocB\tbest forget bright sky\ndocC\tblue sky bright sun\ndocD\tunder butterfly sky hangs\ndocE\tforget blue butterfly\n"}], "source": "# load data - RUN THIS CELL AS IS\n!gsutil cat {sample_docs_loc}"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "36e04f30-d4ee-484f-a6f4-5568a1698a49", "showTitle": false, "title": ""}}, "source": "__Document Similarity Analysis in Spark:__"}, {"cell_type": "code", "execution_count": 37, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "da504044-0c75-4763-adad-2d8e2817265b", "showTitle": false, "title": ""}}, "outputs": [], "source": "# load data - RUN THIS CELL AS IS\ndata = sc.textFile(sample_docs_loc)  "}, {"cell_type": "code", "execution_count": 38, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "13bca2b5-8996-4018-b59b-3db3599f14fd", "showTitle": false, "title": ""}}, "outputs": [], "source": "# helper function - RUN THIS CELL AS IS\ndef splitWords(pair):\n    \"\"\"Mapper 2: tokenize each document and emit postings.\"\"\"\n    doc, text = pair\n    words = text.split(\" \")\n    for w in words:\n        yield (w, [(doc,len(words))])"}, {"cell_type": "code", "execution_count": 39, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "68200122-deb9-434b-84b1-a1a130566b68", "showTitle": false, "title": ""}}, "outputs": [], "source": "# helper function - RUN THIS CELL AS IS\ndef makeCompositeKey(inverted_index):\n    \"\"\"Mapper 3: loop over postings and yield pairs.\"\"\"\n    word, postings = inverted_index\n    # taking advantage of symmetry, output only (a,b), but not (b,a)\n    for subset in itertools.combinations(sorted(postings), 2):\n        yield (str(subset), 1)"}, {"cell_type": "code", "execution_count": 40, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "34b72a6c-cab1-46ea-b50b-00ce7fa49cc4", "showTitle": false, "title": ""}}, "outputs": [], "source": "# helper function - RUN THIS CELL AS IS\ndef jaccard(line):\n    \"\"\"Mapper 4: compute similarity scores\"\"\"\n    (doc1, n1), (doc2, n2) = ast.literal_eval(line[0])\n    total = int(line[1])\n    jaccard = total / float(int(n1) + int(n2) - total)\n    yield doc1+\" - \"+doc2, jaccard"}, {"cell_type": "markdown", "metadata": {}, "source": "### ignore latency warnings\nNOTE: do NOT be alarmed if you see some latency warnings when you run the following command. This can happen when you are accessing your cloud bucket for the first time. Spark is super EAGER! "}, {"cell_type": "code", "execution_count": 41, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "1a706f1c-bbb2-4f45-af1c-9ebee2fbea0c", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[('docA - docE', 0.75),\n ('docA - docB', 0.3333333333333333),\n ('docA - docC', 0.3333333333333333),\n ('docB - docC', 0.3333333333333333),\n ('docD - docE', 0.16666666666666666),\n ('docC - docE', 0.16666666666666666),\n ('docB - docE', 0.16666666666666666),\n ('docC - docD', 0.14285714285714285),\n ('docA - docD', 0.14285714285714285),\n ('docB - docD', 0.14285714285714285)]"}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\n# Spark Job - RUN THIS CELL AS IS\nresult = data.map(lambda line: line.split('\\t')) \\\n             .flatMap(splitWords) \\\n             .reduceByKey(lambda x,y : x+y) \\\n             .flatMap(makeCompositeKey) \\\n             .reduceByKey(lambda x,y : x+y) \\\n             .flatMap(jaccard) \\\n             .takeOrdered(10, key=lambda x: -x[1])\nresult \n#NOTE: do NOT be alarmed if you see some latency warnings here. You ar"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "abd\n"}], "source": "# q4c\n### MULTIPLE ANSWERS - CHOOSE 3\n### QUESTION: The second mapper function,\u00a0splitWords, emits 'postings'. The list of all 'postings' for a word is also\n#             referred to as an 'inverted index'. Which of the following three statements are true regarding 'postings'\n#             and an 'inverted index' based on your reading of the provided code. (Hint: DITP by Lin and Dyer also\n#             contains a chapter on the Inverted Index although in the context of Hadoop rather than Spark. You may\n#             find the illustration in Chapter 4 helpful in answering this question).\n\n#   a.) In general, postings can have a payload but must have a document ID.\n#   b.) Each term is associated with a maximum of one posting list.\n#   c.) Each term can be associated with multiple postings lists.\n#   d.) In our case, the payload for a posting is the document length.\n#   e.) In our case, the payload for a posting is the vocabulary size.\n\n### ENTER ONLY THE LETTERS INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.) and y.), enter \"xy\")\nanswer = \"abd\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": "# q4d (ANSWER IS NOT IN THIS CELL, BUT IN THE FOLLOWING CELLS BELOW.)\n### MULTIPLE CHOICE - FOR EACH '[*]' below.\n### QUESTION: The third mapper, makeCompositeKeys, loops over the inverted index to emit 'pairs' of what? Explain what\n#             information is included in the composite key created at this stage and why it makes sense to synchronize\n#             around that information in the context of performing document similarity calculations. In addition to the\n#             information included in these new keys, what other piece of information will we need to compute Jaccard\n#             or Cosine similarity?\n\n# INSTRUCTIONS: USE THE FOLLOWING CELLS TO ANSWER TO FILL IN THE FOLLOWING PLACEHOLDERS BELOW:\n#      1.) [yield] document pairs\n#      2.) [first_element]  two document IDs and their respective document lengths\n#      3.) [second_element] the number of shared words\n#      4.) [similarity] the intersection count\n\n#   makeCompositeKeys() yields [document pairs], where the first element is a composite key of [ two document IDs and their respective document lengths],\n#   and the second element is the number of shared words. As such, each record represents a unique word that has appeared in both\n#   documents. This allows us to calculate [intersection count], which we need for the Jaccard and Cosine similarity calculations."}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "e\n"}], "source": "# q4d1\n### MULTIPLE CHOICE - [yield]\n\n## RE-ENTERING QUESTION FOR CONVENIENCE:\n#   makeCompositeKeys() yields [yield], where the first element is a composite key of [first_element],\n#   and the second element is [second_element]. As such, each record represents a unique word that has appeared in both\n#   documents. This allows us to calculate [similarity], which we need for the Jaccard and Cosine similarity calculations.\n\n\n#   a.) set of 3 values\n#   b.) list of 2 values\n#   c.) tuple of 3 values\n#   d.) tuple of 1 combined value\n#   e.) tuple of 2 values\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"e\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "e\n"}], "source": "# q4d2\n### MULTIPLE CHOICE - [first_element]\n\n## RE-ENTERING QUESTION FOR CONVENIENCE:\n#   makeCompositeKeys() yields [yield], where the first element is a composite key of [first_element],\n#   and the second element is [second_element]. As such, each record represents a unique word that has appeared in both\n#   documents. This allows us to calculate [similarity], which we need for the Jaccard and Cosine similarity calculations.\n\n\n#   a.) count of one and the document ID\n#   b.) a pair of document ID's for which a word does not belong, along with document lengths\n#   c.) a posting ID and the length of the documents\n#   e.) a pair of documents that both appear in the inverted index of a word and the document lengths\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"e\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "b\n"}], "source": "# q4d3\n### MULTIPLE CHOICE - [second_element]\n\n## RE-ENTERING QUESTION FOR CONVENIENCE:\n#   makeCompositeKeys() yields [yield], where the first element is a composite key of [first_element],\n#   and the second element is [second_element]. As such, each record represents a unique word that has appeared in both\n#   documents. This allows us to calculate [similarity], which we need for the Jaccard and Cosine similarity calculations.\n\n\n#   a.) another composite key\n#   b.) the value (a count of one)\n#   c.) the number of times the word occurs in the corpus\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"b\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "c\n"}], "source": "# q4d4\n### MULTIPLE CHOICE - [similarity]\n\n## RE-ENTERING QUESTION FOR CONVENIENCE:\n#   makeCompositeKeys() yields [yield], where the first element is a composite key of [first_element],\n#   and the second element is [second_element]. As such, each record represents a unique word that has appeared in both\n#   documents. This allows us to calculate [similarity], which we need for the Jaccard and Cosine similarity calculations.\n\n\n#   a.) the number of times the word occurs in either document (union)\n#   b.) the set difference between one document and another (set difference)\n#   c.) the total count of terms occurring in both documents (the intersection)\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"c\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "d\n"}], "source": "# q4e\n### MULTIPLE CHOICE\n### QUESTION: Out of all the Spark transformations we make in this analysis, which are 'wide' transformations and which\n#             are 'narrow' transformations, and why?\n\n#   a.) All of the transformations are narrow because they all operate on a single node of the Spark cluster.\n\n#   b.) The map() transformation is narrow because it maintains a 1 to 1 mapping between function inputs and outputs, all the other\n#       transformations are wide because they do not, and therefore they can potentially increase the size of the output data.\n\n#   c.) The map() and three flatMap()s are wide because they can operate on any part of the data regardless of partitioning.\n#       The reduceByKey()s are narrow transformations, because they \"narrow\" the data by aggregating it.\n\n#   d.) The map() and three flatMap()s are narrow because they can occur in place and don't require any particular partitioning\n#       of the data. The reduceByKey()s are wide transformations, they require a shuffle and we can't guarantee in advance\n#       which partitions contain the information that needs to be aggregated.\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"d\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "6a9f78ca-93fe-46ba-8d26-fa7a8adc6710", "showTitle": false, "title": ""}}, "source": "# About the Data and EDA\nNow that you are comfortable with similarity metrics we turn to the main task in this assignment: \"Synonym\" Detection. As you saw in Question 3 the ability of our algorithm to detect words with similar meanings is highly dependent on our input text. Specifically, we need a large enough corpus of natural language that we can expose our algorithm to a realistic range of contexts in which any given word might get used. Ideally, these 'contexts' would also provide enough signal to distinguish between words with similar semantic roles but different meaning. Finding such a corpus will be easier to accomplish for some words than others.\n\nFor the main task in this portion of the homework you will use data from Google's n-gram corpus. This data is particularly convenient for our task because Google has already done the first step for us: they windowed over a large subset of the web and extracted all 5-grams. If you are interested in learning more about this dataset the original source is: http://books.google.com/ngrams/, and a large subset is available [here from AWS](https://aws.amazon.com/datasets/google-books-ngrams/). \n\nFor this assignment we have provided a subset of the 5-grams data consisting of 191 files of approximately 10MB each. These files are available in your personal data bucket on dataproc. Please only use the provided data so that we can ensure consistent results from student to student.\n\nEach row in our dataset represents one of these 5 grams in the format:\n> `(ngram) \\t (count) \\t (pages_count) \\t (books_count)`\n\n__DISCLAIMER__: In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data.  Calculating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some similar terms."}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "  3.66 MiB  2025-08-23T18:33:42Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/googlebooks-eng-all-5gram-20090715-0-filtered.txt.gz\n      70 B  2025-08-23T18:33:42Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/googlebooks-eng-all-5gram-20090715-1-filtered.txt.gz\n  3.66 MiB  2025-08-23T18:33:42Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/googlebooks-eng-all-5gram-20090715-10-filtered.txt.gz\n  3.67 MiB  2025-08-23T18:33:42Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/googlebooks-eng-all-5gram-20090715-100-filtered.txt.gz\n  3.67 MiB  2025-08-23T18:33:42Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/data/googlebooks-eng-all-5gram-20090715-101-filtered.txt.gz\nException ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\nBrokenPipeError: [Errno 32] Broken pipe\n"}], "source": "!gsutil ls -lh {HW3_FOLDER}/data |head -5 #The data is preloaded into your personal data bucket (all 190 zipped files)"}, {"cell_type": "code", "execution_count": 50, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "67ee798a-99fd-4ada-b8b1-4bfa6d032335", "showTitle": false, "title": ""}}, "outputs": [], "source": "# set global paths to full data folder and to the first file (which we'll use for testing)\nNGRAMS  = f'{HW3_FOLDER}/data'\nF1_PATH = f'{HW3_FOLDER}/data/googlebooks-eng-all-5gram-20090715-0-filtered.txt.gz'"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "d36301d1-c5fd-46c7-91a5-4745bb175741", "showTitle": false, "title": ""}}, "source": "As you develop your code you should use the following file to systems test each of your solutions before running it on the Google data. (Note: these are the 5-grams extracted from our two line Dickens corpus in Question 3... you should find that your Spark job results match the calculations we did \"by hand\").\n\nTest file: __`systems_test.txt`__"}, {"cell_type": "code", "execution_count": 51, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "ed5ebc27-0381-4fe6-901b-fe4fc4f5a10d", "showTitle": false, "title": ""}, "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying from <STDIN>...\n/ [1 files][    0.0 B/    0.0 B]                                                \nOperation completed over 1 objects.                                              \n"}], "source": "systems_test= \"\"\"it was the best of\t1\t1\t1\nage of wisdom it was\t1\t1\t1\nbest of times it was\t1\t1\t1\nit was the age of\t2\t1\t1\nit was the worst of\t1\t1\t1\nof times it was the\t2\t1\t1\nof wisdom it was the\t1\t1\t1\nthe age of wisdom it\t1\t1\t1\nthe best of times it\t1\t1\t1\nthe worst of times it\t1\t1\t1\ntimes it was the age\t1\t1\t1\ntimes it was the worst\t1\t1\t1\nwas the age of wisdom\t1\t1\t1\nwas the best of times\t1\t1\t1\nwas the age of foolishness\t1\t1\t1\nwas the worst of times\t1\t1\t1\nwisdom it was the age\t1\t1\t1\nworst of times it was\t1\t1\t1\"\"\"\n\n\nsystems_test_loc = f'{HW3_FOLDER}/systems_test.txt'\n\n!echo \"{systems_test}\" | gsutil cp - {systems_test_loc}"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "ac26da79-b410-4a8c-9d5d-6c074d48769d", "showTitle": false, "title": ""}}, "source": "Finally, we'll create a Spark RDD for each of these files so that they're easy to access throughout the rest of the assignment."}, {"cell_type": "code", "execution_count": 52, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "4e162747-4cf6-4f36-89ca-a1214512f5c2", "showTitle": false, "title": ""}}, "outputs": [], "source": "# Autograder Skip\n\n# Spark RDDs for each dataset\ntestRDD = sc.textFile(systems_test_loc) \nf1RDD = sc.textFile(F1_PATH)\ndataRDD = sc.textFile(NGRAMS)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "70ae8e81-27de-4975-8b4c-22bbb82c6be5", "showTitle": false, "title": ""}}, "source": "Let's take a peek at what each of these RDDs looks like:"}, {"cell_type": "code", "execution_count": 53, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "6382f9a6-0b3d-4cb5-be31-f4ac27814306", "showTitle": false, "title": ""}}, "outputs": [{"data": {"text/plain": "['it was the best of\\t1\\t1\\t1',\n 'age of wisdom it was\\t1\\t1\\t1',\n 'best of times it was\\t1\\t1\\t1',\n 'it was the age of\\t2\\t1\\t1',\n 'it was the worst of\\t1\\t1\\t1',\n 'of times it was the\\t2\\t1\\t1',\n 'of wisdom it was the\\t1\\t1\\t1',\n 'the age of wisdom it\\t1\\t1\\t1',\n 'the best of times it\\t1\\t1\\t1',\n 'the worst of times it\\t1\\t1\\t1']"}, "execution_count": 53, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\ntestRDD.take(10)"}, {"cell_type": "code", "execution_count": 54, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "7a94a009-6a28-4c71-9cfd-57fc3f0469e4", "showTitle": false, "title": ""}}, "outputs": [{"data": {"text/plain": "['A BILL FOR ESTABLISHING RELIGIOUS\\t59\\t59\\t54',\n 'A Biography of General George\\t92\\t90\\t74',\n 'A Case Study in Government\\t102\\t102\\t78',\n 'A Case Study of Female\\t447\\t447\\t327',\n 'A Case Study of Limited\\t55\\t55\\t43',\n \"A Child's Christmas in Wales\\t1099\\t1061\\t866\",\n 'A Circumstantial Narrative of the\\t62\\t62\\t50',\n 'A City by the Sea\\t62\\t60\\t49',\n 'A Collection of Fairy Tales\\t123\\t117\\t80',\n 'A Collection of Forms of\\t116\\t103\\t82']"}, "execution_count": 54, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\nf1RDD.take(10)"}, {"cell_type": "code", "execution_count": 55, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "3821fa08-6fbd-46dd-9233-92ae8dad4e7b", "showTitle": false, "title": ""}}, "outputs": [{"data": {"text/plain": "['A BILL FOR ESTABLISHING RELIGIOUS\\t59\\t59\\t54',\n 'A Biography of General George\\t92\\t90\\t74',\n 'A Case Study in Government\\t102\\t102\\t78',\n 'A Case Study of Female\\t447\\t447\\t327',\n 'A Case Study of Limited\\t55\\t55\\t43',\n \"A Child's Christmas in Wales\\t1099\\t1061\\t866\",\n 'A Circumstantial Narrative of the\\t62\\t62\\t50',\n 'A City by the Sea\\t62\\t60\\t49',\n 'A Collection of Fairy Tales\\t123\\t117\\t80',\n 'A Collection of Forms of\\t116\\t103\\t82']"}, "execution_count": 55, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\ndataRDD.take(10)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "2bec439f-08e0-45b9-a249-6e57064f627d", "showTitle": false, "title": ""}}, "source": "## Question 5: N-gram EDA part 1 (words)\n\nBefore starting our synonym-detection, let's get a sense for this data. As you saw in questions 3 and 4 the size of the vocabulary will impact the amount of computation we have to do. Write a Spark job that will accomplish the three tasks below as efficiently as possible. (No credit will be awarded for jobs that sort or subset after calling `collect()`-- use the framework to get the minimum information requested). As you develop your code, systems test each job on the provided file with Dickens ngrams, then on a single file from the Ngram dataset before running the full analysis.\n\n\n### Q5 Tasks:\n* __a) Code in Notebook:__ Write a Spark application to retrieve:\n  * The number of unique words that appear in the data. (i.e. size of the vocabulary) \n  * A list of the top 10 words & their counts.\n  * A list of the bottom 10 words & their counts.  \n  \n  __`NOTE  1:`__ _don't forget to lower case the ngrams before extracting words._  \n  __`NOTE  2:`__ _don't forget to take in to account the number of occurrences (count) of each ngram._  \n  __`NOTE  3:`__ _to make this code more reusable, the `EDA1` function code base uses a parameter 'n' to specify the number of top/bottom words to print (in this case we've requested 10)._\n\n* __b) Numeric:__ What is the size of the vocabulary in `dataRDD`?\n\n* __c) Numeric:__ Given the vocab size you just found, how many potential synonym pairs could we form from this corpus? \n\n* __d) Numeric:__ If each term's stripe were 1000 words long, how many tuples would we need to shuffle in order to form the inverted indices? \n\n* __e) Short Response:__ Show and briefly explain your calculations for each part of this question. [__`HINT:`__ see your work from q4 for a review of these concepts.]\n\n* __f) Multiple Choice:__ Looking at the most frequent words and their counts, how useful will these top words be in synonym detection?\n\n* __g) Multiple Choice:__ Looking at the least frequent words and their counts, what is a possible explanation of their low counts?\n\n* __h) Multiple Choice:__ How reliable should we expect the detected 'synonyms' for the least frequent words to be?"}, {"cell_type": "code", "execution_count": 56, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "091af7db-444d-4aa7-bbc3-691f728d2840", "showTitle": false, "title": ""}}, "outputs": [], "source": "# q5a\n# part a - write your spark job here \ndef EDA1(rdd, n):\n    total, top_n, bottom_n = None, None, None\n    # Each row in our dataset represents one of these 5 grams in the format:\n    # > `(ngram) \\t (count) \\t (pages_count) \\t (books_count)`\n    #            'it was the age of\\t2\\t1\\t1'\n    # Each rocord has:  ngram, count, ignore other fields\n    ############# YOUR CODE HERE ###############\n    words_in_rdd = rdd.map(lambda line: line.split('\\t'))\\\n                  .flatMap(lambda parts: [(word.lower(), int(parts[1])) for word in parts[0].split(' ')])\\\n                  .reduceByKey(lambda x, y: x+y)\\\n                  .cache()\n    total = words_in_rdd.count()\n    top_n = words_in_rdd.takeOrdered(n, key = lambda x: -x[1])\n    bottom_n = words_in_rdd.takeOrdered(n, key = lambda x: x[1])\n    ############# (END) YOUR CODE ##############\n    return total, top_n, bottom_n"}, {"cell_type": "code", "execution_count": 57, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "4babf25b-06f1-4d63-86d8-a2e853f13e2c", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Wall time: 0.4689514636993408 seconds\n"}], "source": "# part a - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\nimport time\nstart = time.time()\nvocab_size, most_frequent, least_frequent = EDA1(testRDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n"}, {"cell_type": "code", "execution_count": 58, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "615f0416-8170-4d20-9eb1-42c5abfbeb64", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Vocabulary Size: 10\n ---- Top Words ----|--- Bottom Words ----\n     was         17 |    foolishness   1\n      of         17 |           best   4\n     the         17 |          worst   5\n      it         16 |         wisdom   5\n   times         10 |            age   8\n     age          8 |          times  10\n   worst          5 |             it  16\n  wisdom          5 |            was  17\n    best          4 |             of  17\nfoolishness          1 |            the  17\n"}], "source": "# part a - display results (feel free to modify the formatting code if needed)\nprint(\"Vocabulary Size:\", vocab_size)\nprint(\" ---- Top Words ----|--- Bottom Words ----\")\nfor (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "cf050ce9-3cc6-484c-8036-7d380d4ca0dc", "showTitle": false, "title": ""}}, "source": "Expected output for testRDD:\n<pre>\n    Vocabulary Size: 10\n ---- Top Words ----|--- Bottom Words ----\n     was         17 |    foolishness   1\n      of         17 |           best   4\n     the         17 |          worst   5\n      it         16 |         wisdom   5\n   times         10 |            age   8\n     age          8 |          times  10\n   worst          5 |             it  16\n  wisdom          5 |            was  17\n    best          4 |             of  17\nfoolishness       1 |            the  17  \n</pre>"}, {"cell_type": "code", "execution_count": 59, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "61d1bbae-7d1d-483e-b23a-a32fef6e2264", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 3.1779017448425293 seconds\n"}], "source": "# Autograder Skip\n\n# part a - run a single file, ie., a small sample (RUN THIS CELL AS IS)\nstart = time.time()\nvocab_size, most_frequent, least_frequent = EDA1(f1RDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n"}, {"cell_type": "code", "execution_count": 60, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "79a6aa9a-4c42-483e-b859-9711a8bbff9f", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Vocabulary Size: 36353\n ---- Top Words ----|--- Bottom Words ----\n     the   27691943 |    stakeholder  40\n      of   18590950 |          kenny  40\n      to   11601757 |       jonathan  40\n      in    7470912 |         barnes  40\n       a    6926743 |         arnall  40\n     and    6150529 |       copybook  40\n    that    4077421 |     buonaparte  40\n      is    4074864 |       puzzling  40\n      be    3720812 |             ae  40\n     was    2492074 |       kennelly  40\n"}], "source": "# Autograder Skip\n\n# part a - display results (feel free to modify the formatting code if needed)\nprint(\"Vocabulary Size:\", vocab_size)\nprint(\" ---- Top Words ----|--- Bottom Words ----\")\nfor (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "7e6bd030-552b-4cbb-aafb-955cc5f2575c", "showTitle": false, "title": ""}}, "source": "Expected output for f1RDD\n<pre>\nVocabulary Size: 36353\n ---- Top Words ----|--- Bottom Words ----\n     the   27691943 |    stakeholder  40\n      of   18590950 |          kenny  40\n      to   11601757 |         barnes  40\n      in    7470912 |         arnall  40\n       a    6926743 |     buonaparte  40\n     and    6150529 |       puzzling  40\n    that    4077421 |             hd  40\n      is    4074864 |        corisca  40\n      be    3720812 |       cristina  40\n     was    2492074 |         durban  40\n</pre>"}, {"cell_type": "code", "execution_count": 61, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "f82284a5-a9a1-49b4-b8fa-057d6ed6f368", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 23:============================================>         (156 + 4) / 190]\r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 264.30519247055054 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part a - run full analysis (RUN THIS CELL AS IS)\n\nstart = time.time()\nvocab_size, most_frequent, least_frequent = EDA1(dataRDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Command took 4 minute minutes on N1-Std-4 (4 CPUS) as May 30, 2022\n# "}, {"cell_type": "code", "execution_count": 62, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "47c705e3-8578-48ce-82ca-9cf3274ab801", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Vocabulary Size: 269339\n ---- Top Words ----|--- Bottom Words ----\n     the 5490815394 |    unmurmuring  40\n      of 3698583299 |    scribbler's  40\n      to 2227866570 |      washermen  40\n      in 1421312776 |    viscerating  40\n       a 1361123022 |         mildes  40\n     and 1149577477 |       jaworski  40\n    that  802921147 |            rll  40\n      is  758328796 |          porti  40\n      be  688707130 |     foretastes  40\n      as  492170314 |       parcival  40\n"}], "source": "# Autograder Skip\n\n# part a - display results (feel free to modify the formatting code if needed)\nprint(\"Vocabulary Size:\", vocab_size)\nprint(\" ---- Top Words ----|--- Bottom Words ----\")\nfor (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "b9cd59be-634c-4cad-a6a8-209e0c777905", "showTitle": false, "title": ""}}, "source": "Expected output for dataRDD:\n(bottom words might vary a little due to ties)\n<pre>\nVocabulary Size: 269339\n ---- Top Words ----|--- Bottom Words ----\n     the 5490815394 |   schwetzingen  40\n      of 3698583299 |           cras  40\n      to 2227866570 |       parcival  40\n      in 1421312776 |          porti  40\n       a 1361123022 |    scribbler's  40\n     and 1149577477 |      washermen  40\n    that  802921147 |    viscerating  40\n      is  758328796 |         mildes  40\n      be  688707130 |      scholared  40\n      as  492170314 |       jaworski  40\n</pre>"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "269339\n"}], "source": "# q5b\n### NUMERICAL INPUT\n### QUESTION: What is the size of the vocabulary in dataRDD?\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. PLEASE DO NOT INCLUDE COMMAS TO SEPARATE THOUSANDS.\n### FOR EXAMPLE, IF YOUR ANSWER IS 100,000, answer = \"100000\". THE AUTOGRADER FRAMEWORK IS PETTY LIKE THAT.\n\nanswer = \"269339\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "36271613791\n"}], "source": "# q5c\n### NUMERICAL INPUT\n### QUESTION: Given the vocabulary size you just found, how many potential synonym pairs could we form from this corpus?\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. PLEASE DO NOT INCLUDE COMMAS TO SEPARATE THOUSANDS.\n### FOR EXAMPLE, IF YOUR ANSWER IS 100,000, answer = \"100000\". THE AUTOGRADER FRAMEWORK IS PETTY LIKE THAT.\n\nanswer = \"36271613791\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "269339000\n"}], "source": "# q5d\n### NUMERICAL INPUT\n### QUESTION: If each term's stripe were 1000 words long, how many tuples would we need to shuffle in order to form the inverted indices?\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING. PLEASE DO NOT INCLUDE COMMAS TO SEPARATE THOUSANDS.\n### FOR EXAMPLE, IF YOUR ANSWER IS 100,000, answer = \"100000\". THE AUTOGRADER FRAMEWORK IS PETTY LIKE THAT.\n\nanswer = \"269339000\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nQ5a: it is the code itself\nQ5b: Vocabulary size = 269,339 unique words (written right there from the count() operation)\nQ5c: I used the combination formula as we are selecting 2 words from our n words, ordering not mattering.\nNumber of synonym pairs = C(n,2) = n(n-1)/2, where n = 269,339 = 269,339 \u00d7 269,338 / 2 = 36,272,726,391 pairs\nQ5d: Each of the 269,339 words has 1,000 neighbor pairs when building the inverted index, so we shuffle:\nTuples to shuffle = vocabulary size \u00d7 stripe length = 269,339 \u00d7 1,000 = 269,339,000 tuples\n\n"}], "source": "# q5e\n### SHORT RESPONSE\n### QUESTION: Show and briefly explain your calculations for each part of this question.\n#             If you'd like, feel free to create a Markdown Cell below for LaTex and enter 'See answer below' in the print statement.\n#             Otherwise, an explanation inside the print statement is perfectly fine.\n\n### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n\nprint(\n\"\"\"\nQ5a: it is the code itself\nQ5b: Vocabulary size = 269,339 unique words (written right there from the count() operation)\nQ5c: I used the combination formula as we are selecting 2 words from our n words, ordering not mattering.\nNumber of synonym pairs = C(n,2) = n(n-1)/2, where n = 269,339 = 269,339 \u00d7 269,338 / 2 = 36,272,726,391 pairs\nQ5d: Each of the 269,339 words has 1,000 neighbor pairs when building the inverted index, so we shuffle:\nTuples to shuffle = vocabulary size \u00d7 stripe length = 269,339 \u00d7 1,000 = 269,339,000 tuples\n\"\"\"\n)"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "d\n"}], "source": "# q5f\n### MULTIPLE CHOICE\n### QUESTION: Looking at the most frequent words and their counts, how useful will these top words be in synonym detection?\n\n#   a.) The most frequent words are 'key words' -- they are so common because they help explain the key meaning of the document.\n\n#   b.) The most frequent words are 'stop words' -- they are common because they occur frequently together and\n#       that makes them strong candidates for being synonyms.\n\n#   c.) The most frequent words are 'key words' -- they are common because they occur frequently together and\n#       that makes them strong candidates for being synonyms.\n\n#   d.) The most frequent words are 'stop words' -- they are so common that they probably don't tell us\n#       very much about the meaning of their neighbors.\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"d\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\n"}], "source": "# q5g\n### MULTIPLE CHOICE\n### QUESTION: Looking at the least frequent words and their counts, what is a possible explanation of their low counts?\n\n#   a.) The least frequent words all have the same count -- this is probably an indication that they each only occurred\n#       in a single 5-gram and that 5-gram itself was quite uncommon in the corpus.\n\n#   b.) The least frequent words all have the same count -- this is probably an indication that they each occur\n#       in multiple 5-grams and there are a number of 5-grams that contain those words exactly equal to the count.\n\n#   c.) The least frequent words all have the same count -- this is probably an indication that they are synonyms.\n\n#   d.) The least frequent words all have the same count -- this is probably an indication that they are NOT synonyms.\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"a\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\n"}], "source": "# q5h\n### MULTIPLE CHOICE\n### QUESTION: How reliable should we expect the detected 'synonyms' for the least frequent words to be?\n\n#   a.) I would not expect the detected synonyms to be very robust for these words given that we have very little data on them.\n\n#   b.) I would expect the detected synonyms to be very robust for these words given that we have very precise data on them.\n\n#   c.) The amount of data available is irrelevant with regards to detecting synonyms in this exercise\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"a\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "81a856ad-4475-4912-9de7-fa14d6a73044", "showTitle": false, "title": ""}}, "source": "# Question 6: N-gram EDA part 2 (co-occurrences)\n\nThe computational complexity of synonym analysis depends not only on the number of words, but also on the number of co-ocurrences each word has. In this question you'll take a closer look at that aspect of our data. As before, please test each job on small \"systems test\" (Dickens ngrams) file and on a single file from the Ngram dataset before running the full analysis.\n\n### Q6 Tasks:\n* __a) Code in Notebook:__ Write a spark job that computes:\n  * the number of unique neighbors (i.e. 5-gram co-occuring words) for each word in the vocabulary. \n  * For example, given a five-gram `a b c a b` our desired output of the number of unique neighbors for each unique word as follows: is a:2, b:2, c:2 (from pairs {'a-b', 'b-c', 'a-c'})\n  * More generally, for a single ngram the number of unique co-occuring words is `N-1`, where is `N` is the number of unique words in the n-gram. \n  * The above  five-gram `a b c a b`  has 3 unique words where each word has 2 unique neighors (different to itself) \n  * For \"a b c d e\", each unique word has a 4 neighbors\n\nThe following code demonstrates this neighbor count in action on a single core machine:\n\n```python\nimport itertools\nimport numpy as np\n\ndef get_num_of_uniq_neighbors(a_str = \"a b c a b\")\n    words = a_str.split()\n    # we are looking for neighbors of each word (regardless of location in the n-gram context)\n    # pair (a b) is same as pair (b a)\n    pairs_list = [] # all pair combinations\n    pairs = set()   # just unique pair combinations\n    for i, pair in enumerate(itertools.combinations(words, 2)):\n        if pair[0] != pair[1]:\n            pair_str = \"{}-{}\".format(*sorted(pair)) #sort to make pair (a b) the same as pair (b a)\n            #print(f\"pair {i+1}: {pair_str}\")\n            pairs.add(pair_str)\n            pairs_list+=[pair_str]\n    print(f\"pairs_list: {pairs_list}\")\n    print(f\"uniq pairs: {pairs}\")\n    unigrams = [unigram for bigram in pairs for unigram in bigram.split(\"-\")]\n    print(f\"unigrams: {unigrams}\")\n    for unigram in set(unigrams):\n        print(f\"{unigram}: {sum(np.char.count(unigrams, unigram))}\")\n\nget_num_of_uniq_neighbors(a_str = \"a b c a b\")   \nget_num_of_uniq_neighbors(a_str = \"a b c d e\")   \n\n#pairs_list: ['a-b', 'a-c', 'a-b', 'b-c', 'a-b', 'a-c', 'b-c', 'a-b'] #dropped \"a-a\" and \"b-b\"\n#uniq pairs: {'a-b', 'b-c', 'a-c'}\n#unigrams: ['a', 'b', 'b', 'c', 'a', 'c']\n# a: 2\n# b: 2\n# c: 2\n.....\nd:4\ne:4  \n```\n\n<pre>\n  HINT 2: consider all words within a five-gram to be co-occuring. In other words, a word in a single 5-gram will always have 4 neighbors\n  EXAMPLE:\n    the dog ate cat litter \n    the cat has clean litter \n    \n    Vocabulary:\n    the, dog, ate, litter, cat, has, clean\n    \n    Neighbors:\n    (the, dog) (the, ate) (the, cat) (the, littler), (dog, ate) (dog, cat) (dog, litter), (ate, cat) (ate, litter), (cat, litter)\n    (the, cat) (the, has) (the, clean) (the, litter), (cat, has) (cat, clean) (cat, litter), (has, clean) (has, litter) (clean, litter)\n    \n    Unique neighbors:\n    the 6\n    dog 4\n    ate 4\n    litter 6\n    cat 6\n    has 4\n    clean 4\n </pre>\n    \n    \n  * the top 10 words with the most \"neighbors\"\n  * the bottom 10 words with least \"neighbors\"\n  * a random sample of 1% of the words' neighbor counts    \n  \n[__`NOTE:`__ for the last item, please return only the counts and not the words -- we'll go on to use these in a plotting function that expects a list of integers. Additionally, we'll say for the purposes of this assignment that a word cannot be a neighbor with itself. For example, (cat, cat) wouldn't be valid.]\n\n* __b) Multiple Choice:__ Use the provided code to plot a histogram of the sampled list from `a`. Comment on the distribution you observe. How will this distribution affect our synonym detection analysis?\n\n* __c) Code in Notebook:__ Write a Spark Job to compare word frequencies to number of neighbors.\n\n[__`NOTE:`__ _technically these lists are short enough to compare in memory on your local machine but please design your Spark job as if we were potentially comparing much larger lists._]\n\n* __d) Numeric:__ Of the 1000 words with most neighbors, what percent are also in the list of 1000 most frequent words?\n\n* __e) Numeric:__ Of the 1000 words with least neighbors, what percent are also in the list of 1000 least frequent words?   "}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [], "source": "# q6a\n### PROGRAMMING - SEE CELLS BELOW\n### INSTRUCTIONS:  Write a spark job that computes:\n#                    * the number of unique neighbors (i.e. 5-gram co-occuring words) for each word in the vocabulary.\n\n#                    * For example, given a five-gram `a b c a b` our desired output of the number of unique neighbors\n#                        for each unique word as follows: is a:2, b:2, c:2 (from pairs {'a-b', 'b-c', 'a-c'})\n\n#                    * More generally, for a single ngram the number of unique co-occuring words is `N-1`,\n#                        where is `N` is the number of unique words in the n-gram. \n\n#                    * The above  five-gram `a b c a b`  has 3 unique words where each word has 2 unique\n#                        neighors (different to itself)\n\n#                    * For \"a b c d e\", each unique word has a 4 neighbors"}, {"cell_type": "code", "execution_count": 71, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "pairs_list: ['a-b', 'a-c', 'a-b', 'b-c', 'a-b', 'a-c', 'b-c', 'a-b']\nuniq pairs: {'a-c', 'b-c', 'a-b'}\nunigrams: ['a', 'c', 'b', 'c', 'a', 'b']\nc: 2\nb: 2\na: 2\npairs_list: ['a-b', 'a-c', 'a-d', 'a-e', 'b-c', 'b-d', 'b-e', 'c-d', 'c-e', 'd-e']\nuniq pairs: {'b-c', 'b-d', 'c-d', 'b-e', 'a-c', 'a-d', 'a-e', 'd-e', 'c-e', 'a-b'}\nunigrams: ['b', 'c', 'b', 'd', 'c', 'd', 'b', 'e', 'a', 'c', 'a', 'd', 'a', 'e', 'd', 'e', 'c', 'e', 'a', 'b']\ne: 4\nc: 4\nd: 4\nb: 4\na: 4\n"}], "source": "# HINT: run cell as is\nimport itertools\nimport numpy as np\ndef get_num_of_uniq_neighbors(a_str = \"a b c a b\"):\n    words = a_str.split()\n    # we are looking for neighbors of each word (regardless of location in the n-gram context)\n    # pair (a b) is same as pair (b a)\n    pairs_list = [] # all pair combinations\n    pairs = set()   # just unique pair combinations\n    for i, pair in enumerate(itertools.combinations(words, 2)):\n        if pair[0] != pair[1]:\n            pair_str = \"{}-{}\".format(*sorted(pair)) #sort to make pair (a b) the same as pair (b a)\n            #print(f\"pair {i+1}: {pair_str}\")\n            pairs.add(pair_str)\n            pairs_list+=[pair_str]\n    print(f\"pairs_list: {pairs_list}\")\n    print(f\"uniq pairs: {pairs}\")\n    unigrams = [unigram for bigram in pairs for unigram in bigram.split(\"-\")]\n    print(f\"unigrams: {unigrams}\")\n    for unigram in set(unigrams):\n        print(f\"{unigram}: {sum(np.char.count(unigrams, unigram))}\")\n\nget_num_of_uniq_neighbors(a_str = \"a b c a b\")   \nget_num_of_uniq_neighbors(a_str = \"a b c d e\")   \n\n#  output\n#pairs_list: ['a-b', 'a-c', 'a-b', 'b-c', 'a-b', 'a-c', 'b-c', 'a-b'] #dropped \"a-a\" and \"b-b\"\n#uniq pairs: {'a-b', 'b-c', 'a-c'}\n#unigrams: ['a', 'b', 'b', 'c', 'a', 'c']\n# a: 2\n# b: 2\n# c: 2"}, {"cell_type": "code", "execution_count": 72, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "7965afbc-ba04-4482-91c6-369579a412ed", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part a - spark job\ndef EDA2(rdd,n):\n    top_n, bottom_n, sampled_counts = None, None, None\n    ############# YOUR CODE HERE ###############\n\n    # I need a helper function to get the unique neighbors for each ngram\n    def get_unique_pairs(ngram_text):\n        words = ngram_text.lower().split()\n        pairs = set()\n        for pair in itertools.combinations(words, 2):\n            if pair[0] != pair[1]:\n                pair_str = \"{}-{}\".format(*sorted(pair))\n                pairs.add(pair_str)\n        return pairs\n    \n    # main spark part: extract unique pairs, then count neighbors per word\n    resultsRDD = rdd.map(lambda x: x.split('\\t')[0]) \\\n                    .flatMap(lambda ngram: get_unique_pairs(ngram)) \\\n                    .distinct() \\\n                    .flatMap(lambda pair: pair.split('-')) \\\n                    .map(lambda word: (word, 1)) \\\n                    .reduceByKey(lambda x, y: x + y) \\\n                    .cache()\n    \n    top_n = resultsRDD.takeOrdered(n, key=lambda x: -x[1])\n    bottom_n = resultsRDD.takeOrdered(n, key=lambda x: x[1])\n    sampled_counts = resultsRDD.sample(False, 0.01).map(lambda x: x[1]).collect()\n    ############# (END) YOUR CODE ##############\n    return top_n, bottom_n, sampled_counts"}, {"cell_type": "code", "execution_count": 73, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "28365223-462e-4232-abd6-cfa2bc38af66", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Wall time: 0.42907094955444336 seconds\n"}], "source": "# Autograder Skip\n\n# part a - systems test (RUN THIS CELL AS IS)\nstart = time.time()\nmost_nbrs, least_nbrs, sample_counts = EDA2(testRDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Command took 0.83 seconds -- by kyleiwaniec@gmail.com at 12/8/2020, 1:41:59 PM on HW-S21"}, {"cell_type": "code", "execution_count": 74, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "d387813d-7682-467e-b501-d3332ca207fb", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": " --- Most Co-Words ---|--- Least Co-Words ----\n         was        9 |     foolishness    4\n          of        9 |            best    5\n         the        9 |           worst    5\n          it        8 |          wisdom    5\n         age        7 |             age    7\n       times        7 |           times    7\n        best        5 |              it    8\n       worst        5 |             was    9\n      wisdom        5 |              of    9\n foolishness        4 |             the    9\n"}], "source": "# Autograder Skip\n\n# part a - display results (feel free to modify the formatting code if needed)\nprint(\" --- Most Co-Words ---|--- Least Co-Words ----\")\nfor (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "a002829e-8e36-40ec-bde8-1d7d8a39d104", "showTitle": false, "title": ""}}, "source": "Expected output for testRDD:\n<pre>\n --- Most Co-Words ---|--- Least Co-Words ----\n         was        9 |     foolishness    4\n          of        9 |            best    5\n         the        9 |           worst    5\n          it        8 |          wisdom    5\n         age        7 |             age    7\n       times        7 |           times    7\n        best        5 |              it    8\n       worst        5 |             was    9\n      wisdom        5 |              of    9\n foolishness        4 |             the    9\n </pre>"}, {"cell_type": "code", "execution_count": 75, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "f23d9344-a019-4985-bb5b-f98a7bf03bb6", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 34:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 12.000590801239014 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part a - single file test (RUN THIS CELL AS IS)\nstart = time.time()\nmost_nbrs, least_nbrs, sample_counts = EDA2(f1RDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Command took 13.80 seconds -- DCE\n\n#[Stage 40:>                                                         (0 + 1) / 1]\n#Wall time: 10.146554470062256 seconds\n\n#[Stage 50:>                                                         (0 + 4) / 4]\n# Wall time: 6.513125658035278 seconds\n"}, {"cell_type": "code", "execution_count": 76, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "9722b282-a935-48c6-8a10-54a72290f639", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": " --- Most Co-Words ---|--- Least Co-Words ----\n         the    25548 |              vo    1\n          of    22496 |             gem    2\n         and    16489 |           pizza    2\n          to    14249 |            hoot    2\n          in    13891 |     palpitation    2\n           a    13045 |      noncleaved    2\n        that     8011 |            twel    2\n          is     7947 |             sud    2\n        with     7552 |          dalles    2\n          by     7400 |        premiers    2\n"}], "source": "# Autograder Skip\n\n# part a - display results (feel free to modify the formatting code if needed)\nprint(\" --- Most Co-Words ---|--- Least Co-Words ----\")\nfor (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "91305880-710d-4bea-8611-be8482ff1fb6", "showTitle": false, "title": ""}}, "source": "Expected output for f1RDD:\n<pre>\n --- Most Co-Words ---|--- Least Co-Words ----\n         the    25548 |              vo    1\n          of    22496 |      noncleaved    2\n         and    16489 |        premiers    2\n          to    14249 |        enclaves    2\n          in    13891 |   selectiveness    2\n           a    13045 |           trill    2\n        that     8011 |           pizza    2\n          is     7947 |            hoot    2\n        with     7552 |     palpitation    2\n          by     7400 |            twel    2\n</pre>"}, {"cell_type": "code", "execution_count": 77, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "7c1d0769-3bbb-4c2f-886c-a6c286d1552c", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 50:=================================================>    (175 + 4) / 190]\r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 826.868729352951 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part a - full data (RUN THIS CELL AS IS)\nstart = time.time()\nmost_nbrs, least_nbrs, sample_counts = EDA2(dataRDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n\n# Wall time: 796.3152396678925 seconds on n1-std-4 (with gunzipped txt files)"}, {"cell_type": "code", "execution_count": 78, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "dd5932f9-e92a-4001-a11f-7fe9ef1441ad", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": " --- Most Co-Words ---|--- Least Co-Words ----\n         the   164982 |          cococo    1\n          of   155708 |            inin    1\n         and   132814 |        charuhas    1\n          in   110615 |         ooooooo    1\n          to    94358 |           iiiii    1\n           a    89197 |          iiiiii    1\n          by    67266 |             cnj    1\n        with    65127 |            choh    1\n        that    61174 |             neg    1\n          as    60652 |      cococococo    1\n"}], "source": "# Autograder Skip\n\n# q6a\n# part a - display results (feel free to modify the formatting code if needed)\nprint(\" --- Most Co-Words ---|--- Least Co-Words ----\")\nfor (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "5d6764e4-62a4-4fce-9761-9aa5ee03d30e", "showTitle": false, "title": ""}}, "source": "Expected output for dataRDD: \n(bottom words might vary a little due to ties)\n<pre>\n --- Most Co-Words ---|--- Least Co-Words ----\n         the   164982 |          cococo    1\n          of   155708 |            inin    1\n         and   132814 |        charuhas    1\n          in   110615 |         ooooooo    1\n          to    94358 |           iiiii    1\n           a    89197 |          iiiiii    1\n          by    67266 |             cnj    1\n        with    65127 |            choh    1\n        that    61174 |             neg    1\n          as    60652 |      cococococo    1\n</pre>"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "589e0ce5-0d5e-4d34-8948-1f907fb3d316", "showTitle": false, "title": ""}}, "source": "__`NOTE:`__ _before running the plotting code below, make sure that the variable_ `sample_counts` _points to the list generated in_ `part a`."}, {"cell_type": "code", "execution_count": 79, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "41df9fd4-f75a-45f3-86f9-46d207136f18", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "NOTE: we'll exclude the 11 words with more than 6000 nbrs in this 2646 count sample.\n"}, {"data": {"image/png": "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", "text/plain": "<Figure size 1500x500 with 2 Axes>"}, "metadata": {}, "output_type": "display_data"}], "source": "# part b - plot histogram (RUN THIS CELL AS IS - feel free to modify format)\n\n# removing extreme upper tail for a better visual\ncounts = np.array(sample_counts)[np.array(sample_counts) < 6000]\nt = sum(np.array(sample_counts) > 6000)\nn = len(counts)\nprint(\"NOTE: we'll exclude the %s words with more than 6000 nbrs in this %s count sample.\" % (t,n))\n\n# set up figure\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (15,5))\n\n# plot regular hist\nax1.hist(counts, bins=50)\nax1.set_title('Freqency of Number of Co-Words', color='0.1')\nax1.set_facecolor('0.9')\nax1.tick_params(axis='both', colors='0.1')\nax1.grid(True)\n\n# plot log scale hist\nax2.hist(counts, bins=50)\nax2.set_title('(log)Freqency of Number of Co-Words', color='0.1')\nax2.set_facecolor('0.9')\nax2.tick_params(axis='both', colors='0.1')\nax2.grid(True)\nplt.yscale('log')\n"}, {"cell_type": "code", "execution_count": 80, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "d\n"}], "source": "# q6b\n### MULTIPLE CHOICE\n### QUESTION: Use the provided code to plot a histogram of the sampled list from `a`. How will this distribution affect\n#             our synonym detection analysis? What else can we say about our analysis moving forward?\n\n#             Choose the ** WRONG ** statement below.\n\n#   a.) The majority of words in this corpus have between 0 and 1000 co-words\n\n#   b.) Stop words help create the long right tail\n\n#   c.) Words with the most number of neighbors will be easiest to find synonyms for\n\n#   d.) Words with the lowest number of neighbors will be easiest to find synonyms for\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"d\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 81, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "15310f55-9345-4dd1-907b-a719f475e693", "showTitle": false, "title": ""}}, "outputs": [], "source": "# q6c\n# part c - spark job\ndef compareRankings(rdd1, rdd2):\n    percent_overlap = None\n    ############# YOUR CODE HERE ###############\n    total = rdd1.count()\n    words1 = rdd1.map(lambda x: x[0])\n    words2 = rdd2.map(lambda x: x[0])\n    overlap_count = words1.intersection(words2).count()\n    percent_overlap = (overlap_count / total) * 100\n    ############# (END) YOUR CODE ##############\n    return percent_overlap"}, {"cell_type": "code", "execution_count": null, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "daa4ad30-6c4d-4622-8bd6-1a5c6f94287e", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Of the 1000 words with most neighbors, 88.0 percent are also in the list of 1000 most frequent words.\nOf the 1000 words with least neighbors, 1.9 percent are also in the list of 1000 least frequent words.\n"}], "source": "# Autograder Skip\n\n# part d/e - get lists for comparison (RUN THIS CELL AS IS...)\n# (... then change 'testRDD' to 'f1RDD'/'dataRDD' when ready)\ntotal, topWords, bottomWords = EDA1(dataRDD, 1000)\ntopNbrs, bottomNbrs, sample_counts = EDA2(dataRDD, 1000)\ntwRDD = sc.parallelize(topWords)\nbwRDD = sc.parallelize(bottomWords)\ntnRDD = sc.parallelize(topNbrs)\nbnRDD = sc.parallelize(bottomNbrs)\ntop_overlap = compareRankings(tnRDD, twRDD)\nbottom_overlap = compareRankings(bnRDD,bwRDD)\nprint(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\nprint(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")"}, {"cell_type": "code", "execution_count": 84, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "88\n"}], "source": "# q6d\n### NUMERICAL INPUT\n### QUESTION: Of the 1000 words with MOST neighbors, what percent are also in the list of 1000 MOST frequent words?\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING, AND DO NOT INCLUDE THE '%' SIGN IN THE RESPONSE AND ROUND.\n### FOR EXAMPLE, IF YOUR ANSWER IS 64.87%, answer = \"65\". THE AUTOGRADER FRAMEWORK IS PETTY LIKE THAT.\n\nanswer = \"88\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1.9\n"}], "source": "# q6e\n### NUMERICAL INPUT\n### QUESTION: Of the 1000 words with LEAST neighbors, what percent are also in the list of 1000 LEAST frequent words?\n\n### Limit your answer to an integer percentage only and do not include the % sign in the response\n#   (e.g. if your calculations yield a result of 64.87%,\u00a0to respond 65%, just type 65)\n\n### ENTER ONLY THE ANSWER INSIDE THE ANSWER VARIABLE, AS A STRING, AND DO NOT INCLUDE THE '%' SIGN IN THE RESPONSE AND ROUND.\n### FOR EXAMPLE, IF YOUR ANSWER IS 2.54%, answer = \"2.5\". THE AUTOGRADER FRAMEWORK IS PETTY LIKE THAT.\n\nanswer = \"1.9\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "8bda2949-b3f0-4446-8010-4f3d53bfd605", "showTitle": false, "title": ""}, "tags": []}, "source": "# Question 7: Basis Vocabulary & Stripes\n\nEvery word that appears in our data is a potential feature for our synonym detection analysis. However as we've discussed, some are likely to be more useful than others. In this question, you'll choose a judicious subset of these words to form our 'basis vocabulary'. Practically speaking, this means that when we build our stripes, we are only going to keep track of when a term co-occurs with one of these basis words. \n\n\n### Q7 Tasks:\n* __a) Multiple Choice:__ Suppose we were deciding between two different basis vocabularies: the 1000 most frequent words or the 1000 least frequent words. How would this choice impact the quality of the synonyms we are able to detect? How does this choice relate to the ideas of 'overfitting' or 'underfitting' a training set?\n\n* __b) Short Response:__ Explain your answer from Q7a.\n\n* __c) Multiple Choice:__ If we had a much larger dataset, computing the full ordered list of words would be extremely expensive. If we need to none-the-less get an estimate of word frequency in order to decide on a basis vocabulary, what alternative strategy could we take?\n\n* __d) Multiple Choice:__ What is another way to describe the Basis Vocabulary in machine learning terms?\n\n* __e) Code in Notebook:__ Complete the function `get_vocab()` and run the provided spark job that does the following:\n  * tokenizes, removes stopwords and computes a word count on the ngram data\n  * subsets the top 10,000 words (these are the terms we'll consider as potential synonyms)\n  * subsets words 9,000-9,999 (this will be our 1,000 word basis vocabulary)    \n  (to put it another way - of the top 10,000 words, the bottom 1,000 form the basis vocabulary)\n  * saves the full 10K word list and the 1K basis vocabulary to file for use in `d`.  \n\n* __f) Code in Notebook:__ Write a spark job that builds co-occurrence stripes for the top 10K words in the ngram data using the basis vocabulary you developed in `part e`. This job/function, unlike others so far, should return an RDD (which we will then use in Q8)."}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\n"}], "source": "# q7a\n### MULTIPLE CHOICE\n### QUESTION: Suppose we were deciding between two different basis vocabularies: the 1000 most frequent words or the 1000 least\n#             frequent words. How would this choice impact the quality of the synonyms we are able to detect? How does\n#             this choice relate to the ideas of 'overfitting' or 'underfitting' a training set?\u00a0\n\n#   a.) 1000 most frequent words would underfit, while 1000 least frequent words would overfit\n\n#   b.) 1000 most frequent words would overfit, while 1000 least frequent words would underfit\n\n#   c.) Using 1000 most frequent words and 1000 least frequent words would overfit\n\n#   d.) Using 1000 most frequent words and 1000 least frequent words would underfit\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"a\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 87, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nUsing the 1000 most frequent words as the basis vocabulary would underfit because these are stopwords that appear\nin basically all contexts. Stop words like the, of, and, to, in, etc provide no ability to distinguish since most\nword co-occurs with them. If all words share the same features, we cannot distinguish between different meanings.\nThis means having features that are constant across all training examples, preventing the model from learning\nmeaningful patterns. Using the 1000 least frequent words, we would overfit because these rare terms appear in very\nfew contexts, (our results show mainly 40 times each). With such sparse data, any detected co-occurrence patterns\nare more likely due to random chance rather than true semantic similarity. A word appearing in just a handful of\nrare contexts provides insufficient evidence to generalize about its meaning. This is like training on too few\nexamples with very specific features, causing the model to fit noise rather than signal and fail to generalize to\nnew data. An optimal vocabulary would use mid-frequency words that are common enough to provide statistical\nevidence across many contexts but distinguishing enough to separate between different word meanings. This balances\nhaving sufficient data to detect patterns while maintaining features that actually vary meaningfully across\ndifferent semantic categories.\n\n"}], "source": "# q7b\n### SHORT RESPONSE\n### QUESTION: Please explain your answer from\u00a0 Q7.a.\u00a0\n\n### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n\nprint(\n\"\"\"\nUsing the 1000 most frequent words as the basis vocabulary would underfit because these are stopwords that appear\nin basically all contexts. Stop words like the, of, and, to, in, etc provide no ability to distinguish since most\nword co-occurs with them. If all words share the same features, we cannot distinguish between different meanings.\nThis means having features that are constant across all training examples, preventing the model from learning\nmeaningful patterns. Using the 1000 least frequent words, we would overfit because these rare terms appear in very\nfew contexts, (our results show mainly 40 times each). With such sparse data, any detected co-occurrence patterns\nare more likely due to random chance rather than true semantic similarity. A word appearing in just a handful of\nrare contexts provides insufficient evidence to generalize about its meaning. This is like training on too few\nexamples with very specific features, causing the model to fit noise rather than signal and fail to generalize to\nnew data. An optimal vocabulary would use mid-frequency words that are common enough to provide statistical\nevidence across many contexts but distinguishing enough to separate between different word meanings. This balances\nhaving sufficient data to detect patterns while maintaining features that actually vary meaningfully across\ndifferent semantic categories.\n\"\"\"\n)"}, {"cell_type": "code", "execution_count": 88, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "b\n"}], "source": "# q7c\n### MULTIPLE CHOICE\n### QUESTION: If we had a much larger dataset, computing the full ordered list of words would be extremely expensive.\n#             If we need to nonetheless get an estimate of word frequency in order to decide on a basis vocabulary,\n#             what alternative strategy could we take?\n\n#   a.) We should remove common stop words before calculating the word frequency for the rest of words in the corpus\n\n#   b.) Instead of computing the frequency over the whole corpus we could sample files from the corpus and use\n#       that smaller dataset to get our vocabulary\n\n#   c.) The order of words frequency matter and therefore, we should not implement any estimation strategies \n\n#   d.) None of the answers is correct\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"b\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 89, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "d\n"}], "source": "# q7d\n### MULTIPLE CHOICE\n### QUESTION: What is another way to describe the Basis Vocabulary in machine learning terms?\n\n#   a.) Stop-words\n\n#   b.) 1000-grams\n\n#   c.) Postings\n\n#   d.) Features\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"d\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 92, "metadata": {}, "outputs": [], "source": "# q7e\n### PROGRAMMING - SEE CELLS BELOW\n### INSTRUCTIONS:  Complete the function `get_vocab()` and run the provided spark job that does the following:\n#                    * tokenizes, removes stopwords and computes a word count on the ngram data\n\n#                    * subsets the top 10,000 words (these are the terms we'll consider as potential synonyms)\n\n#                    * subsets words 9,000-9,999 (this will be our 1,000 word basis vocabulary)    \n#                        (to put it another way - of the top 10,000 words, the bottom 1,000 form the basis vocabulary)\n\n#                    * saves the full 10K word list and the 1K basis vocabulary to file for use in `d`.  "}, {"cell_type": "code", "execution_count": 93, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "b853bc36-399b-4f21-af49-697c2019f6a9", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part e - provided stopwords (RUN THIS CELL AS IS)\nSTOPWORDS =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n              'ourselves', 'you', 'your', 'yours', 'yourself', \n              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n              'her', 'hers', 'herself', 'it', 'its', 'itself', \n              'they', 'them', 'their', 'theirs', 'themselves', \n              'what', 'which', 'who', 'whom', 'this', 'that', \n              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n              'between', 'into', 'through', 'during', 'before', \n              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n              'further', 'then', 'once', 'here', 'there', 'when', \n              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n              'too', 'very', 'should', 'can', 'now', 'will', 'just', \n              'would', 'could', 'may', 'must', 'one', 'much', \"it's\",\n              \"can't\", \"won't\", \"don't\", \"shouldn't\", \"hasn't\"]"}, {"cell_type": "code", "execution_count": 94, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "6f3bad8f-3d76-401d-8687-e16db841de68", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part e - get the vocabulary and basis (RUN THIS CELL AS IS)\ndef get_vocab(rdd, n_total, n_basis):\n    vocab, basis = None, None\n    ############# YOUR CODE HERE ###############\n    stopwords = sc.broadcast(set(STOPWORDS))\n    \n    wordRDD = rdd.map(lambda line: line.split('\\t')[:2]) \\\n                .flatMap(lambda parts: [(word.lower(), int(parts[1])) \n                                        for word in parts[0].split(\" \") \n                                        if word.lower() not in stopwords.value]) \\\n                .reduceByKey(lambda x, y: x + y) \\\n                .sortBy(lambda x: -x[1]) \\\n                .zipWithIndex() \\\n                .filter(lambda item: item[1] < n_total) \\\n                .cache()\n    stopwords.unpersist()\n    \n    vocab = wordRDD.map(lambda item: item[0][0]).collect()\n    basis = wordRDD.filter(lambda item: item[1] >= (n_total - n_basis)) \\\n                   .map(lambda item: item[0][0]) \\\n                   .collect()\n    \n    wordRDD.unpersist()    \n    ############# (END) YOUR CODE ##############\n    return vocab, basis"}, {"cell_type": "code", "execution_count": 95, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "817d0400-d9a5-4f5f-9fc8-106fc2614c75", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 105:======================================>              (138 + 4) / 190]\r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 236.86803460121155 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part e - run your job (RUN THIS CELL AS IS)\nstart = time.time()\nVOCAB, BASIS = get_vocab(dataRDD, 10000, 1000)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# 268.0176115036011 seconds"}, {"cell_type": "code", "execution_count": 97, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "320fa840-3dd9-40ce-9584-4e7861d5b8f2", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part e - save to file (RUN THIS CELL AS IS)\n# with open(\"vocabulary.txt\", \"w\") as file:\n#    file.write(str(VOCAB))\n# with open(\"basis.txt\", \"w\") as file:\n#    file.write(str(BASIS))"}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying from <STDIN>...\n/ [1 files][    0.0 B/    0.0 B]                                                \nOperation completed over 1 objects.                                              \nCopying from <STDIN>...\n/ [1 files][    0.0 B/    0.0 B]                                                \nOperation completed over 1 objects.                                              \n"}], "source": "# part e - save to file (RUN THIS CELL AS IS)\n!echo \"{','.join(VOCAB)}\" | gsutil cp - {HW3_FOLDER}/output/vocabulary.txt\n!echo \"{','.join(BASIS)}\" | gsutil cp - {HW3_FOLDER}/output/basis.txt\n"}, {"cell_type": "code", "execution_count": 99, "metadata": {}, "outputs": [], "source": "# q7f\n### PROGRAMMING - SEE CELLS BELOW\n### INSTRUCTIONS: Write a spark job that builds co-occurrence stripes for the top 10K words in the ngram data\n#                 using the basis vocabulary you developed in `part e`. This job/function, unlike others so far,\n#                 should return an RDD (which we will then use in Q8).\n"}, {"cell_type": "code", "execution_count": 100, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "e26c9657-5646-4835-b065-489c874ad30b", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part f - spark job\ndef buildStripes(rdd, vocab, basis):\n    stripesRDD = None\n    ############# YOUR CODE HERE ###############\n    b_vocab = sc.broadcast(set(vocab))\n    b_basis = sc.broadcast(set(basis))\n    \n    def get_stripe_pairs(ngram_text, vocab_set, basis_set):\n        words = ngram_text.lower().split()\n        vocab_words = [w for w in words if w in vocab_set]\n        basis_words = [w for w in words if w in basis_set]\n        \n        for vocab_word in vocab_words:\n            for basis_word in basis_words:\n                if vocab_word != basis_word:\n                    yield (vocab_word, basis_word)\n    \n    stripesRDD = rdd.map(lambda line: line.split('\\t')[0]) \\\n                    .flatMap(lambda ngram: get_stripe_pairs(ngram, b_vocab.value, b_basis.value)) \\\n                    .groupByKey() \\\n                    .mapValues(set)\n    \n    b_vocab.unpersist()\n    b_basis.unpersist()\n    ############# (END) YOUR CODE ##############\n    return stripesRDD"}, {"cell_type": "markdown", "metadata": {}, "source": "##### HINT\n\n```python\ntestRDD = ['it was the best of\\t1\\t1\\t1',\n 'age of wisdom it was\\t1\\t1\\t1',\n 'best of times it was\\t1\\t1\\t1',\n 'it was the age of\\t2\\t1\\t1',\n .....]\n```\n\n\nIf given a 5-gram Record: \n`vocab_word1, basis1, vocab_word2, basis2, junk_word`\n\nThen emit one should emit the following records: \n* vocab_word1: {basis1, basis2}\n* vocab_word2: {basis1, basis2}"}, {"cell_type": "code", "execution_count": 101, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "2a70b273-fb86-4f86-8111-a3480ecf18a7", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('best', {'times'}), ('worst', {'times'}), ('foolishness', {'age'}), ('age', {'wisdom', 'times', 'foolishness'}), ('wisdom', {'age'}), ('times', {'worst', 'best', 'age'})]\nWall time: 0.16455435752868652 seconds\n"}, {"data": {"text/plain": "\"\\n[('worst', {'times'}), ('best', {'times'}), ('foolishness', {'age'}), ('age', {'wisdom', 'foolishness', 'times'}), ('wisdom', {'age'}), ('times', {'age', 'best', 'worst'})]\\n\""}, "execution_count": 101, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\n# part f - run your systems test (RUN THIS CELL AS IS)\n\n\"\"\"testRDD = ['it was the best of\\t1\\t1\\t1',\n 'age of wisdom it was\\t1\\t1\\t1',\n 'best of times it was\\t1\\t1\\t1',\n 'it was the age of\\t2\\t1\\t1',\n .....]\n\"\"\"\nVOCAB, BASIS = get_vocab(testRDD, 10, 10)\ntestStripesRDD = buildStripes(testRDD, VOCAB, BASIS)\nstart = time.time()\nprint(testStripesRDD.collect())\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Wall time: 0.1581110954284668 seconds\n# Expected results\n'''\n[('worst', {'times'}), ('best', {'times'}), ('foolishness', {'age'}), ('age', {'wisdom', 'foolishness', 'times'}), ('wisdom', {'age'}), ('times', {'age', 'best', 'worst'})]\n'''"}, {"cell_type": "code", "execution_count": 102, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "2d6a2071-70d0-482b-9290-4e91d9f543d9", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 125:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "[('zippor', {'balak'}), ('zedong', {'mao'}), ('zeal', {'infallibility'}), ('youth', {'mould', 'constrained'}), ('younger', {'careers'})]\nWall time: 1.4811792373657227 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "\"\\n[('zippor', {'balak'}), ('zedong', {'mao'}), ('zeal', {'infallibility'}), ('youth', {'mould', 'constrained'}), ('younger', {'careers'})]\\n\""}, "execution_count": 102, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\n# part f - run your single file test (RUN THIS CELL AS IS)\nVOCAB, BASIS = get_vocab(f1RDD, 10000, 1000)\nf1StripesRDD = buildStripes(f1RDD, VOCAB, BASIS).cache()\nstart = time.time()\nprint(f1StripesRDD.top(5))\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Wall time: 1.55739426612854 seconds\n# Expected results\n'''\n[('zippor', {'balak'}), ('zedong', {'mao'}), ('zeal', {'infallibility'}), ('youth', {'mould', 'constrained'}), ('younger', {'careers'})]\n'''"}, {"cell_type": "code", "execution_count": 103, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "470096b5-c854-4a3a-b8b7-50e45cd23c72", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 128:=================================================>   (177 + 4) / 190]\r"}, {"name": "stdout", "output_type": "stream", "text": "zones\n['residential', 'adhesion', 'buffer', 'subdivided', 'localities', 'uppermost', 'remotest', 'environments', 'saturation', 'gaza', 'warmer', 'parks']\n-------\nzone\n['diffuse', 'penis', 'officially', 'originate', 'flexor', 'masculine', 'ie', 'sandy', 'turbulent', 'americas', 'parked', 'fibrous', 'tribal', 'inorganic', 'poorly', 'narrower', 'accumulate', 'guides', 'uppermost', 'auxiliary', 'articular', 'assisting', 'glowing', 'penetrating', 'southeastern', 'residential', 'transitional', 'intervening', 'buffer', 'subdivided', 'defines', 'unusually', 'saturation', 'cracks', 'cartilage', 'atlas', 'traversed', 'illuminated', 'vomiting', 'alaska', 'avoidance', 'au', 'excitation', 'contamination', 'persia', 'trigger']\n-------\nzinc\n['phosphorus', 'leukemia', \"alzheimer's\", 'coating', 'wasting', 'pancreas', 'weighing', 'transcription', 'ammonium', 'diamond', 'dysfunction', 'dipped', 'burns', 'hydroxide', 'metallic', 'dietary', 'radioactive', 'insoluble']\n-------\nWall time: 142.96420097351074 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "'\\nzones\\n[\\'remotest\\', \\'adhesion\\', \\'residential\\', \\'subdivided\\', \\'environments\\', \\'gaza\\', \\'saturation\\', \\'localities\\', \\'uppermost\\', \\'warmer\\', \\'buffer\\', \\'parks\\']\\n-------\\nzone\\n[\\'tribal\\', \\'narrower\\', \\'fibrous\\', \\'saturation\\', \\'originate\\', \\'auxiliary\\', \\'ie\\', \\'buffer\\', \\'transitional\\', \\'turbulent\\', \\'vomiting\\', \\'americas\\', \\'articular\\', \\'poorly\\', \\'intervening\\', \\'officially\\', \\'accumulate\\', \\'assisting\\', \\'flexor\\', \\'traversed\\', \\'unusually\\', \\'uppermost\\', \\'cartilage\\', \\'inorganic\\', \\'illuminated\\', \\'glowing\\', \\'contamination\\', \\'trigger\\', \\'masculine\\', \\'defines\\', \\'avoidance\\', \\'residential\\', \\'southeastern\\', \\'penis\\', \\'cracks\\', \\'atlas\\', \\'excitation\\', \\'persia\\', \\'diffuse\\', \\'subdivided\\', \\'alaska\\', \\'guides\\', \\'au\\', \\'sandy\\', \\'penetrating\\', \\'parked\\']\\n-------\\nzinc\\n[\\'ammonium\\', \\'coating\\', \\'pancreas\\', \\'insoluble\\', \"alzheimer\\'s\", \\'diamond\\', \\'radioactive\\', \\'metallic\\', \\'weighing\\', \\'dysfunction\\', \\'wasting\\', \\'phosphorus\\', \\'transcription\\', \\'dipped\\', \\'hydroxide\\', \\'burns\\', \\'leukemia\\', \\'dietary\\']\\n-------\\n'"}, "execution_count": 103, "metadata": {}, "output_type": "execute_result"}], "source": "# Autograder Skip\n\n# part f - run the full analysis and take a look at a few stripes (RUN THIS CELL AS IS)\n#VOCAB = ast.literal_eval(open(\"vocabulary.txt\", \"r\").read())\n#BASIS = ast.literal_eval(open(\"basis.txt\", \"r\").read())\nwordzz= !gsutil cat {HW3_FOLDER}/output/vocabulary.txt\nVOCAB = wordzz[0].split(\",\")\nwordzz= !gsutil cat {HW3_FOLDER}/output/basis.txt\nBASIS = wordzz[0].split(\",\")\n\nstripesRDD = buildStripes(dataRDD, VOCAB, BASIS).cache()\n\nstart = time.time()\nfor wrd, stripe in stripesRDD.top(3):\n    print(wrd)\n    print(list(stripe))\n    print('-------')\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Wall time: 214.13801431655884 seconds\n# Expected results:\n'''\nzones\n['remotest', 'adhesion', 'residential', 'subdivided', 'environments', 'gaza', 'saturation', 'localities', 'uppermost', 'warmer', 'buffer', 'parks']\n-------\nzone\n['tribal', 'narrower', 'fibrous', 'saturation', 'originate', 'auxiliary', 'ie', 'buffer', 'transitional', 'turbulent', 'vomiting', 'americas', 'articular', 'poorly', 'intervening', 'officially', 'accumulate', 'assisting', 'flexor', 'traversed', 'unusually', 'uppermost', 'cartilage', 'inorganic', 'illuminated', 'glowing', 'contamination', 'trigger', 'masculine', 'defines', 'avoidance', 'residential', 'southeastern', 'penis', 'cracks', 'atlas', 'excitation', 'persia', 'diffuse', 'subdivided', 'alaska', 'guides', 'au', 'sandy', 'penetrating', 'parked']\n-------\nzinc\n['ammonium', 'coating', 'pancreas', 'insoluble', \"alzheimer's\", 'diamond', 'radioactive', 'metallic', 'weighing', 'dysfunction', 'wasting', 'phosphorus', 'transcription', 'dipped', 'hydroxide', 'burns', 'leukemia', 'dietary']\n-------\n'''"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Checkpoint the stripes (Just RUN all cells in this section AS IS and review outputs)\nLet's save your full stripes to disk. Then we can reload later if needed. We repartition our data first and then save (as otherwise, we will end up with 190 partitions; I wonder why!).\n\n```python \n!gsutil -m rm -r {HW3_FOLDER}/stripes 2> /dev/null   ##remove old results\nstripesRDD.repartition(4).saveAsTextFile(f'{HW3_FOLDER}/stripes')  #repartition and write partitions to Google Cloud Bucket\n!gsutil ls -lh {HW3_FOLDER}/stripes \n```\n\nThe above produces the following output directory: \n```\n       0 B  2022-09-15T20:14:37Z  gs://<your_bucket_name>/notebooks/jupyter/Assignments/HW3/stripes/\n       0 B  2022-09-15T20:14:37Z  gs://<your_bucket_name>/notebooks/jupyter/Assignments/HW3/stripes/_SUCCESS\n  1.74 MiB  2022-09-15T20:14:36Z  gs://<your_bucket_name>/notebooks/jupyter/Assignments/HW3/stripes/part-00000\n   1.6 MiB  2022-09-15T20:14:36Z  gs://<your_bucket_name>/notebooks/jupyter/Assignments/HW3/stripes/part-00001\n  1.58 MiB  2022-09-15T20:14:36Z  gs://<your_bucket_name>/notebooks/jupyter/Assignments/HW3/stripes/part-00002\n  1.52 MiB  2022-09-15T20:14:36Z  gs://<your_bucket_name>/notebooks/jupyter/Assignments/HW3/stripes/part-00003\nTOTAL: 6 objects, 6745577 bytes (6.43 MiB)\n```\n\nThe following code displays the stripe of cooccurence words for the term `sea`:\n\n```python\n!gsutil cat {HW3_FOLDER}/stripes/part-00000|head -n 1\n```\n```python\n('sea', {'sweeping', 'twisted', 'athenians', 'fog', 'tumult', 'repression', 'morphology', 'jane', 'secreted', 'tents', 'barred', 'sadness', 'hamlet', 'turbulent', 'rains', 'robe', 'imagery', 'myths', 'orient', 'intervening', 'victories', 'accumulate', 'sinners', 'constancy', 'strained', 'sermons', 'shoe', 'trembled', 'merged', 'eastward', 'avoidance', 'sensibility', 'informing', 'silently', 'dip', 'surround', 'blocked', 'voyages', 'bursting', 'vastly', 'southeastern', 'cracks', 'tore', 'temperament', \"ship's\", 'odor', 'atlas', 'matthew', 'ether', 'colonization', 'irresistible', 'shells', 'alaska', 'gaza', 'distributions', 'farthest', 'silly', 'flush', 'ugly', 'transparent', 'arabian', 'sandy', 'steering', 'penetrating', 'burns', 'norway', 'thames', 'moonlight', 'plunge', 'beset', 'yielding', 'tuesday', 'impacts', 'cheese', 'convex', 'armistice', 'polished', 'freshness', 'belgium', 'saturation', 'dumb', 'spoil', 'shines', 'sunset', 'softly', 'laden', 'realms', 'alexandria', 'parallels', 'weep', 'ushered', 'violently', 'expanse', 'travellers', 'insoluble', 'downs', 'roofs', 'filtered', 'ashore', 'graces', 'obscured', 'establishments', 'traversed', 'crystalline', 'warmer', 'skins', 'viewing', 'fascination', 'liverpool', 'contamination', 'sails', 'masculine', 'usages', 'bucket', 'dipped', 'dew', 'fare', 'overlooking', 'necks', 'sticks', 'weighing', 'danube', 'mast', 'phosphorus', 'mate', 'attested', 'anonymous', 'wax', 'finishing', 'parked', 'flocks', 'humidity', 'endurance', 'terrors', 'carpet', 'misfortunes', 'hydroxide', 'crazy', 'priesthood', 'hungary', 'nova', 'believeth', 'remotest', 'occupants', 'complexion', 'floors', 'stationary', 'provoked', 'osmotic', 'spoils', 'clearance', 'hangs', 'openings', 'halfway', 'inorganic', 'nursery', 'vigilance', 'conqueror', 'ft', 'feathers', 'roses', 'emblem', 'lawn', 'damp', 'switzerland', 'drinks', 'contradictory', 'drained', 'ordinances', 'captains', 'barren', 'steamer', 'pursuits', 'storms', 'wasting', 'frankly', 'sequences', 'pitched', 'aggravated', 'viceroy', 'leaped', 'cunning', 'simon', 'marching', 'lends', 'sherman', 'centered', 'genome', 'iran', 'sued', 'imputed', 'perilous', 'desperately', 'southward', 'maiden', 'unusually', 'crosses', 'revealing', 'uppermost', 'remission', 'inherit', 'sunny', 'ink', 'restless', 'lighting', 'serpent', 'scarlet', 'hebrews', 'flourish', 'terminology', 'bidding', 'autobiography', 'despise', 'signification', 'preparatory', 'radioactive', 'drying', 'persia', 'unfamiliar', 'twist', 'fiery', 'boon', 'delights', 'commonest', 'bounty', 'traders', 'whoever'})\n```"}, {"cell_type": "code", "execution_count": 104, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "4c83537a-cc0c-4629-9aa1-eaefeb9bfa46", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part f - save your full stripes to file for ease of retrival later... Please run code as is \n!gsutil -m rm -r {HW3_FOLDER}/stripes 2> /dev/null   ##remove old results\nstripesRDD.repartition(4).saveAsTextFile(f'{HW3_FOLDER}/stripes')  #repartition and write partitions to Google Cloud Bucket"}, {"cell_type": "code", "execution_count": 105, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "       0 B  2025-10-06T06:11:10Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/stripes/\n       0 B  2025-10-06T06:11:11Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/stripes/_SUCCESS\n  1.74 MiB  2025-10-06T06:11:09Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/stripes/part-00000\n   1.6 MiB  2025-10-06T06:11:09Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/stripes/part-00001\n  1.58 MiB  2025-10-06T06:11:09Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/stripes/part-00002\n  1.52 MiB  2025-10-06T06:11:09Z  gs://arun-fall-2025/notebooks/jupyter/Assignments/HW3/stripes/part-00003\nTOTAL: 6 objects, 6745577 bytes (6.43 MiB)\n"}], "source": "# part f - list all partitions in the saved output folder... (RUN THIS CELL AS IS)\n!gsutil ls -lh {HW3_FOLDER}/stripes "}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "('pairs', {'twisted', 'individually', 'terminals', 'originate', 'groove', 'canals', 'cleaned', 'contending', 'alternate', 'wires', 'telescope', 'kashmir', 'twin', 'troop', 'masculine', 'feathers', 'dissociation', 'transcription', 'housed', 'schizophrenia', 'sequences', 'transparent', 'au', 'arches', 'ganglia'})\n('dissolution', {'tribal', 'beset', 'polymer', 'impending', 'initiate', 'worldly', 'ie', 'myths', 'uric', 'assemblies', 'ferdinand', 'regeneration', 'preside', 'procuring', 'awaiting', 'hungarian', 'provoke', 'postpone', 'proclaim', 'extracellular', 'anarchy', 'preparatory', 'weighing', 'prussian', 'whigs', 'sabha', 'undergoing'})\n"}], "source": "# q7f\n# part f - display word stripes for a couple of terms... (RUN THIS CELL AS IS)\n!gsutil cat {HW3_FOLDER}/stripes/part-00000|head -n 2"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "755130f1-1af3-472e-bd74-3435b7e2939f", "showTitle": false, "title": ""}}, "source": "# Question 8: Synonym Detection\n\nWe're now ready to perform the main synonym detection analysis. In the tasks below you will compute cosine, jaccard, dice and overlap similarity measurements for each pair of words in our vocabulary and then sort your results to find the most similar pairs of words in this dataset. __`IMPORTANT:`__ When you get to the sorting step please __sort on cosine similarity__ only, so that we can ensure consistent results from student to student. \n\nRemember to test each step of your work with the small files before running your code on the full dataset. This is a computationally intense task: well designed code can be the difference between a 20min job and a 2hr job. __`NOTE:`__ _as you are designing your code you may want to review questions 3 and 4 where we modeled some of the key pieces of this analysis._"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src='https://raw.githubusercontent.com/UCB-w261/w261-environment/master/hw-images/HW3/interted_index_drawing.png' style='width:80%'>"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "755130f1-1af3-472e-bd74-3435b7e2939f", "showTitle": false, "title": ""}}, "source": "### Q8 Tasks:\n\n* __a) Multiple Choice:__ In question 7 you wrote a function that would create word stripes for each `term` in our vocabulary. These word stripes are essentially an 'embedded representation' of the `term`'s meaning.\n\n    What is the 'feature space' for this representation?\n    * A. the 'basis' vocabulary\n    * B. the  stripe\n    * C. the total vocabulary\n    * D. the postings\n\n\n* __b) Multiple Choice:__ In question 7 you wrote a function that would create word stripes for each `term` in our vocabulary. These word stripes are essentially an 'embedded representation' of the `term`'s meaning. \n\n    What is the maximum length of a stripe?\n    * A. 1000\n    * B. 9999\n    * C. 5000\n    * D. unlimited \n\n\n* __c) Multiple Choice:__ Remember that we are going to treat these stripes as 'documents' and perform similarity analysis on them. The first step is to emit postings which then get collected to form an 'inverted index.'\n\n    How many rows will there be in our inverted index?\n    * A. 1000\n    * B. 9999\n    * C. 5000\n    * D. unlimited \n\n\n* __d) Short Response:__ In the demo from question 2, we were able to compute the cosine similarity directly from the stripes (we did this using their vector form, but could have used the list instead). So why do we need the inverted index?\n\n* __e) Code in Notebook:__ Write a spark job that does the following:\n  * loops over the stripes from Q7 and emits postings for the `term` _(key:term, value:posting)_   \n  * aggregates the postings to create an inverted index _(key:term, value:list of postings)_\n  * loops over all pairs of `term`s that appear in the same postings list and emits co-occurrence counts\n  * aggregates co-occurrences _(key:word pair, value:count + other payload)_\n  * uses the counts (along with the accompanying information) to compute the cosine, jacard, dice and overlap similarity metrics for each pair of words in the vocabulary \n  * retrieve the top 20 and bottom 20 most/least similar pairs of words\n  * also return the cached sorted RDD for use in the next question  \n  __`NOTE 1`:__ _Don't forget to include the stripe length when you are creating the postings & co-occurrence pairs. A composite key is the way to go here._  \n  __`NOTE 2`:__ _Please make sure that your final results are sorted according to cosine similarity otherwise your results may not match the expected result & you will be marked wrong._\n\n\n* __f) Short Response:__ Comment on the quality of the \"synonyms\" your analysis comes up with. Do you notice anything odd about these pairs of words? Discuss at least one idea for how you might go about improving on the analysis."}, {"cell_type": "code", "execution_count": 107, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "a\n"}], "source": "# q8a\n### MULTIPLE CHOICE\n### QUESTION: What is the 'feature space' for this representation?\n\n#   a.) the 'basis' vocabulary\n#   b.) the stripe\n#   c.) the total vocabulary\n#   d.) the postings\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"a\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 108, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "b\n"}], "source": "# q8b\n### MULTIPLE CHOICE\n### QUESTION: Recall that a stripe represents a vector of co-occurring words. This vector of co-occurring words could\n#             be the entire vocabulary but is generally restricted to an informative subset of words. In the\n#             context of synonym detection throughout this assignment, what is the maximum length of a stripe of\n#             co-occurring words?\n\n#   a.) 10000\n#   b.) 1000\n#   c.) 269339\n#   d.) vocabulary size\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"b\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 109, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "b\n"}], "source": "# q8c\n### MULTIPLE CHOICE\n### QUESTION: \u00a0Again, in the context of the synonym detection problem throughout this assignment.\n#              How many rows will there be in our inverted index?\n\n#   a.) 10000\n#   b.) 1000\n#   c.) 269339\n#   d.) vocabulary size\n\n### ENTER ONLY THE LETTER INSIDE THE PRINT STATEMENT. (i.e. if your answer is x.), enter \"x\" inside the answer variable.)\nanswer = \"b\"\n\n\n#####################\nprint(answer)"}, {"cell_type": "code", "execution_count": 110, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nIn Question 2, we computed cosine similarity directly from stripes because we had all the data in memory on a\nsingle machine with only 6 words. For our large-scale problem with 10,000 vocabulary words, computing similarities\ndirectly would require comparing every stripe against every other stripe, resulting in millions of pairwise\ncomparisons. This brute force approach would be computationally expensive and wasteful because most word pairs\nshare no common basis words and then have zero similarity.The inverted index takes advantage of sparsity by\ntransposing the data structure. Instead of comparing all stripes to each other, we group vocab words by the basis\nwords they share. For each basis word in the inverted index, we only emit pairs of vocab words that both contain\nthat basis word in their stripes. This means we only compute similarities for word pairs that actually have\nnon-zero overlap, skipping the vast majority of comparisons that would produce zero. The inverted index reduces\ncomputation to O, which is smaller due to sparsity. This is the main insight from the DISCO algorithm, allowing\ndimension-independent similarity computation that scales linearly rather than quadratically with vocabulary size.\n\n"}], "source": "# q8d\n### SHORT RESPONSE\n### QUESTION: In the demo from question 2, we were able to compute the cosine similarity directly from\n#             the stripes (we did this using their vector form, but could have used the list instead).\n#             So why do we need the inverted index?\n\n### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n\nprint(\n\"\"\"\nIn Question 2, we computed cosine similarity directly from stripes because we had all the data in memory on a\nsingle machine with only 6 words. For our large-scale problem with 10,000 vocabulary words, computing similarities\ndirectly would require comparing every stripe against every other stripe, resulting in millions of pairwise\ncomparisons. This brute force approach would be computationally expensive and wasteful because most word pairs\nshare no common basis words and then have zero similarity.The inverted index takes advantage of sparsity by\ntransposing the data structure. Instead of comparing all stripes to each other, we group vocab words by the basis\nwords they share. For each basis word in the inverted index, we only emit pairs of vocab words that both contain\nthat basis word in their stripes. This means we only compute similarities for word pairs that actually have\nnon-zero overlap, skipping the vast majority of comparisons that would produce zero. The inverted index reduces\ncomputation to O, which is smaller due to sparsity. This is the main insight from the DISCO algorithm, allowing\ndimension-independent similarity computation that scales linearly rather than quadratically with vocabulary size.\n\"\"\"\n)"}, {"cell_type": "code", "execution_count": 111, "metadata": {}, "outputs": [], "source": "# q8e\n### PROGRAMMING - SEE CELLS BELOW\n### INSTRUCTIONS: Write a spark job that does the following:\n#                   * loops over the stripes from Q7 and emits postings for the `term` (key:term, value:posting)\n\n#                   * aggregates the postings to create an inverted index (key:term, value:list of postings)\n\n#                   * loops over all pairs of `term`s that appear in the same postings list and emits co-occurrence counts\n\n#                   * aggregates co-occurrences (key:word pair, value:count + other payload)\n\n#                   * uses the counts (along with the accompanying information) to compute the cosine, jacard, dice\n#                       and overlap similarity metrics for each pair of words in the vocabulary\n\n#                   * retrieve the top 20 and bottom 20 most/least similar pairs of words\n\n#                   * also return the cached sorted RDD for use in the next question\n\n#                   NOTE 1: Don't forget to include the stripe length when you are creating the postings &\n#                               co-occurrence pairs. A composite key is the way to go here.\n\n#                   NOTE 2: Please make sure that your final results are sorted according to cosine similarity\n#                               otherwise your results may not match the expected result & you will be marked wrong."}, {"cell_type": "code", "execution_count": 112, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "b56bac93-522f-4c29-8895-3b4844720edb", "showTitle": false, "title": ""}}, "outputs": [], "source": "# helper function for pretty printing (RUN THIS CELL AS IS)\ndef displayOutput(lines):\n    template = \"{:25}|{:6}, {:7}, {:7}, {:5}\"\n    print(template.format(\"Pair\", \"Cosine\", \"Jaccard\", \"Overlap\", \"Dice\"))\n    for pair, scores in lines:\n        scores = [round(s,4) for s in scores]\n        print(template.format(pair, *scores))"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "97a29461-b203-49cb-8bc1-16127644446c", "showTitle": false, "title": ""}}, "source": "__`TIP:`__ Feel free to define helper functions within the main function to help you organize your code. Readability is important! Eg:\n```\ndef similarityAnlysis(stripesRDD):\n    \"\"\"main docstring\"\"\"\n    \n    simScoresRDD, top_n, bottom_n = None, None, None\n    \n    ############ YOUR CODE HERE ###########\n    def helper1():\n        \"\"\"helper docstring\"\"\"\n        return x\n        \n    def helper2():\n        \"\"\"helper docstring\"\"\"\n        return x\n        \n    # main spark job starts here\n    \n        ...etc\n    ############ (END) YOUR CODE ###########\n    return simScoresRDD, top_n, bottom_n\n```"}, {"cell_type": "code", "execution_count": 113, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "c654dc13-824b-4905-920f-d11443f3d074", "showTitle": false, "title": ""}}, "outputs": [], "source": "# part e - write your spark job in the space provided\ndef similarityAnalysis(stripesRDD, n):\n    \"\"\"\n    This function defines a Spark DAG to compute cosine, jaccard, \n    overlap and dice scores for each pair of words in the stripes\n    provided. \n    \n    Output: an RDD, a list of top n, a list of bottom n\n    \"\"\"\n    simScoresRDD, top_n, bottom_n = None, None, None\n    \n    ############### YOUR CODE HERE ################\n    def sim_scores(line):\n        \"\"\"Compute all similarity metrics for a word pair\"\"\"\n        (word1, n1), (word2, n2) = ast.literal_eval(line[0])\n        intersection = int(line[1])\n        # Cosine similarity\n        cosine = intersection / (np.sqrt(n1) * np.sqrt(n2))\n        # Jaccard similarity\n        jaccard = intersection / float(n1 + n2 - intersection)\n        # Overlap similarity\n        overlap = intersection / min(n1, n2)\n        # Dice similarity\n        dice = (2 * intersection) / (n1 + n2)\n        yield word1 + \" - \" + word2, [cosine, jaccard, overlap, dice]\n    \n    # Build inverted index from stripes. For each stripe (vocab_word, {basis_words}), emit\n    # (basis_word, (vocab_word, stripe_length)). Emit word pairs that share basis words. Aggregate intersection\n    # counts. And compute similarities\n    simScoresRDD = stripesRDD.flatMap(lambda x: [(basis_word, [(x[0], len(x[1]))]) \n                                                   for basis_word in x[1]]) \\\n                              .reduceByKey(lambda x, y: x + y) \\\n                              .flatMap(lambda x: [(str(subset), 1) \n                                                  for subset in itertools.combinations(sorted(x[1]), 2)]) \\\n                              .reduceByKey(lambda x, y: x + y) \\\n                              .flatMap(sim_scores) \\\n                              .cache()\n    \n    # Sort by cosine similarity\n    top_n = simScoresRDD.takeOrdered(n, key=lambda x: -x[1][0])\n    bottom_n = simScoresRDD.takeOrdered(n, key=lambda x: x[1][0])\n    \n    ############### (END) YOUR CODE ##############\n    # NOTE: I CHANGED THIS BECAUSE WE WERE SUPPOSED TO RETURN THE RDD!\n    return simScoresRDD, top_n, bottom_n"}, {"cell_type": "code", "execution_count": 114, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "d0eb2822-463a-462a-b71f-f5f84b340b45", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Wall time: 0.23660659790039062 seconds\n"}], "source": "# Autograder Skip\n\n# part e - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\nstart = time.time()\ntestResult, top_n, bottom_n = similarityAnalysis(testStripesRDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Wall time: 1.4768586158752441 seconds"}, {"cell_type": "code", "execution_count": 115, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "89790631-03c5-4fad-a582-ae8102ef6f9b", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 143:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 1.4229302406311035 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part e - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\nstart = time.time()\nf1Result, top_n, bottom_n = similarityAnalysis(f1StripesRDD, 10)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Wall time: 1.9845571517944336 seconds"}, {"cell_type": "code", "execution_count": 116, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "a770823e-2656-4db3-acf3-cdfc5901de42", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Pair                     |Cosine, Jaccard, Overlap, Dice \ncommentary - curious     |   1.0,     1.0,     1.0,   1.0\ncommentary - lady        |   1.0,     1.0,     1.0,   1.0\ncommentary - learn       |   1.0,     1.0,     1.0,   1.0\ncommentary - owe         |   1.0,     1.0,     1.0,   1.0\ncommentary - really      |   1.0,     1.0,     1.0,   1.0\ncommentary - reply       |   1.0,     1.0,     1.0,   1.0\ncommentary - toes        |   1.0,     1.0,     1.0,   1.0\ncommentary - tone        |   1.0,     1.0,     1.0,   1.0\ncurious - lady           |   1.0,     1.0,     1.0,   1.0\ncurious - learn          |   1.0,     1.0,     1.0,   1.0\n"}], "source": "displayOutput(top_n)"}, {"cell_type": "code", "execution_count": 117, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "d7c73eab-154b-49db-9e52-83c6c6a8a8ae", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Pair                     |Cosine, Jaccard, Overlap, Dice \npart - time              |0.0294,  0.0149,  0.0303, 0.0294\ntime - upon              |0.0314,  0.0159,  0.0345, 0.0312\ntime - two               |0.0314,  0.0159,  0.0345, 0.0312\nmade - time              |0.0325,  0.0164,   0.037, 0.0323\nfirst - time             |0.0338,  0.0169,    0.04, 0.0333\nnew - time               |0.0352,  0.0175,  0.0435, 0.0345\nlittle - part            |0.0355,  0.0179,  0.0417, 0.0351\npart - us                |0.0355,  0.0179,  0.0417, 0.0351\nmade - upon              |0.0357,  0.0182,   0.037, 0.0357\nmade - two               |0.0357,  0.0182,   0.037, 0.0357\n"}], "source": "displayOutput(bottom_n)"}, {"cell_type": "code", "execution_count": 118, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "98d0be65-e606-44ab-ac0b-2df0cd9dce3b", "showTitle": false, "title": ""}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 155:====================================================>(189 + 1) / 190]\r"}, {"name": "stdout", "output_type": "stream", "text": "Wall time: 796.1880712509155 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Autograder Skip\n\n# part e - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\nstart = time.time()\nresult, top_n, bottom_n = similarityAnalysis(stripesRDD, 20)\nprint(\"Wall time: {} seconds\".format(time.time() - start))\n# Command took 14 minutes -- May 30, 2022 on GC n1-std-4"}, {"cell_type": "code", "execution_count": 120, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "1e276d49-96f1-4f6a-8858-00d804989777", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Pair                     |Cosine, Jaccard, Overlap, Dice \nfirst - time             |  0.89,  0.8012,  0.9149, 0.8897\ntime - well              |0.8895,   0.801,   0.892, 0.8895\ngreat - time             | 0.875,  0.7757,   0.925, 0.8737\npart - well              | 0.874,  0.7755,  0.9018, 0.8735\nfirst - well             |0.8717,  0.7722,  0.8936, 0.8715\npart - time              |0.8715,  0.7715,  0.9018, 0.871\ntime - upon              |0.8668,   0.763,  0.9152, 0.8656\nmade - time              | 0.866,  0.7619,  0.9109, 0.8649\nmade - well              |0.8601,  0.7531,  0.9022, 0.8592\ntime - way               |0.8587,  0.7487,  0.9259, 0.8563\ngreat - well             |0.8526,  0.7412,  0.8988, 0.8514\ntime - two               |0.8517,  0.7389,  0.9094, 0.8498\nfirst - great            |0.8497,  0.7381,  0.8738, 0.8493\nfirst - part             |0.8471,  0.7348,  0.8527, 0.8471\ngreat - upon             |0.8464,  0.7338,  0.8475, 0.8464\nupon - well              |0.8444,   0.729,   0.889, 0.8433\nnew - time               |0.8426,   0.724,  0.9133, 0.8399\nfirst - two              |0.8411,  0.7249,  0.8737, 0.8405\nway - well               |0.8357,  0.7146,  0.8986, 0.8335\ntime - us                |0.8357,  0.7105,  0.9318, 0.8308\n"}], "source": "displayOutput(top_n)"}, {"cell_type": "code", "execution_count": 121, "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "c0d9efbd-b518-4bda-98a2-5bb5f1117f2b", "showTitle": false, "title": ""}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Pair                     |Cosine, Jaccard, Overlap, Dice \nregion - write           |0.0067,  0.0032,  0.0085, 0.0065\nrelation - snow          |0.0067,  0.0026,  0.0141, 0.0052\ncardiac - took           |0.0074,  0.0023,  0.0217, 0.0045\never - tumor             |0.0076,   0.002,  0.0263, 0.004\ncame - tumor             |0.0076,   0.002,  0.0263, 0.004\nlet - therapy            |0.0076,   0.003,  0.0161, 0.0059\nrelated - stay           |0.0078,  0.0036,  0.0116, 0.0072\nfactors - hear           |0.0078,  0.0039,  0.0094, 0.0077\nimplications - round     |0.0078,  0.0033,  0.0145, 0.0066\ncame - proteins          |0.0079,   0.002,  0.0286, 0.0041\npopulation - window      |0.0079,  0.0039,    0.01, 0.0077\nlove - proportional      | 0.008,  0.0029,  0.0185, 0.0058\ngot - multiple           | 0.008,  0.0034,  0.0149, 0.0067\nchanges - fort           |0.0081,  0.0032,  0.0161, 0.0065\nlayer - wife             |0.0081,  0.0038,  0.0119, 0.0075\nfive - sympathy          |0.0081,  0.0034,  0.0149, 0.0068\narrival - essential      |0.0081,   0.004,  0.0093, 0.008\ndesert - function        |0.0081,  0.0031,  0.0175, 0.0062\nfundamental - stood      |0.0081,  0.0038,  0.0115, 0.0077\npatients - plain         |0.0081,   0.004,  0.0103, 0.0079\n"}], "source": "displayOutput(bottom_n)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "09c78c64-6d5e-4ae0-b098-b7ac7d02830a", "showTitle": false, "title": ""}}, "source": "__Expected output f1RDD:__  \n<table>\n<th>MOST SIMILAR:</th>\n<th>LEAST SIMILAR:</th>\n<tr><td><pre>\nPair                     |Cosine, Jaccard, Overlap, Dice \ncommentary - lady        |   1.0,     1.0,     1.0,   1.0\ncommentary - toes        |   1.0,     1.0,     1.0,   1.0\ncommentary - reply       |   1.0,     1.0,     1.0,   1.0\ncurious - tone           |   1.0,     1.0,     1.0,   1.0\ncurious - lady           |   1.0,     1.0,     1.0,   1.0\ncurious - owe            |   1.0,     1.0,     1.0,   1.0\nlady - tone              |   1.0,     1.0,     1.0,   1.0\nreply - tone             |   1.0,     1.0,     1.0,   1.0\nlady - toes              |   1.0,     1.0,     1.0,   1.0\nlady - reply             |   1.0,     1.0,     1.0,   1.0\n</pre></td>\n<td><pre>\n\nPair                     |Cosine, Jaccard, Overlap, Dice \npart - time              |0.0294,  0.0149,  0.0303, 0.0294\ntime - upon              |0.0314,  0.0159,  0.0345, 0.0312\ntime - two               |0.0314,  0.0159,  0.0345, 0.0312\nmade - time              |0.0325,  0.0164,   0.037, 0.0323\nfirst - time             |0.0338,  0.0169,    0.04, 0.0333\nnew - time               |0.0352,  0.0175,  0.0435, 0.0345\npart - us                |0.0355,  0.0179,  0.0417, 0.0351\nlittle - part            |0.0355,  0.0179,  0.0417, 0.0351\nmade - two               |0.0357,  0.0182,   0.037, 0.0357\nmade - upon              |0.0357,  0.0182,   0.037, 0.0357\n</pre></td></tr>\n</table>\n\n__Expected output dataRDD:__  \n<table>\n<th>Most Similar</th>\n<th>Least Similar</th>\n<tr><td><pre>\nPair                     |Cosine, Jaccard, Overlap, Dice \nfirst - time             |  0.89,  0.8012,  0.9149, 0.8897\ntime - well              |0.8895,   0.801,   0.892, 0.8895\ngreat - time             | 0.875,  0.7757,   0.925, 0.8737\npart - well              | 0.874,  0.7755,  0.9018, 0.8735\nfirst - well             |0.8717,  0.7722,  0.8936, 0.8715\npart - time              |0.8715,  0.7715,  0.9018, 0.871\ntime - upon              |0.8668,   0.763,  0.9152, 0.8656\nmade - time              | 0.866,  0.7619,  0.9109, 0.8649\nmade - well              |0.8601,  0.7531,  0.9022, 0.8592\ntime - way               |0.8587,  0.7487,  0.9259, 0.8563\ngreat - well             |0.8526,  0.7412,  0.8988, 0.8514\ntime - two               |0.8517,  0.7389,  0.9094, 0.8498\nfirst - great            |0.8497,  0.7381,  0.8738, 0.8493\nfirst - part             |0.8471,  0.7348,  0.8527, 0.8471\ngreat - upon             |0.8464,  0.7338,  0.8475, 0.8464\nupon - well              |0.8444,   0.729,   0.889, 0.8433\nnew - time               |0.8426,   0.724,  0.9133, 0.8399\nfirst - two              |0.8411,  0.7249,  0.8737, 0.8405\nway - well               |0.8357,  0.7146,  0.8986, 0.8335\ntime - us                |0.8357,  0.7105,  0.9318, 0.8308\n\n</pre></td>\n<td><pre>\nPair                     |Cosine, Jaccard, Overlap, Dice \nregion - write           |0.0067,  0.0032,  0.0085, 0.0065\nrelation - snow          |0.0067,  0.0026,  0.0141, 0.0052\ncardiac - took           |0.0074,  0.0023,  0.0217, 0.0045\never - tumor             |0.0076,   0.002,  0.0263, 0.004\ncame - tumor             |0.0076,   0.002,  0.0263, 0.004\nlet - therapy            |0.0076,   0.003,  0.0161, 0.0059\nrelated - stay           |0.0078,  0.0036,  0.0116, 0.0072\nfactors - hear           |0.0078,  0.0039,  0.0094, 0.0077\nimplications - round     |0.0078,  0.0033,  0.0145, 0.0066\ncame - proteins          |0.0079,   0.002,  0.0286, 0.0041\npopulation - window      |0.0079,  0.0039,    0.01, 0.0077\nlove - proportional      | 0.008,  0.0029,  0.0185, 0.0058\ngot - multiple           | 0.008,  0.0034,  0.0149, 0.0067\nchanges - fort           |0.0081,  0.0032,  0.0161, 0.0065\nlayer - wife             |0.0081,  0.0038,  0.0119, 0.0075\nfive - sympathy          |0.0081,  0.0034,  0.0149, 0.0068\narrival - essential      |0.0081,   0.004,  0.0093, 0.008\ndesert - function        |0.0081,  0.0031,  0.0175, 0.0062\nfundamental - stood      |0.0081,  0.0038,  0.0115, 0.0077\npatients - plain         |0.0081,   0.004,  0.0103, 0.0079\n</pre></td></tr>\n</table>"}, {"cell_type": "code", "execution_count": 122, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\nOverall, I think the quality of the \"synonyms\" is fine but there are odd things about the pairs of words. The most\nsimilar word pairs like first-time, time-well, great-time, and part-time are not true synonyms but rather words\nthat frequently appear together in common phrases. They have high similarity because they share many contextual\nfeatures in the basis vocabulary, not because they have similar meanings. For example, first and time appear\ntogether in phrases like first time, and both co-occur with similar prepositions and articles in the basis\nvocabulary. One odd pattern is that many high-scoring pairs are function words or very common words that share\ngeneric contexts rather than semantic similarity. The pairs seem to demonstrate co-occurrence patterns and\ngrammatical relationships more than true synonymy. Words like time, first, made, and part take the top results\nbecause they are versatile words appearing in many ways. To improve the analysis, we could implement several\nstrategies. First, we could use more sophisticated weighting like TF-IDF or PMI to downweight common co-occurrences\nand emphasize distinctive ones. Next, we could expand the basis vocabulary to include more mid-frequency content\nwords rather than just the 9000-10000 ranked words, which may still be too common. Also, we could filter out\ngrammatically related pairs by using part-of-speech tagging to only compare words of the same type. Finally,\nwe could apply dimensionality reduction techniques to create dense embeddings that better capture semantic\nrelationships beyond simple co-occurrence counts.\n\n"}], "source": "# q8f\n### SHORT RESPONSE\n### QUESTION: Comment on the quality of the \"synonyms\" your analysis comes up with. Do you notice anything\n#             odd about these pairs of words? Discuss at least one idea for how you might go about improving\n#             on the analysis.\n\n### ENTER ANSWER INSIDE THE PRINT STATEMENT.\n\nprint(\n\"\"\"\nOverall, I think the quality of the \"synonyms\" is fine but there are odd things about the pairs of words. The most\nsimilar word pairs like first-time, time-well, great-time, and part-time are not true synonyms but rather words\nthat frequently appear together in common phrases. They have high similarity because they share many contextual\nfeatures in the basis vocabulary, not because they have similar meanings. For example, first and time appear\ntogether in phrases like first time, and both co-occur with similar prepositions and articles in the basis\nvocabulary. One odd pattern is that many high-scoring pairs are function words or very common words that share\ngeneric contexts rather than semantic similarity. The pairs seem to demonstrate co-occurrence patterns and\ngrammatical relationships more than true synonymy. Words like time, first, made, and part take the top results\nbecause they are versatile words appearing in many ways. To improve the analysis, we could implement several\nstrategies. First, we could use more sophisticated weighting like TF-IDF or PMI to downweight common co-occurrences\nand emphasize distinctive ones. Next, we could expand the basis vocabulary to include more mid-frequency content\nwords rather than just the 9000-10000 ranked words, which may still be too common. Also, we could filter out\ngrammatically related pairs by using part-of-speech tagging to only compare words of the same type. Finally,\nwe could apply dimensionality reduction techniques to create dense embeddings that better capture semantic\nrelationships beyond simple co-occurrence counts.\n\"\"\"\n)"}, {"cell_type": "markdown", "metadata": {"application/vnd.databricks.v1+cell": {"inputWidgets": {}, "nuid": "63c40693-e756-4efd-985a-7db35c58af83", "showTitle": false, "title": ""}}, "source": "### Congratulations, you have completed HW3! "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"application/vnd.databricks.v1+notebook": {"dashboards": [], "language": "python", "notebookName": "hw3_Workbook_revF20", "notebookOrigID": 2162291293013809, "widgets": {}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}, "toc-autonumbering": true}, "nbformat": 4, "nbformat_minor": 4}