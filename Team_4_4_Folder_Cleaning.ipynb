{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2eaff9a-b0e2-4a13-8028-9d6fc16542be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, mean, lit, first, desc, isnan, sum, avg, countDistinct, lit, unix_timestamp, to_timestamp, datediff\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3162b1e7-0910-4bd5-b4bc-f4d621c37ba6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764479862121}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/student-groups/Group_4_4/2015_final_feature_engineered_data_with_dep_delay/</td><td>2015_final_feature_engineered_data_with_dep_delay/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/</td><td>CSVs_5Y/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/Charts/</td><td>Charts/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/Charts_5Y/</td><td>Charts_5Y/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/JOINED_1Y_2015.parquet/</td><td>JOINED_1Y_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/JOINED_1Y_2019.parquet/</td><td>JOINED_1Y_2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/JOINED_3M_2015.parquet/</td><td>JOINED_3M_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/JOINED_5Y_2015_2019.parquet/</td><td>JOINED_5Y_2015_2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/JOINED_FUTURE.parquet/</td><td>JOINED_FUTURE.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/airport-zones.csv</td><td>airport-zones.csv</td><td>1067908</td><td>1765735854000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/appendix_b4_column_classification_2015-2019.csv</td><td>appendix_b4_column_classification_2015-2019.csv</td><td>14739</td><td>1764800578000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/best_model.pth</td><td>best_model.pth</td><td>2714274</td><td>1765675487000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/best_model_single.pth</td><td>best_model_single.pth</td><td>332292</td><td>1765745896000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint1_missing_analysis.png</td><td>checkpoint1_missing_analysis.png</td><td>630176</td><td>1764723871000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint4_analysis_report_2015-2019.txt</td><td>checkpoint4_analysis_report_2015-2019.txt</td><td>4542</td><td>1764800578000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_1_initial_joined_2015.parquet/</td><td>checkpoint_1_initial_joined_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_1_initial_joined_2019.parquet/</td><td>checkpoint_1_initial_joined_2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_1_initial_joined_5Y_2015-2019.parquet/</td><td>checkpoint_1_initial_joined_5Y_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_2_cleaned_imputed_2015-2019.parquet/</td><td>checkpoint_2_cleaned_imputed_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_2_cleaned_imputed_2015.parquet/</td><td>checkpoint_2_cleaned_imputed_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_3_basic_features_2015-2019.parquet/</td><td>checkpoint_3_basic_features_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_3_basic_features_2015.parquet/</td><td>checkpoint_3_basic_features_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_3_basic_features_2019.parquet/</td><td>checkpoint_3_basic_features_2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_3a_before_pagerank_2015-2019.parquet/</td><td>checkpoint_3a_before_pagerank_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_4_advanced_features_2015-2019.parquet/</td><td>checkpoint_4_advanced_features_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_4_advanced_features_2015.parquet/</td><td>checkpoint_4_advanced_features_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015-2019.parquet/</td><td>checkpoint_5_final_clean_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015-2019_refined.parquet/</td><td>checkpoint_5_final_clean_2015-2019_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015.parquet/</td><td>checkpoint_5_final_clean_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_5a_comprehensive_all_features_2015-2019.parquet/</td><td>checkpoint_5a_comprehensive_all_features_2015-2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_5a_final_data_with_all_features_2015.parquet/</td><td>checkpoint_5a_final_data_with_all_features_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_6_regression_baseline_2015_2019.parquet/</td><td>checkpoint_6_regression_baseline_2015_2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_6_regression_features_baseline_2015.parquet/</td><td>checkpoint_6_regression_features_baseline_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_6_regression_test_2019.parquet/</td><td>checkpoint_6_regression_test_2019.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_6_regression_train_2015_2018.parquet/</td><td>checkpoint_6_regression_train_2015_2018.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoint_7_regression_features_post_baseline_2015.parquet/</td><td>checkpoint_7_regression_features_post_baseline_2015.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/checkpoints/</td><td>checkpoints/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cnn/</td><td>cnn/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cnn_pytorch_results.json</td><td>cnn_pytorch_results.json</td><td>5727</td><td>1765676171000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_test_2019_base.parquet/</td><td>cp6_test_2019_base.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_test_2019_refined.parquet/</td><td>cp6_test_2019_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_train_2015-2018_undersampled_0_5_ratio.parquet/</td><td>cp6_train_2015-2018_undersampled_0_5_ratio.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_train_2015-2018_weighted.parquet/</td><td>cp6_train_2015-2018_weighted.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_train_2015_2017_refined.parquet/</td><td>cp6_train_2015_2017_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_train_2015_2017_undersampled_0_5_ratio.parquet/</td><td>cp6_train_2015_2017_undersampled_0_5_ratio.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_train_2015_2017_weighted.parquet/</td><td>cp6_train_2015_2017_weighted.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_train_universe_2015-2018_base.parquet/</td><td>cp6_train_universe_2015-2018_base.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/cp6_val_2018_refined.parquet/</td><td>cp6_val_2018_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/csvs_1Y/</td><td>csvs_1Y/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/data_12M/</td><td>data_12M/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/df_otpw_12M_clean.parquet/</td><td>df_otpw_12M_clean.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/df_otpw_3m.parquet/</td><td>df_otpw_3m.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/df_weather.parquet/</td><td>df_weather.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/downsampled_5_pct/</td><td>downsampled_5_pct/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/downsampled_5pct_csv/</td><td>downsampled_5pct_csv/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/feature_views/</td><td>feature_views/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/final_feature_engineered_data_with_dep_delay/</td><td>final_feature_engineered_data_with_dep_delay/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/future_joins/</td><td>future_joins/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/joined_3m_clean_imputed.parquet/</td><td>joined_3m_clean_imputed.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/joined_3m_final_clean.parquet/</td><td>joined_3m_final_clean.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/mds_data/</td><td>mds_data/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/mlp/</td><td>mlp/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/models/</td><td>models/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/regression_predictions.parquet/</td><td>regression_predictions.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/regression_predictions_partitioned/</td><td>regression_predictions_partitioned/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/regression_results/</td><td>regression_results/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/temp_join_result.parquet/</td><td>temp_join_result.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/test_data_regression_scaled_2015/</td><td>test_data_regression_scaled_2015/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/tfrecords/</td><td>tfrecords/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/timeseries/</td><td>timeseries/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/tiny_cnn_pytorch_data/</td><td>tiny_cnn_pytorch_data/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/tiny_cp6_test_2019_refined.parquet/</td><td>tiny_cp6_test_2019_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/tiny_cp6_train_2015_2017_refined.parquet/</td><td>tiny_cp6_train_2015_2017_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/tiny_cp6_val_2018_refined.parquet/</td><td>tiny_cp6_val_2018_refined.parquet/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/train_data_regression_scaled_2015/</td><td>train_data_regression_scaled_2015/</td><td>0</td><td>1765750287352</td></tr><tr><td>dbfs:/student-groups/Group_4_4/training_history.json</td><td>training_history.json</td><td>11753</td><td>1765676033000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/training_history_single.json</td><td>training_history_single.json</td><td>445</td><td>1765749065000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/undersampled_cp6_train_2015_2017_refined.parquet/</td><td>undersampled_cp6_train_2015_2017_refined.parquet/</td><td>0</td><td>1765750287352</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/student-groups/Group_4_4/2015_final_feature_engineered_data_with_dep_delay/",
         "2015_final_feature_engineered_data_with_dep_delay/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/",
         "CSVs_5Y/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/Charts/",
         "Charts/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/Charts_5Y/",
         "Charts_5Y/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/JOINED_1Y_2015.parquet/",
         "JOINED_1Y_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/JOINED_1Y_2019.parquet/",
         "JOINED_1Y_2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/JOINED_3M_2015.parquet/",
         "JOINED_3M_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/JOINED_5Y_2015_2019.parquet/",
         "JOINED_5Y_2015_2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/JOINED_FUTURE.parquet/",
         "JOINED_FUTURE.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/airport-zones.csv",
         "airport-zones.csv",
         1067908,
         1765735854000
        ],
        [
         "dbfs:/student-groups/Group_4_4/appendix_b4_column_classification_2015-2019.csv",
         "appendix_b4_column_classification_2015-2019.csv",
         14739,
         1764800578000
        ],
        [
         "dbfs:/student-groups/Group_4_4/best_model.pth",
         "best_model.pth",
         2714274,
         1765675487000
        ],
        [
         "dbfs:/student-groups/Group_4_4/best_model_single.pth",
         "best_model_single.pth",
         332292,
         1765745896000
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint1_missing_analysis.png",
         "checkpoint1_missing_analysis.png",
         630176,
         1764723871000
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint4_analysis_report_2015-2019.txt",
         "checkpoint4_analysis_report_2015-2019.txt",
         4542,
         1764800578000
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_1_initial_joined_2015.parquet/",
         "checkpoint_1_initial_joined_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_1_initial_joined_2019.parquet/",
         "checkpoint_1_initial_joined_2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_1_initial_joined_5Y_2015-2019.parquet/",
         "checkpoint_1_initial_joined_5Y_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_2_cleaned_imputed_2015-2019.parquet/",
         "checkpoint_2_cleaned_imputed_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_2_cleaned_imputed_2015.parquet/",
         "checkpoint_2_cleaned_imputed_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_3_basic_features_2015-2019.parquet/",
         "checkpoint_3_basic_features_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_3_basic_features_2015.parquet/",
         "checkpoint_3_basic_features_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_3_basic_features_2019.parquet/",
         "checkpoint_3_basic_features_2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_3a_before_pagerank_2015-2019.parquet/",
         "checkpoint_3a_before_pagerank_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_4_advanced_features_2015-2019.parquet/",
         "checkpoint_4_advanced_features_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_4_advanced_features_2015.parquet/",
         "checkpoint_4_advanced_features_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015-2019.parquet/",
         "checkpoint_5_final_clean_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015-2019_refined.parquet/",
         "checkpoint_5_final_clean_2015-2019_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015.parquet/",
         "checkpoint_5_final_clean_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_5a_comprehensive_all_features_2015-2019.parquet/",
         "checkpoint_5a_comprehensive_all_features_2015-2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_5a_final_data_with_all_features_2015.parquet/",
         "checkpoint_5a_final_data_with_all_features_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_6_regression_baseline_2015_2019.parquet/",
         "checkpoint_6_regression_baseline_2015_2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_6_regression_features_baseline_2015.parquet/",
         "checkpoint_6_regression_features_baseline_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_6_regression_test_2019.parquet/",
         "checkpoint_6_regression_test_2019.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_6_regression_train_2015_2018.parquet/",
         "checkpoint_6_regression_train_2015_2018.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoint_7_regression_features_post_baseline_2015.parquet/",
         "checkpoint_7_regression_features_post_baseline_2015.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/checkpoints/",
         "checkpoints/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cnn/",
         "cnn/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cnn_pytorch_results.json",
         "cnn_pytorch_results.json",
         5727,
         1765676171000
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_test_2019_base.parquet/",
         "cp6_test_2019_base.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_test_2019_refined.parquet/",
         "cp6_test_2019_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_train_2015-2018_undersampled_0_5_ratio.parquet/",
         "cp6_train_2015-2018_undersampled_0_5_ratio.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_train_2015-2018_weighted.parquet/",
         "cp6_train_2015-2018_weighted.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_train_2015_2017_refined.parquet/",
         "cp6_train_2015_2017_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_train_2015_2017_undersampled_0_5_ratio.parquet/",
         "cp6_train_2015_2017_undersampled_0_5_ratio.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_train_2015_2017_weighted.parquet/",
         "cp6_train_2015_2017_weighted.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_train_universe_2015-2018_base.parquet/",
         "cp6_train_universe_2015-2018_base.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/cp6_val_2018_refined.parquet/",
         "cp6_val_2018_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/csvs_1Y/",
         "csvs_1Y/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/data_12M/",
         "data_12M/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/df_otpw_12M_clean.parquet/",
         "df_otpw_12M_clean.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/df_otpw_3m.parquet/",
         "df_otpw_3m.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/df_weather.parquet/",
         "df_weather.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/downsampled_5_pct/",
         "downsampled_5_pct/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/downsampled_5pct_csv/",
         "downsampled_5pct_csv/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/feature_views/",
         "feature_views/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/final_feature_engineered_data_with_dep_delay/",
         "final_feature_engineered_data_with_dep_delay/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/future_joins/",
         "future_joins/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/joined_3m_clean_imputed.parquet/",
         "joined_3m_clean_imputed.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/joined_3m_final_clean.parquet/",
         "joined_3m_final_clean.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/mds_data/",
         "mds_data/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/mlp/",
         "mlp/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/models/",
         "models/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/regression_predictions.parquet/",
         "regression_predictions.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/regression_predictions_partitioned/",
         "regression_predictions_partitioned/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/regression_results/",
         "regression_results/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/temp_join_result.parquet/",
         "temp_join_result.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/test_data_regression_scaled_2015/",
         "test_data_regression_scaled_2015/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/tfrecords/",
         "tfrecords/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/timeseries/",
         "timeseries/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/tiny_cnn_pytorch_data/",
         "tiny_cnn_pytorch_data/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/tiny_cp6_test_2019_refined.parquet/",
         "tiny_cp6_test_2019_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/tiny_cp6_train_2015_2017_refined.parquet/",
         "tiny_cp6_train_2015_2017_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/tiny_cp6_val_2018_refined.parquet/",
         "tiny_cp6_val_2018_refined.parquet/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/train_data_regression_scaled_2015/",
         "train_data_regression_scaled_2015/",
         0,
         1765750287352
        ],
        [
         "dbfs:/student-groups/Group_4_4/training_history.json",
         "training_history.json",
         11753,
         1765676033000
        ],
        [
         "dbfs:/student-groups/Group_4_4/training_history_single.json",
         "training_history_single.json",
         445,
         1765749065000
        ],
        [
         "dbfs:/student-groups/Group_4_4/undersampled_cp6_train_2015_2017_refined.parquet/",
         "undersampled_cp6_train_2015_2017_refined.parquet/",
         0,
         1765750287352
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_BASE_DIR = \"dbfs:/student-groups/Group_4_4/\"\n",
    "display(dbutils.fs.ls(f\"{joined_BASE_DIR}/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d14a8b-ba4e-4a5f-a650-35bee242c558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/airport_performance_top30.csv</td><td>airport_performance_top30.csv</td><td>1707</td><td>1764814397000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/appendix_b1_column_classification.csv</td><td>appendix_b1_column_classification.csv</td><td>8458</td><td>1764725266000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/appendix_b5_column_classification_2015-2019.csv</td><td>appendix_b5_column_classification_2015-2019.csv</td><td>18125</td><td>1764809193000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/appendix_b5a_column_classification_2015-2019.csv</td><td>appendix_b5a_column_classification_2015-2019.csv</td><td>6645</td><td>1765376813000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/carrier_performance_metrics.csv</td><td>carrier_performance_metrics.csv</td><td>2297</td><td>1764814392000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint1_analysis_report.txt</td><td>checkpoint1_analysis_report.txt</td><td>1960</td><td>1764725266000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint3_analysis_report.txt</td><td>checkpoint3_analysis_report.txt</td><td>2774</td><td>1764730597000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint3_new_features_list.txt</td><td>checkpoint3_new_features_list.txt</td><td>2134</td><td>1764730597000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint5_analysis_report.txt</td><td>checkpoint5_analysis_report.txt</td><td>3795</td><td>1764809193000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_5a_comprehensive_categorization.csv</td><td>checkpoint_5a_comprehensive_categorization.csv</td><td>146</td><td>1764828111000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_5a_feature_sources.csv</td><td>checkpoint_5a_feature_sources.csv</td><td>4470</td><td>1764828111000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_5a_skipped_duplicates.csv</td><td>checkpoint_5a_skipped_duplicates.csv</td><td>1500</td><td>1764828111000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_summary.txt</td><td>checkpoint_summary.txt</td><td>4129</td><td>1764810977000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/comprehensive_data_dictionary_2015-2019.csv</td><td>comprehensive_data_dictionary_2015-2019.csv</td><td>16772</td><td>1764815665000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/dataset_sizes_summary_2015-2019.csv</td><td>dataset_sizes_summary_2015-2019.csv</td><td>623</td><td>1764815667000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/day_of_week_analysis.csv</td><td>day_of_week_analysis.csv</td><td>369</td><td>1764814382000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_encoding_inventory_2015-2019.txt</td><td>feature_encoding_inventory_2015-2019.txt</td><td>6290</td><td>1764808081000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_engineering_summary.csv</td><td>feature_engineering_summary.csv</td><td>388</td><td>1764813896000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_evolution_summary.csv</td><td>feature_evolution_summary.csv</td><td>589</td><td>1764813895000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_families_summary_2015-2019.csv</td><td>feature_families_summary_2015-2019.csv</td><td>840</td><td>1764815667000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_lists_for_modeling.txt</td><td>feature_lists_for_modeling.txt</td><td>7382</td><td>1764808081000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_transformations_2015-2019.csv</td><td>feature_transformations_2015-2019.csv</td><td>2335</td><td>1764815668000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/final_comprehensive_report.txt</td><td>final_comprehensive_report.txt</td><td>8117</td><td>1764813900000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/final_dataset_profile.csv</td><td>final_dataset_profile.csv</td><td>797</td><td>1764813896000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/final_feature_summary_2015-2019.csv</td><td>final_feature_summary_2015-2019.csv</td><td>1119</td><td>1764815668000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/final_verification_summary.txt</td><td>final_verification_summary.txt</td><td>1365</td><td>1764805430000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/hourly_delay_analysis.csv</td><td>hourly_delay_analysis.csv</td><td>897</td><td>1764814382000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_analysis_report.txt</td><td>missing_data_analysis_report.txt</td><td>5913</td><td>1765374513000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_comparison.csv</td><td>missing_data_comparison.csv</td><td>6925</td><td>1764805046000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_reduction_summary.csv</td><td>missing_data_reduction_summary.csv</td><td>498</td><td>1764813895000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_stages_summary.csv</td><td>missing_data_stages_summary.csv</td><td>655</td><td>1765374513000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/non_numerical_feature_handling_2015-2019.csv</td><td>non_numerical_feature_handling_2015-2019.csv</td><td>1669</td><td>1764815668000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/quality_metrics_summary.csv</td><td>quality_metrics_summary.csv</td><td>458</td><td>1764813895000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/quarterly_delay_analysis.csv</td><td>quarterly_delay_analysis.csv</td><td>195</td><td>1764814382000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/raw_vs_derived_features_2015-2019.csv</td><td>raw_vs_derived_features_2015-2019.csv</td><td>275</td><td>1764815668000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/state_level_analysis.csv</td><td>state_level_analysis.csv</td><td>1890</td><td>1764814397000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/table_descriptions_summary.csv</td><td>table_descriptions_summary.csv</td><td>2406</td><td>1764815664000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/target_distribution_summary.csv</td><td>target_distribution_summary.csv</td><td>380</td><td>1764813895000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/temporal_integrity_verification_report.txt</td><td>temporal_integrity_verification_report.txt</td><td>2928</td><td>1764806972000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/temporal_verification_results.csv</td><td>temporal_verification_results.csv</td><td>451</td><td>1764806972000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/top_predictors_summary.csv</td><td>top_predictors_summary.csv</td><td>828</td><td>1764813896000</td></tr><tr><td>dbfs:/student-groups/Group_4_4/CSVs_5Y/yearly_delay_analysis.csv</td><td>yearly_delay_analysis.csv</td><td>241</td><td>1764814382000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/airport_performance_top30.csv",
         "airport_performance_top30.csv",
         1707,
         1764814397000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/appendix_b1_column_classification.csv",
         "appendix_b1_column_classification.csv",
         8458,
         1764725266000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/appendix_b5_column_classification_2015-2019.csv",
         "appendix_b5_column_classification_2015-2019.csv",
         18125,
         1764809193000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/appendix_b5a_column_classification_2015-2019.csv",
         "appendix_b5a_column_classification_2015-2019.csv",
         6645,
         1765376813000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/carrier_performance_metrics.csv",
         "carrier_performance_metrics.csv",
         2297,
         1764814392000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint1_analysis_report.txt",
         "checkpoint1_analysis_report.txt",
         1960,
         1764725266000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint3_analysis_report.txt",
         "checkpoint3_analysis_report.txt",
         2774,
         1764730597000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint3_new_features_list.txt",
         "checkpoint3_new_features_list.txt",
         2134,
         1764730597000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint5_analysis_report.txt",
         "checkpoint5_analysis_report.txt",
         3795,
         1764809193000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_5a_comprehensive_categorization.csv",
         "checkpoint_5a_comprehensive_categorization.csv",
         146,
         1764828111000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_5a_feature_sources.csv",
         "checkpoint_5a_feature_sources.csv",
         4470,
         1764828111000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_5a_skipped_duplicates.csv",
         "checkpoint_5a_skipped_duplicates.csv",
         1500,
         1764828111000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/checkpoint_summary.txt",
         "checkpoint_summary.txt",
         4129,
         1764810977000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/comprehensive_data_dictionary_2015-2019.csv",
         "comprehensive_data_dictionary_2015-2019.csv",
         16772,
         1764815665000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/dataset_sizes_summary_2015-2019.csv",
         "dataset_sizes_summary_2015-2019.csv",
         623,
         1764815667000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/day_of_week_analysis.csv",
         "day_of_week_analysis.csv",
         369,
         1764814382000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_encoding_inventory_2015-2019.txt",
         "feature_encoding_inventory_2015-2019.txt",
         6290,
         1764808081000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_engineering_summary.csv",
         "feature_engineering_summary.csv",
         388,
         1764813896000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_evolution_summary.csv",
         "feature_evolution_summary.csv",
         589,
         1764813895000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_families_summary_2015-2019.csv",
         "feature_families_summary_2015-2019.csv",
         840,
         1764815667000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_lists_for_modeling.txt",
         "feature_lists_for_modeling.txt",
         7382,
         1764808081000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/feature_transformations_2015-2019.csv",
         "feature_transformations_2015-2019.csv",
         2335,
         1764815668000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/final_comprehensive_report.txt",
         "final_comprehensive_report.txt",
         8117,
         1764813900000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/final_dataset_profile.csv",
         "final_dataset_profile.csv",
         797,
         1764813896000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/final_feature_summary_2015-2019.csv",
         "final_feature_summary_2015-2019.csv",
         1119,
         1764815668000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/final_verification_summary.txt",
         "final_verification_summary.txt",
         1365,
         1764805430000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/hourly_delay_analysis.csv",
         "hourly_delay_analysis.csv",
         897,
         1764814382000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_analysis_report.txt",
         "missing_data_analysis_report.txt",
         5913,
         1765374513000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_comparison.csv",
         "missing_data_comparison.csv",
         6925,
         1764805046000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_reduction_summary.csv",
         "missing_data_reduction_summary.csv",
         498,
         1764813895000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/missing_data_stages_summary.csv",
         "missing_data_stages_summary.csv",
         655,
         1765374513000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/non_numerical_feature_handling_2015-2019.csv",
         "non_numerical_feature_handling_2015-2019.csv",
         1669,
         1764815668000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/quality_metrics_summary.csv",
         "quality_metrics_summary.csv",
         458,
         1764813895000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/quarterly_delay_analysis.csv",
         "quarterly_delay_analysis.csv",
         195,
         1764814382000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/raw_vs_derived_features_2015-2019.csv",
         "raw_vs_derived_features_2015-2019.csv",
         275,
         1764815668000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/state_level_analysis.csv",
         "state_level_analysis.csv",
         1890,
         1764814397000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/table_descriptions_summary.csv",
         "table_descriptions_summary.csv",
         2406,
         1764815664000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/target_distribution_summary.csv",
         "target_distribution_summary.csv",
         380,
         1764813895000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/temporal_integrity_verification_report.txt",
         "temporal_integrity_verification_report.txt",
         2928,
         1764806972000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/temporal_verification_results.csv",
         "temporal_verification_results.csv",
         451,
         1764806972000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/top_predictors_summary.csv",
         "top_predictors_summary.csv",
         828,
         1764813896000
        ],
        [
         "dbfs:/student-groups/Group_4_4/CSVs_5Y/yearly_delay_analysis.csv",
         "yearly_delay_analysis.csv",
         241,
         1764814382000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_BASE_DIR = \"dbfs:/student-groups/Group_4_4/CSVs_5Y/\"\n",
    "display(dbutils.fs.ls(f\"{joined_BASE_DIR}/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c184e3-2d20-4a86-9da7-9c1fa4f6477e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nUNDERSTANDING MODIFICATION TIME\n================================================================================\n\nOriginal value: 1764483306738\nMilliseconds since epoch (Jan 1, 1970)\n\nConverted to readable date:\n  Full: 2025-11-30 06:15:06.738000\n  Date: 2025-11-30\n  Date & Time: 2025-11-30 06:15:06\n  Friendly: November 30, 2025 at 06:15:06 AM\n\n================================================================================\nCODE TO SHOW READABLE DATES IN FILE LISTINGS\n================================================================================\n\n# Function to convert modification time\nfrom datetime import datetime\n\ndef format_timestamp(timestamp_ms):\n    \"\"\"Convert Unix timestamp in milliseconds to readable format\"\"\"\n    timestamp_sec = timestamp_ms / 1000\n    dt = datetime.fromtimestamp(timestamp_sec)\n    return dt.strftime('%Y-%m-%d %H:%M:%S')\n\n# Use in your file listings\nfiles = dbutils.fs.ls(\"dbfs:/student-groups/Group_4_4\")\n\nfor f in files:\n    readable_date = format_timestamp(f.modificationTime)\n    print(f\"{f.name:60s} {readable_date}\")\n\n\n================================================================================\nEXAMPLE: LIST FILES WITH READABLE DATES\n================================================================================\n\nFiles in dbfs:/student-groups/Group_4_4:\n\nName                                                         Modified             Size      \n------------------------------------------------------------------------------------------\njoined_3m_clean_imputed.parquet/                             2025-11-30 06:25:11  DIR       \njoined_3m_final_clean.parquet/                               2025-11-30 06:25:11  DIR       \nmart/                                                        2025-11-30 06:25:11  DIR       \nregression_results/                                          2025-11-30 06:25:11  DIR       \ntest_data_regression_scaled_2015/                            2025-11-30 06:25:11  DIR       \ntimeseries/                                                  2025-11-30 06:25:11  DIR       \ntrain_data_regression_scaled_2015/                           2025-11-30 06:25:11  DIR       \n2015_final_feature_engineered_data_with_dep_delay/           2025-11-30 06:25:11  DIR       \nCharts/                                                      2025-11-30 06:25:11  DIR       \nJOINED_1Y.parquet/                                           2025-11-30 06:25:11  DIR       \nJOINED_1Y_2015.parquet/                                      2025-11-30 06:25:11  DIR       \nJOINED_1Y_2019.parquet/                                      2025-11-30 06:25:11  DIR       \nJOINED_3M.parquet/                                           2025-11-30 06:25:11  DIR       \nJOINED_3M_2015.parquet/                                      2025-11-30 06:25:11  DIR       \nJOINED_5Y_2015_2019.parquet/                                 2025-11-30 06:25:11  DIR       \ncheckpoint_1_initial_joined_2015.parquet/                    2025-11-30 06:25:11  DIR       \ncheckpoint_1_initial_joined_2019.parquet/                    2025-11-30 06:25:11  DIR       \ncheckpoint_2_cleaned_imputed_2015.parquet/                   2025-11-30 06:25:11  DIR       \ncheckpoint_3_basic_features_2015.parquet/                    2025-11-30 06:25:11  DIR       \ncheckpoint_3_basic_features_2019.parquet/                    2025-11-30 06:25:11  DIR       \n\n... and 13 more files\n\n================================================================================\nFILES GROUPED BY MODIFICATION DATE\n================================================================================\n\n\uD83D\uDCC5 2025-11-30 (33 items)\n--------------------------------------------------------------------------------\n  06:25:11 - 2015_final_feature_engineered_data_with_dep_delay/\n  06:25:11 - Charts/\n  06:25:11 - JOINED_1Y.parquet/\n  06:25:11 - JOINED_1Y_2015.parquet/\n  06:25:11 - JOINED_1Y_2019.parquet/\n  06:25:11 - JOINED_3M.parquet/\n  06:25:11 - JOINED_3M_2015.parquet/\n  06:25:11 - JOINED_5Y_2015_2019.parquet/\n  06:25:11 - checkpoint_1_initial_joined_2015.parquet/\n  06:25:11 - checkpoint_1_initial_joined_2019.parquet/\n  ... and 23 more files\n\n================================================================================\n TIMESTAMP CONVERSION COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONVERT MODIFICATION TIME TO READABLE FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Example modification time from your data\n",
    "modification_time_ms = 1764483306738\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"UNDERSTANDING MODIFICATION TIME\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Convert to readable date\n",
    "# ============================================================================\n",
    "\n",
    "# Convert milliseconds to seconds\n",
    "modification_time_sec = modification_time_ms / 1000\n",
    "\n",
    "# Convert to datetime object\n",
    "dt = datetime.fromtimestamp(modification_time_sec)\n",
    "\n",
    "print(f\"\\nOriginal value: {modification_time_ms}\")\n",
    "print(f\"Milliseconds since epoch (Jan 1, 1970)\")\n",
    "print()\n",
    "print(f\"Converted to readable date:\")\n",
    "print(f\"  Full: {dt}\")\n",
    "print(f\"  Date: {dt.strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Date & Time: {dt.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"  Friendly: {dt.strftime('%B %d, %Y at %I:%M:%S %p')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Show how to convert in your file listing\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CODE TO SHOW READABLE DATES IN FILE LISTINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Function to convert modification time\n",
    "from datetime import datetime\n",
    "\n",
    "def format_timestamp(timestamp_ms):\n",
    "    \\\"\\\"\\\"Convert Unix timestamp in milliseconds to readable format\\\"\\\"\\\"\n",
    "    timestamp_sec = timestamp_ms / 1000\n",
    "    dt = datetime.fromtimestamp(timestamp_sec)\n",
    "    return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Use in your file listings\n",
    "files = dbutils.fs.ls(\"dbfs:/student-groups/Group_4_4\")\n",
    "\n",
    "for f in files:\n",
    "    readable_date = format_timestamp(f.modificationTime)\n",
    "    print(f\"{f.name:60s} {readable_date}\")\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# Example with actual file listing\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE: LIST FILES WITH READABLE DATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def format_timestamp(timestamp_ms):\n",
    "    \"\"\"Convert Unix timestamp in milliseconds to readable format\"\"\"\n",
    "    timestamp_sec = timestamp_ms / 1000\n",
    "    dt = datetime.fromtimestamp(timestamp_sec)\n",
    "    return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(BASE_DIR)\n",
    "    \n",
    "    print(f\"\\nFiles in {BASE_DIR}:\")\n",
    "    print(f\"\\n{'Name':<60} {'Modified':<20} {'Size':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Sort by modification time (newest first)\n",
    "    files_sorted = sorted(files, key=lambda x: x.modificationTime, reverse=True)\n",
    "    \n",
    "    for f in files_sorted[:20]:  # Show first 20\n",
    "        readable_date = format_timestamp(f.modificationTime)\n",
    "        \n",
    "        # Format size\n",
    "        if f.size == 0:\n",
    "            size_str = \"DIR\" if f.name.endswith('/') else \"0 B\"\n",
    "        elif f.size < 1024**2:\n",
    "            size_str = f\"{f.size/1024:.1f} KB\"\n",
    "        elif f.size < 1024**3:\n",
    "            size_str = f\"{f.size/(1024**2):.1f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{f.size/(1024**3):.2f} GB\"\n",
    "        \n",
    "        print(f\"{f.name:<60} {readable_date:<20} {size_str:<10}\")\n",
    "    \n",
    "    if len(files) > 20:\n",
    "        print(f\"\\n... and {len(files) - 20} more files\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Group files by date\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FILES GROUPED BY MODIFICATION DATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(BASE_DIR)\n",
    "    \n",
    "    # Group by date\n",
    "    from collections import defaultdict\n",
    "    files_by_date = defaultdict(list)\n",
    "    \n",
    "    for f in files:\n",
    "        timestamp_sec = f.modificationTime / 1000\n",
    "        dt = datetime.fromtimestamp(timestamp_sec)\n",
    "        date_str = dt.strftime('%Y-%m-%d')\n",
    "        files_by_date[date_str].append(f)\n",
    "    \n",
    "    # Sort dates\n",
    "    for date in sorted(files_by_date.keys(), reverse=True)[:5]:  # Last 5 days\n",
    "        print(f\"\\n\uD83D\uDCC5 {date} ({len(files_by_date[date])} items)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for f in sorted(files_by_date[date], key=lambda x: x.name)[:10]:\n",
    "            timestamp_sec = f.modificationTime / 1000\n",
    "            dt = datetime.fromtimestamp(timestamp_sec)\n",
    "            time_str = dt.strftime('%H:%M:%S')\n",
    "            print(f\"  {time_str} - {f.name}\")\n",
    "        \n",
    "        if len(files_by_date[date]) > 10:\n",
    "            print(f\"  ... and {len(files_by_date[date]) - 10} more files\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" TIMESTAMP CONVERSION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abd4f58-edb0-4011-9ff2-4904736061a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nALL FILES IN BASE DIRECTORY (NOT FOLDERS)\n================================================================================\nBase: dbfs:/student-groups/Group_4_4\n\nTotal files (not directories): 0\n\nBreakdown:\n  TXT files: 0\n  CSV files: 0\n  JPG files: 0\n  PNG files: 0\n  Other files: 0\n\n================================================================================\nALL FILES:\n================================================================================\n--------------------------------------------------------------------------------\nTOTAL                                                                             0 B\n\n================================================================================\nSIMPLE LIST (for copying):\n================================================================================\n\n\n================================================================================\nPYTHON LIST FORMAT:\n================================================================================\n\nfiles = [\n]\n\n================================================================================\n LISTING COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIST ALL FILES (NOT DIRECTORIES)\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL FILES IN BASE DIRECTORY (NOT FOLDERS)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Base: {BASE_DIR}\\n\")\n",
    "\n",
    "def get_size_str(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    elif size_bytes < 1024:\n",
    "        return f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024**2:\n",
    "        return f\"{size_bytes/1024:.1f} KB\"\n",
    "    elif size_bytes < 1024**3:\n",
    "        return f\"{size_bytes/(1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes/(1024**3):.2f} GB\"\n",
    "\n",
    "try:\n",
    "    all_items = dbutils.fs.ls(BASE_DIR)\n",
    "    \n",
    "    # Filter out directories (items ending with '/')\n",
    "    files_only = [item for item in all_items if not item.name.endswith('/')]\n",
    "    \n",
    "    # Sort by name\n",
    "    files_only = sorted(files_only, key=lambda x: x.name)\n",
    "    \n",
    "    # Categorize by extension\n",
    "    txt_files = []\n",
    "    csv_files = []\n",
    "    jpg_files = []\n",
    "    png_files = []\n",
    "    other_files = []\n",
    "    \n",
    "    for item in files_only:\n",
    "        if item.name.endswith('.txt'):\n",
    "            txt_files.append(item)\n",
    "        elif item.name.endswith('.csv'):\n",
    "            csv_files.append(item)\n",
    "        elif item.name.endswith('.jpg') or item.name.endswith('.jpeg'):\n",
    "            jpg_files.append(item)\n",
    "        elif item.name.endswith('.png'):\n",
    "            png_files.append(item)\n",
    "        else:\n",
    "            other_files.append(item)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"Total files (not directories): {len(files_only)}\\n\")\n",
    "    print(f\"Breakdown:\")\n",
    "    print(f\"  TXT files: {len(txt_files)}\")\n",
    "    print(f\"  CSV files: {len(csv_files)}\")\n",
    "    print(f\"  JPG files: {len(jpg_files)}\")\n",
    "    print(f\"  PNG files: {len(png_files)}\")\n",
    "    print(f\"  Other files: {len(other_files)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Display all files\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL FILES:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_size = 0\n",
    "    \n",
    "    for i, item in enumerate(files_only, 1):\n",
    "        size_str = get_size_str(item.size)\n",
    "        total_size = total_size + item.size\n",
    "        \n",
    "        # Determine file type emoji\n",
    "        if item.name.endswith('.txt'):\n",
    "            emoji = \"\uD83D\uDCC4\"\n",
    "        elif item.name.endswith('.csv'):\n",
    "            emoji = \"\uD83D\uDCCA\"\n",
    "        elif item.name.endswith('.jpg') or item.name.endswith('.jpeg'):\n",
    "            emoji = \"\uD83D\uDCF7\"\n",
    "        elif item.name.endswith('.png'):\n",
    "            emoji = \"\uD83D\uDDBC\"\n",
    "        else:\n",
    "            emoji = \"\uD83D\uDCC1\"\n",
    "        \n",
    "        print(f\"{i:2d}. {emoji} {item.name:70s} {size_str:>10s}\")\n",
    "    \n",
    "    # Total size\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'TOTAL':74s} {get_size_str(total_size):>10s}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Simple list for copying\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SIMPLE LIST (for copying):\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for item in files_only:\n",
    "        print(item.name)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Python list format\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PYTHON LIST FORMAT:\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"files = [\")\n",
    "    for item in files_only:\n",
    "        print(f'    \"{item.name}\",')\n",
    "    print(\"]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" LISTING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec49193-5ad2-444b-a155-8660d0e764e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nALL FOLDERS IN BASE DIRECTORY\n================================================================================\nBase: dbfs:/student-groups/Group_4_4\n\nTotal folders: 52\n\nBreakdown:\n  Parquet folders: 36\n  Working folders (Charts, csvs_1Y, etc.): 5\n  Other folders: 11\n\n================================================================================\nALL FOLDERS WITH DETAILS:\n================================================================================\n 1. \uD83D\uDCC2 2015_final_feature_engineered_data_with_dep_delay             25 files,   519.0 MB\n 2. \uD83D\uDCC1 Charts                                                        2 dirs,  16 files,   839.7 KB\n 3. \uD83D\uDCE6 JOINED_1Y.parquet                                             26 files,   489.3 MB\n 4. \uD83D\uDCE6 JOINED_1Y_2015.parquet                                        27 files,   389.5 MB\n 5. \uD83D\uDCE6 JOINED_1Y_2019.parquet                                        26 files,   489.2 MB\n 6. \uD83D\uDCE6 JOINED_3M.parquet                                             12 files,    86.7 MB\n 7. \uD83D\uDCE6 JOINED_3M_2015.parquet                                        13 files,    86.7 MB\n 8. \uD83D\uDCE6 JOINED_5Y_2015_2019.parquet                                   66 files,    2.21 GB\n 9. \uD83D\uDCC2 _checkpoints                                                  1 dirs,   0 files,        0 B\n10. \uD83D\uDCC2 charts                                                        37 files,     9.6 MB\n11. \uD83D\uDCE6 checkpoint_1_initial_joined_2015.parquet                      23 files,   390.9 MB\n12. \uD83D\uDCE6 checkpoint_1_initial_joined_2019.parquet                       8 files,   402.8 MB\n13. \uD83D\uDCE6 checkpoint_2_cleaned_imputed_2015.parquet                     14 files,   193.6 MB\n14. \uD83D\uDCE6 checkpoint_3_basic_features_2015.parquet                      12 files,   268.8 MB\n15. \uD83D\uDCE6 checkpoint_3_basic_features_2019.parquet                      12 files,   268.7 MB\n16. \uD83D\uDCE6 checkpoint_4_advanced_features_2015.parquet                   70 files,    1.13 GB\n17. \uD83D\uDCE6 checkpoint_5_final_clean_2015.parquet                         21 files,   611.9 MB\n18. \uD83D\uDCE6 checkpoint_5a_final_data_with_all_features_2015.parquet       21 files,   611.9 MB\n19. \uD83D\uDCE6 checkpoint_6_regression_features_baseline_2015.parquet        2 dirs,   0 files,        0 B\n20. \uD83D\uDCE6 checkpoint_7_regression_features_post_baseline_2015.parquet   2 dirs,   0 files,        0 B\n21. \uD83D\uDCC1 csvs_1Y                                                       45 files,     1.3 MB\n22. \uD83D\uDCC2 data_12M                                                     11 dirs,   0 files,        0 B\n23. \uD83D\uDCE6 df_joined_1Y_2015_clean.parquet                               18 files,   250.3 MB\n24. \uD83D\uDCE6 df_otpw_12M_clean.parquet                                     37 files,   117.0 MB\n25. \uD83D\uDCE6 df_otpw_3m.parquet                                             7 files,   108.2 MB\n26. \uD83D\uDCE6 df_weather.parquet                                            15 files,    1.07 GB\n27. \uD83D\uDCC2 feature_views                                                 2 dirs,   0 files,        0 B\n28. \uD83D\uDCC2 final_feature_engineered_data_with_dep_delay                  26 files,   511.9 MB\n29. \uD83D\uDCC2 future_joins                                                  2 dirs,   0 files,        0 B\n30. \uD83D\uDCE6 joined_1Y_2015_features_clean.parquet                         25 files,   273.2 MB\n31. \uD83D\uDCE6 joined_1Y_2015_final_clean.parquet                            14 files,   186.6 MB\n32. \uD83D\uDCE6 joined_1Y_2015_final_feature_clean.parquet                    26 files,   316.0 MB\n33. \uD83D\uDCE6 joined_1Y_clean_imputed.parquet                               30 files,   265.4 MB\n34. \uD83D\uDCE6 joined_1Y_feat.parquet                                        14 files,   521.8 MB\n35. \uD83D\uDCE6 joined_1Y_feat_2.parquet                                      30 files,   793.4 MB\n36. \uD83D\uDCE6 joined_1Y_feat_temp.parquet                                   14 files,   356.2 MB\n37. \uD83D\uDCE6 joined_1Y_feat_temp2.parquet                                   1 files,        0 B\n38. \uD83D\uDCE6 joined_1Y_feat_temp_2.parquet                                 14 files,   352.4 MB\n39. \uD83D\uDCE6 joined_1Y_features_clean.parquet                              12 files,   343.3 MB\n40. \uD83D\uDCE6 joined_1Y_final_clean.parquet                                 19 files,   241.3 MB\n41. \uD83D\uDCE6 joined_1Y_final_feature_clean.parquet                         21 files,   408.8 MB\n42. \uD83D\uDCC2 joined_1Y_final_feature_clean_with_removed_features           53 files,    1.14 GB\n43. \uD83D\uDCE6 joined_3m_clean_imputed.parquet                               14 files,    95.7 MB\n44. \uD83D\uDCE6 joined_3m_final_clean.parquet                                 16 files,    78.5 MB\n45. \uD83D\uDCC2 mart                                                          3 dirs,   0 files,        0 B\n46. \uD83D\uDCCA plots_phase2                                                   0 files,        0 B\n47. \uD83D\uDCCA regression_results                                            1 dirs,   0 files,        0 B\n48. \uD83D\uDCE6 temp2.parquet                                                 39 files,   479.3 MB\n49. \uD83D\uDCE6 temp3-4.parquet                                               29 files,   404.5 MB\n50. \uD83D\uDCC2 test_data_regression_scaled_2015                              27 files,    91.0 MB\n51. \uD83D\uDCC2 timeseries                                                    5 dirs,   0 files,        0 B\n52. \uD83D\uDCC2 train_data_regression_scaled_2015                             27 files,   326.1 MB\n--------------------------------------------------------------------------------\nTOTAL STORAGE                                                          16.62 GB\n\n================================================================================\n\uD83D\uDCE6 PARQUET FOLDERS (36):\n================================================================================\n  JOINED_1Y.parquet                                                  26 files,   489.3 MB\n  JOINED_1Y_2015.parquet                                             27 files,   389.5 MB\n  JOINED_1Y_2019.parquet                                             26 files,   489.2 MB\n  JOINED_3M.parquet                                                  12 files,    86.7 MB\n  JOINED_3M_2015.parquet                                             13 files,    86.7 MB\n  JOINED_5Y_2015_2019.parquet                                        66 files,    2.21 GB\n  checkpoint_1_initial_joined_2015.parquet                           23 files,   390.9 MB\n  checkpoint_1_initial_joined_2019.parquet                            8 files,   402.8 MB\n  checkpoint_2_cleaned_imputed_2015.parquet                          14 files,   193.6 MB\n  checkpoint_3_basic_features_2015.parquet                           12 files,   268.8 MB\n  checkpoint_3_basic_features_2019.parquet                           12 files,   268.7 MB\n  checkpoint_4_advanced_features_2015.parquet                        70 files,    1.13 GB\n  checkpoint_5_final_clean_2015.parquet                              21 files,   611.9 MB\n  checkpoint_5a_final_data_with_all_features_2015.parquet            21 files,   611.9 MB\n  checkpoint_6_regression_features_baseline_2015.parquet              0 files,        0 B\n  checkpoint_7_regression_features_post_baseline_2015.parquet         0 files,        0 B\n  df_joined_1Y_2015_clean.parquet                                    18 files,   250.3 MB\n  df_otpw_12M_clean.parquet                                          37 files,   117.0 MB\n  df_otpw_3m.parquet                                                  7 files,   108.2 MB\n  df_weather.parquet                                                 15 files,    1.07 GB\n  joined_1Y_2015_features_clean.parquet                              25 files,   273.2 MB\n  joined_1Y_2015_final_clean.parquet                                 14 files,   186.6 MB\n  joined_1Y_2015_final_feature_clean.parquet                         26 files,   316.0 MB\n  joined_1Y_clean_imputed.parquet                                    30 files,   265.4 MB\n  joined_1Y_feat.parquet                                             14 files,   521.8 MB\n  joined_1Y_feat_2.parquet                                           30 files,   793.4 MB\n  joined_1Y_feat_temp.parquet                                        14 files,   356.2 MB\n  joined_1Y_feat_temp2.parquet                                        1 files,        0 B\n  joined_1Y_feat_temp_2.parquet                                      14 files,   352.4 MB\n  joined_1Y_features_clean.parquet                                   12 files,   343.3 MB\n  joined_1Y_final_clean.parquet                                      19 files,   241.3 MB\n  joined_1Y_final_feature_clean.parquet                              21 files,   408.8 MB\n  joined_3m_clean_imputed.parquet                                    14 files,    95.7 MB\n  joined_3m_final_clean.parquet                                      16 files,    78.5 MB\n  temp2.parquet                                                      39 files,   479.3 MB\n  temp3-4.parquet                                                    29 files,   404.5 MB\n  -----------------------------------------------------------------              14.06 GB\n\n================================================================================\n\uD83D\uDCC1 WORKING FOLDERS (5):\n================================================================================\n  Charts                                              2 dirs,  16 files,   839.7 KB\n  charts                                              37 files,     9.6 MB\n  csvs_1Y                                             45 files,     1.3 MB\n  plots_phase2                                         0 files,        0 B\n  regression_results                                  1 dirs,   0 files,        0 B\n\n================================================================================\nSIMPLE FOLDER LIST:\n================================================================================\n\n2015_final_feature_engineered_data_with_dep_delay/\nCharts/\nJOINED_1Y.parquet/\nJOINED_1Y_2015.parquet/\nJOINED_1Y_2019.parquet/\nJOINED_3M.parquet/\nJOINED_3M_2015.parquet/\nJOINED_5Y_2015_2019.parquet/\n_checkpoints/\ncharts/\ncheckpoint_1_initial_joined_2015.parquet/\ncheckpoint_1_initial_joined_2019.parquet/\ncheckpoint_2_cleaned_imputed_2015.parquet/\ncheckpoint_3_basic_features_2015.parquet/\ncheckpoint_3_basic_features_2019.parquet/\ncheckpoint_4_advanced_features_2015.parquet/\ncheckpoint_5_final_clean_2015.parquet/\ncheckpoint_5a_final_data_with_all_features_2015.parquet/\ncheckpoint_6_regression_features_baseline_2015.parquet/\ncheckpoint_7_regression_features_post_baseline_2015.parquet/\ncsvs_1Y/\ndata_12M/\ndf_joined_1Y_2015_clean.parquet/\ndf_otpw_12M_clean.parquet/\ndf_otpw_3m.parquet/\ndf_weather.parquet/\nfeature_views/\nfinal_feature_engineered_data_with_dep_delay/\nfuture_joins/\njoined_1Y_2015_features_clean.parquet/\njoined_1Y_2015_final_clean.parquet/\njoined_1Y_2015_final_feature_clean.parquet/\njoined_1Y_clean_imputed.parquet/\njoined_1Y_feat.parquet/\njoined_1Y_feat_2.parquet/\njoined_1Y_feat_temp.parquet/\njoined_1Y_feat_temp2.parquet/\njoined_1Y_feat_temp_2.parquet/\njoined_1Y_features_clean.parquet/\njoined_1Y_final_clean.parquet/\njoined_1Y_final_feature_clean.parquet/\njoined_1Y_final_feature_clean_with_removed_features/\njoined_3m_clean_imputed.parquet/\njoined_3m_final_clean.parquet/\nmart/\nplots_phase2/\nregression_results/\ntemp2.parquet/\ntemp3-4.parquet/\ntest_data_regression_scaled_2015/\ntimeseries/\ntrain_data_regression_scaled_2015/\n\n================================================================================\nPYTHON LIST FORMAT:\n================================================================================\n\nfolders = [\n    \"2015_final_feature_engineered_data_with_dep_delay\",\n    \"Charts\",\n    \"JOINED_1Y.parquet\",\n    \"JOINED_1Y_2015.parquet\",\n    \"JOINED_1Y_2019.parquet\",\n    \"JOINED_3M.parquet\",\n    \"JOINED_3M_2015.parquet\",\n    \"JOINED_5Y_2015_2019.parquet\",\n    \"_checkpoints\",\n    \"charts\",\n    \"checkpoint_1_initial_joined_2015.parquet\",\n    \"checkpoint_1_initial_joined_2019.parquet\",\n    \"checkpoint_2_cleaned_imputed_2015.parquet\",\n    \"checkpoint_3_basic_features_2015.parquet\",\n    \"checkpoint_3_basic_features_2019.parquet\",\n    \"checkpoint_4_advanced_features_2015.parquet\",\n    \"checkpoint_5_final_clean_2015.parquet\",\n    \"checkpoint_5a_final_data_with_all_features_2015.parquet\",\n    \"checkpoint_6_regression_features_baseline_2015.parquet\",\n    \"checkpoint_7_regression_features_post_baseline_2015.parquet\",\n    \"csvs_1Y\",\n    \"data_12M\",\n    \"df_joined_1Y_2015_clean.parquet\",\n    \"df_otpw_12M_clean.parquet\",\n    \"df_otpw_3m.parquet\",\n    \"df_weather.parquet\",\n    \"feature_views\",\n    \"final_feature_engineered_data_with_dep_delay\",\n    \"future_joins\",\n    \"joined_1Y_2015_features_clean.parquet\",\n    \"joined_1Y_2015_final_clean.parquet\",\n    \"joined_1Y_2015_final_feature_clean.parquet\",\n    \"joined_1Y_clean_imputed.parquet\",\n    \"joined_1Y_feat.parquet\",\n    \"joined_1Y_feat_2.parquet\",\n    \"joined_1Y_feat_temp.parquet\",\n    \"joined_1Y_feat_temp2.parquet\",\n    \"joined_1Y_feat_temp_2.parquet\",\n    \"joined_1Y_features_clean.parquet\",\n    \"joined_1Y_final_clean.parquet\",\n    \"joined_1Y_final_feature_clean.parquet\",\n    \"joined_1Y_final_feature_clean_with_removed_features\",\n    \"joined_3m_clean_imputed.parquet\",\n    \"joined_3m_final_clean.parquet\",\n    \"mart\",\n    \"plots_phase2\",\n    \"regression_results\",\n    \"temp2.parquet\",\n    \"temp3-4.parquet\",\n    \"test_data_regression_scaled_2015\",\n    \"timeseries\",\n    \"train_data_regression_scaled_2015\",\n]\n\n================================================================================\n LISTING COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIST ALL FOLDERS (DIRECTORIES ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL FOLDERS IN BASE DIRECTORY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Base: {BASE_DIR}\\n\")\n",
    "\n",
    "def get_size_str(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    elif size_bytes < 1024:\n",
    "        return f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024**2:\n",
    "        return f\"{size_bytes/1024:.1f} KB\"\n",
    "    elif size_bytes < 1024**3:\n",
    "        return f\"{size_bytes/(1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes/(1024**3):.2f} GB\"\n",
    "\n",
    "try:\n",
    "    all_items = dbutils.fs.ls(BASE_DIR)\n",
    "    \n",
    "    # Filter only directories (items ending with '/')\n",
    "    folders_only = [item for item in all_items if item.name.endswith('/')]\n",
    "    \n",
    "    # Sort by name\n",
    "    folders_only = sorted(folders_only, key=lambda x: x.name)\n",
    "    \n",
    "    # Categorize folders\n",
    "    parquet_folders = []\n",
    "    work_folders = []\n",
    "    other_folders = []\n",
    "    \n",
    "    for item in folders_only:\n",
    "        if item.name.endswith('.parquet/'):\n",
    "            parquet_folders.append(item)\n",
    "        elif item.name.rstrip('/') in ['Charts', 'csvs_1Y', 'charts', 'plots_phase2', 'regression_results']:\n",
    "            work_folders.append(item)\n",
    "        else:\n",
    "            other_folders.append(item)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"Total folders: {len(folders_only)}\\n\")\n",
    "    print(f\"Breakdown:\")\n",
    "    print(f\"  Parquet folders: {len(parquet_folders)}\")\n",
    "    print(f\"  Working folders (Charts, csvs_1Y, etc.): {len(work_folders)}\")\n",
    "    print(f\"  Other folders: {len(other_folders)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Display all folders with details\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL FOLDERS WITH DETAILS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_storage = 0\n",
    "    \n",
    "    for i, item in enumerate(folders_only, 1):\n",
    "        folder_name = item.name.rstrip('/')\n",
    "        \n",
    "        try:\n",
    "            # Get folder contents\n",
    "            contents = dbutils.fs.ls(item.path)\n",
    "            \n",
    "            # Count items and calculate size\n",
    "            num_files = 0\n",
    "            num_subdirs = 0\n",
    "            folder_size = 0\n",
    "            \n",
    "            for content_item in contents:\n",
    "                if content_item.name.endswith('/'):\n",
    "                    num_subdirs += 1\n",
    "                else:\n",
    "                    num_files += 1\n",
    "                    folder_size = folder_size + content_item.size\n",
    "            \n",
    "            total_storage = total_storage + folder_size\n",
    "            size_str = get_size_str(folder_size)\n",
    "            \n",
    "            # Determine emoji\n",
    "            if item.name.endswith('.parquet/'):\n",
    "                emoji = \"\uD83D\uDCE6\"\n",
    "            elif folder_name in ['Charts', 'csvs_1Y']:\n",
    "                emoji = \"\uD83D\uDCC1\"\n",
    "            elif folder_name in ['plots_phase2', 'regression_results']:\n",
    "                emoji = \"\uD83D\uDCCA\"\n",
    "            else:\n",
    "                emoji = \"\uD83D\uDCC2\"\n",
    "            \n",
    "            # Display info\n",
    "            if num_subdirs > 0:\n",
    "                print(f\"{i:2d}. {emoji} {folder_name:60s} {num_subdirs:>2} dirs, {num_files:>3} files, {size_str:>10s}\")\n",
    "            else:\n",
    "                print(f\"{i:2d}. {emoji} {folder_name:60s} {num_files:>3} files, {size_str:>10s}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{i:2d}.  {folder_name:60s} (error reading)\")\n",
    "    \n",
    "    # Total\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'TOTAL STORAGE':68s} {get_size_str(total_storage):>10s}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Parquet folders separately\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\uD83D\uDCE6 PARQUET FOLDERS ({len(parquet_folders)}):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    parquet_total = 0\n",
    "    \n",
    "    for item in parquet_folders:\n",
    "        folder_name = item.name.rstrip('/')\n",
    "        \n",
    "        try:\n",
    "            contents = dbutils.fs.ls(item.path)\n",
    "            num_files = 0\n",
    "            folder_size = 0\n",
    "            \n",
    "            for content_item in contents:\n",
    "                if not content_item.name.endswith('/'):\n",
    "                    num_files += 1\n",
    "                    folder_size = folder_size + content_item.size\n",
    "            \n",
    "            parquet_total = parquet_total + folder_size\n",
    "            size_str = get_size_str(folder_size)\n",
    "            \n",
    "            print(f\"  {folder_name:65s} {num_files:>3} files, {size_str:>10s}\")\n",
    "            \n",
    "        except:\n",
    "            print(f\"  {folder_name:65s} (error)\")\n",
    "    \n",
    "    print(f\"  {'-'*65} {get_size_str(parquet_total):>21s}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Working folders separately\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\uD83D\uDCC1 WORKING FOLDERS ({len(work_folders)}):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for item in work_folders:\n",
    "        folder_name = item.name.rstrip('/')\n",
    "        \n",
    "        try:\n",
    "            contents = dbutils.fs.ls(item.path)\n",
    "            num_files = 0\n",
    "            num_subdirs = 0\n",
    "            folder_size = 0\n",
    "            \n",
    "            for content_item in contents:\n",
    "                if content_item.name.endswith('/'):\n",
    "                    num_subdirs += 1\n",
    "                else:\n",
    "                    num_files += 1\n",
    "                    folder_size = folder_size + content_item.size\n",
    "            \n",
    "            size_str = get_size_str(folder_size)\n",
    "            \n",
    "            if num_subdirs > 0:\n",
    "                print(f\"  {folder_name:50s} {num_subdirs:>2} dirs, {num_files:>3} files, {size_str:>10s}\")\n",
    "            else:\n",
    "                print(f\"  {folder_name:50s} {num_files:>3} files, {size_str:>10s}\")\n",
    "            \n",
    "        except:\n",
    "            print(f\"  {folder_name:50s} (error)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Simple list for reference\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SIMPLE FOLDER LIST:\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for item in folders_only:\n",
    "        print(item.name)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Python list format\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PYTHON LIST FORMAT:\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"folders = [\")\n",
    "    for item in folders_only:\n",
    "        folder_name = item.name.rstrip('/')\n",
    "        print(f'    \"{folder_name}\",')\n",
    "    print(\"]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" LISTING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3505d70-faa3-44c5-a8f9-ee85092e97ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nDETAILED FOLDER CONTENTS\n================================================================================\n\nAnalyzing 29 folders...\n\n================================================================================\nSUMMARY TABLE\n================================================================================\n\n#   Folder Name                                                       Files  Subdirs  Size        \n----------------------------------------------------------------------------------------------------\n1   joined_1Y_final_feature_clean_with_removed_features               53     0        1.14 GB     \n2   joined_1Y_feat_2.parquet                                          30     0        793.4 MB    \n3   joined_1Y_feat.parquet                                            14     0        521.8 MB    \n4   temp2.parquet                                                     39     0        479.3 MB    \n5   joined_1Y_final_feature_clean.parquet                             21     0        408.8 MB    \n6   temp3-4.parquet                                                   29     0        404.5 MB    \n7   joined_1Y_feat_temp.parquet                                       14     0        356.2 MB    \n8   joined_1Y_feat_temp_2.parquet                                     14     0        352.4 MB    \n9   joined_1Y_features_clean.parquet                                  12     0        343.3 MB    \n10  train_data_regression_scaled_2015                                 27     0        326.1 MB    \n11  joined_1Y_2015_final_feature_clean.parquet                        26     0        316.0 MB    \n12  joined_1Y_2015_features_clean.parquet                             25     0        273.2 MB    \n13  joined_1Y_clean_imputed.parquet                                   30     0        265.4 MB    \n14  df_joined_1Y_2015_clean.parquet                                   18     0        250.3 MB    \n15  joined_1Y_final_clean.parquet                                     19     0        241.3 MB    \n16  joined_1Y_2015_final_clean.parquet                                14     0        186.6 MB    \n17  joined_3m_clean_imputed.parquet                                   14     0        95.7 MB     \n18  test_data_regression_scaled_2015                                  27     0        91.0 MB     \n19  joined_3m_final_clean.parquet                                     16     0        78.5 MB     \n20  charts                                                            37     0        9.6 MB      \n21  Charts                                                            16     2        839.7 KB    \n22  _checkpoints                                                      0      1        0 B         \n23  data_12M                                                          0      11       0 B         \n24  future_joins                                                      0      2        0 B         \n25  joined_1Y_feat_temp2.parquet                                      1      0        0 B         \n26  mart                                                              0      3        0 B         \n27  plots_phase2                                                      0      0        0 B          (EMPTY)\n28  regression_results                                                0      1        0 B         \n29  timeseries                                                        0      5        0 B         \n----------------------------------------------------------------------------------------------------\nTOTAL                                                                 6.80 GB     \n\n================================================================================\nDETAILED CONTENTS\n================================================================================\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_final_feature_clean_with_removed_features\n================================================================================\n  Files: 53, Subdirs: 0, Total Size: 1.14 GB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_8700001934593135307                                   5.0 KB\n    \uD83D\uDCC4 _started_8700001934593135307                                        0 B\n    \uD83D\uDCE6 part-00000-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1734-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00001-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1735-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00002-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1736-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00003-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1737-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00004-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1738-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00005-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1739-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00006-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1740-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00007-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1741-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00008-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1742-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00009-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1743-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00010-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1744-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00011-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1745-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00012-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1746-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00013-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1747-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00014-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1748-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00015-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1749-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00016-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1750-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00017-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1751-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00018-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1752-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00019-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1753-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00020-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1754-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00021-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1755-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00022-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1756-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00023-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1757-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00024-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1758-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00025-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1759-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00026-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1760-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00027-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1761-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00028-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1762-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00029-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1763-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00030-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1764-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00031-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1765-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00032-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1766-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00033-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1767-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00034-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1768-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00035-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1769-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00036-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1770-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00037-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1771-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00038-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1772-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00039-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1773-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00040-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1774-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00041-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1775-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00042-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1776-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00043-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1777-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00044-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1778-1.c000.snappy.parquet    23.4 MB\n    \uD83D\uDCE6 part-00045-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1779-1.c000.snappy.parquet    23.3 MB\n    \uD83D\uDCE6 part-00046-tid-8700001934593135307-159416e3-86ea-4f1b-807a-84d479f35e6c-1780-1.c000.snappy.parquet    23.4 MB\n    ... and 3 more files\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_feat_2.parquet\n================================================================================\n  Files: 30, Subdirs: 0, Total Size: 793.4 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_3643796445394423930                                   2.7 KB\n    \uD83D\uDCC4 _started_3643796445394423930                                        0 B\n    \uD83D\uDCE6 part-00000-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4484-1.c000.snappy.parquet    27.9 MB\n    \uD83D\uDCE6 part-00001-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4479-1.c000.snappy.parquet    29.3 MB\n    \uD83D\uDCE6 part-00002-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4470-1.c000.snappy.parquet    30.9 MB\n    \uD83D\uDCE6 part-00003-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4475-1.c000.snappy.parquet    30.0 MB\n    \uD83D\uDCE6 part-00004-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4461-1.c000.snappy.parquet    32.5 MB\n    \uD83D\uDCE6 part-00005-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4482-1.c000.snappy.parquet    28.8 MB\n    \uD83D\uDCE6 part-00006-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4468-1.c000.snappy.parquet    31.4 MB\n    \uD83D\uDCE6 part-00007-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4460-1.c000.snappy.parquet    32.6 MB\n    \uD83D\uDCE6 part-00008-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4481-1.c000.snappy.parquet    29.4 MB\n    \uD83D\uDCE6 part-00009-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4464-1.c000.snappy.parquet    31.5 MB\n    \uD83D\uDCE6 part-00010-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4463-1.c000.snappy.parquet    32.4 MB\n    \uD83D\uDCE6 part-00011-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4466-1.c000.snappy.parquet    31.6 MB\n    \uD83D\uDCE6 part-00012-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4462-1.c000.snappy.parquet    32.2 MB\n    \uD83D\uDCE6 part-00013-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4478-1.c000.snappy.parquet    29.1 MB\n    \uD83D\uDCE6 part-00014-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4471-1.c000.snappy.parquet    30.9 MB\n    \uD83D\uDCE6 part-00015-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4485-1.c000.snappy.parquet    26.4 MB\n    \uD83D\uDCE6 part-00016-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4472-1.c000.snappy.parquet    30.9 MB\n    \uD83D\uDCE6 part-00017-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4465-1.c000.snappy.parquet    31.2 MB\n    \uD83D\uDCE6 part-00018-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4474-1.c000.snappy.parquet    30.2 MB\n    \uD83D\uDCE6 part-00019-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4469-1.c000.snappy.parquet    30.8 MB\n    \uD83D\uDCE6 part-00020-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4483-1.c000.snappy.parquet    28.3 MB\n    \uD83D\uDCE6 part-00021-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4476-1.c000.snappy.parquet    30.8 MB\n    \uD83D\uDCE6 part-00022-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4477-1.c000.snappy.parquet    29.4 MB\n    \uD83D\uDCE6 part-00023-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4480-1.c000.snappy.parquet    29.0 MB\n    \uD83D\uDCE6 part-00024-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4467-1.c000.snappy.parquet    31.4 MB\n    \uD83D\uDCE6 part-00025-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4473-1.c000.snappy.parquet    30.8 MB\n    \uD83D\uDCE6 part-00026-tid-3643796445394423930-2210c7e2-b882-4e46-ae5e-e12d904b1df3-4486-1.c000.snappy.parquet     3.8 MB\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_feat.parquet\n================================================================================\n  Files: 14, Subdirs: 0, Total Size: 521.8 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_2300020650154153927                                   3.1 KB\n    \uD83D\uDCC4 _committed_6576138488998908170                                   2.2 KB\n    \uD83D\uDCC4 _committed_vacuum2631416144209699650                               96 B\n    \uD83D\uDCC4 _started_2300020650154153927                                        0 B\n    \uD83D\uDCE6 part-00000-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-11-1.c000.snappy.parquet    67.2 MB\n    \uD83D\uDCE6 part-00001-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-13-1.c000.snappy.parquet    55.3 MB\n    \uD83D\uDCE6 part-00002-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-15-1.c000.snappy.parquet    49.2 MB\n    \uD83D\uDCE6 part-00003-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-17-1.c000.snappy.parquet    58.8 MB\n    \uD83D\uDCE6 part-00004-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-12-1.c000.snappy.parquet    60.2 MB\n    \uD83D\uDCE6 part-00005-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-14-1.c000.snappy.parquet    54.8 MB\n    \uD83D\uDCE6 part-00006-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-16-1.c000.snappy.parquet    51.8 MB\n    \uD83D\uDCE6 part-00007-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-18-1.c000.snappy.parquet    64.7 MB\n    \uD83D\uDCE6 part-00008-tid-2300020650154153927-41532292-eac9-4525-af12-e9c9ff4a2818-19-1.c000.snappy.parquet    59.6 MB\n\n================================================================================\n\uD83D\uDCC1 temp2.parquet\n================================================================================\n  Files: 39, Subdirs: 0, Total Size: 479.3 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_3761756374203105802                                   3.6 KB\n    \uD83D\uDCC4 _started_3761756374203105802                                        0 B\n    \uD83D\uDCE6 part-00000-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80881-1.c000.snappy.parquet    12.8 MB\n    \uD83D\uDCE6 part-00001-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80882-1.c000.snappy.parquet    15.1 MB\n    \uD83D\uDCE6 part-00002-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80883-1.c000.snappy.parquet     5.7 MB\n    \uD83D\uDCE6 part-00003-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80884-1.c000.snappy.parquet    14.2 MB\n    \uD83D\uDCE6 part-00004-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80885-1.c000.snappy.parquet     8.5 MB\n    \uD83D\uDCE6 part-00005-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80886-1.c000.snappy.parquet    15.6 MB\n    \uD83D\uDCE6 part-00006-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80887-1.c000.snappy.parquet    11.8 MB\n    \uD83D\uDCE6 part-00007-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80888-1.c000.snappy.parquet    10.5 MB\n    \uD83D\uDCE6 part-00008-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80889-1.c000.snappy.parquet     9.9 MB\n    \uD83D\uDCE6 part-00009-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80890-1.c000.snappy.parquet    14.5 MB\n    \uD83D\uDCE6 part-00010-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80891-1.c000.snappy.parquet    19.3 MB\n    \uD83D\uDCE6 part-00011-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80892-1.c000.snappy.parquet     7.5 MB\n    \uD83D\uDCE6 part-00012-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80893-1.c000.snappy.parquet    14.3 MB\n    \uD83D\uDCE6 part-00013-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80894-1.c000.snappy.parquet    13.7 MB\n    \uD83D\uDCE6 part-00014-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80895-1.c000.snappy.parquet    15.1 MB\n    \uD83D\uDCE6 part-00015-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80896-1.c000.snappy.parquet    10.2 MB\n    \uD83D\uDCE6 part-00016-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80897-1.c000.snappy.parquet    15.5 MB\n    \uD83D\uDCE6 part-00017-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80898-1.c000.snappy.parquet     3.7 MB\n    \uD83D\uDCE6 part-00018-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80899-1.c000.snappy.parquet    13.3 MB\n    \uD83D\uDCE6 part-00019-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80900-1.c000.snappy.parquet     4.8 MB\n    \uD83D\uDCE6 part-00020-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80901-1.c000.snappy.parquet    22.1 MB\n    \uD83D\uDCE6 part-00021-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80902-1.c000.snappy.parquet    16.2 MB\n    \uD83D\uDCE6 part-00022-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80903-1.c000.snappy.parquet     3.5 MB\n    \uD83D\uDCE6 part-00023-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80904-1.c000.snappy.parquet    42.0 MB\n    \uD83D\uDCE6 part-00024-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80905-1.c000.snappy.parquet     7.2 MB\n    \uD83D\uDCE6 part-00025-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80906-1.c000.snappy.parquet    29.3 MB\n    \uD83D\uDCE6 part-00026-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80907-1.c000.snappy.parquet    12.6 MB\n    \uD83D\uDCE6 part-00027-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80908-1.c000.snappy.parquet    12.5 MB\n    \uD83D\uDCE6 part-00028-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80909-1.c000.snappy.parquet    16.0 MB\n    \uD83D\uDCE6 part-00029-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80910-1.c000.snappy.parquet     3.7 MB\n    \uD83D\uDCE6 part-00030-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80911-1.c000.snappy.parquet    28.6 MB\n    \uD83D\uDCE6 part-00031-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80912-1.c000.snappy.parquet    12.1 MB\n    \uD83D\uDCE6 part-00032-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80913-1.c000.snappy.parquet     6.3 MB\n    \uD83D\uDCE6 part-00033-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80914-1.c000.snappy.parquet    10.4 MB\n    \uD83D\uDCE6 part-00034-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80915-1.c000.snappy.parquet     8.3 MB\n    \uD83D\uDCE6 part-00035-tid-3761756374203105802-43d08947-0317-40a7-a08d-2d6a6110d1d9-80916-1.c000.snappy.parquet    12.4 MB\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_final_feature_clean.parquet\n================================================================================\n  Files: 21, Subdirs: 0, Total Size: 408.8 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_3975264383292680051                                   1.8 KB\n    \uD83D\uDCC4 _started_3975264383292680051                                        0 B\n    \uD83D\uDCE6 part-00000-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19385-1.c000.snappy.parquet    29.2 MB\n    \uD83D\uDCE6 part-00001-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19389-1.c000.snappy.parquet    24.5 MB\n    \uD83D\uDCE6 part-00002-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19387-1.c000.snappy.parquet    27.6 MB\n    \uD83D\uDCE6 part-00003-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19386-1.c000.snappy.parquet    28.2 MB\n    \uD83D\uDCE6 part-00004-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19391-1.c000.snappy.parquet    22.8 MB\n    \uD83D\uDCE6 part-00005-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19399-1.c000.snappy.parquet    11.5 MB\n    \uD83D\uDCE6 part-00006-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19388-1.c000.snappy.parquet    23.7 MB\n    \uD83D\uDCE6 part-00007-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19382-1.c000.snappy.parquet    29.2 MB\n    \uD83D\uDCE6 part-00008-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19397-1.c000.snappy.parquet    18.3 MB\n    \uD83D\uDCE6 part-00009-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19393-1.c000.snappy.parquet    20.4 MB\n    \uD83D\uDCE6 part-00010-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19398-1.c000.snappy.parquet    12.5 MB\n    \uD83D\uDCE6 part-00011-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19383-1.c000.snappy.parquet    28.3 MB\n    \uD83D\uDCE6 part-00012-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19384-1.c000.snappy.parquet    28.3 MB\n    \uD83D\uDCE6 part-00013-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19394-1.c000.snappy.parquet    21.9 MB\n    \uD83D\uDCE6 part-00014-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19396-1.c000.snappy.parquet    18.0 MB\n    \uD83D\uDCE6 part-00015-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19392-1.c000.snappy.parquet    21.5 MB\n    \uD83D\uDCE6 part-00016-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19390-1.c000.snappy.parquet    23.5 MB\n    \uD83D\uDCE6 part-00017-tid-3975264383292680051-a5f0dc4c-f146-4372-b1c5-f3819bc8f47b-19395-1.c000.snappy.parquet    19.3 MB\n\n================================================================================\n\uD83D\uDCC1 temp3-4.parquet\n================================================================================\n  Files: 29, Subdirs: 0, Total Size: 404.5 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_2081232585286448435                                   2.6 KB\n    \uD83D\uDCC4 _started_2081232585286448435                                        0 B\n    \uD83D\uDCE6 part-00000-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66186-1.c000.snappy.parquet    15.6 MB\n    \uD83D\uDCE6 part-00001-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66187-1.c000.snappy.parquet    14.1 MB\n    \uD83D\uDCE6 part-00002-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66188-1.c000.snappy.parquet    15.0 MB\n    \uD83D\uDCE6 part-00003-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66189-1.c000.snappy.parquet    14.1 MB\n    \uD83D\uDCE6 part-00004-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66190-1.c000.snappy.parquet    14.5 MB\n    \uD83D\uDCE6 part-00005-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66191-1.c000.snappy.parquet    16.7 MB\n    \uD83D\uDCE6 part-00006-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66192-1.c000.snappy.parquet    12.7 MB\n    \uD83D\uDCE6 part-00007-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66193-1.c000.snappy.parquet    16.5 MB\n    \uD83D\uDCE6 part-00008-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66194-1.c000.snappy.parquet    14.9 MB\n    \uD83D\uDCE6 part-00009-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66195-1.c000.snappy.parquet    14.8 MB\n    \uD83D\uDCE6 part-00010-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66196-1.c000.snappy.parquet    13.1 MB\n    \uD83D\uDCE6 part-00011-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66197-1.c000.snappy.parquet     9.0 MB\n    \uD83D\uDCE6 part-00012-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66198-1.c000.snappy.parquet    16.4 MB\n    \uD83D\uDCE6 part-00013-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66199-1.c000.snappy.parquet    15.2 MB\n    \uD83D\uDCE6 part-00014-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66200-1.c000.snappy.parquet    17.9 MB\n    \uD83D\uDCE6 part-00015-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66201-1.c000.snappy.parquet    17.3 MB\n    \uD83D\uDCE6 part-00016-tid-2081232585286448435-d21ea430-070e-4399-b882-28143a070a23-66202-1.c000.snappy.parquet   \n\n*** WARNING: max output size exceeded, skipping output. ***\n\n05-9432-6b6da9007cba-14849-1.c000.snappy.parquet     9.7 MB\n    \uD83D\uDCE6 part-00009-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14850-1.c000.snappy.parquet     9.9 MB\n    \uD83D\uDCE6 part-00010-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14851-1.c000.snappy.parquet     8.9 MB\n    \uD83D\uDCE6 part-00011-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14852-1.c000.snappy.parquet     6.0 MB\n    \uD83D\uDCE6 part-00012-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14853-1.c000.snappy.parquet    10.8 MB\n    \uD83D\uDCE6 part-00013-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14854-1.c000.snappy.parquet    10.1 MB\n    \uD83D\uDCE6 part-00014-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14855-1.c000.snappy.parquet    11.4 MB\n    \uD83D\uDCE6 part-00015-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14856-1.c000.snappy.parquet    11.1 MB\n    \uD83D\uDCE6 part-00016-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14857-1.c000.snappy.parquet    20.1 MB\n    \uD83D\uDCE6 part-00017-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14858-1.c000.snappy.parquet     4.7 MB\n    \uD83D\uDCE6 part-00018-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14859-1.c000.snappy.parquet    13.9 MB\n    \uD83D\uDCE6 part-00019-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14860-1.c000.snappy.parquet     7.7 MB\n    \uD83D\uDCE6 part-00020-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14861-1.c000.snappy.parquet     7.2 MB\n    \uD83D\uDCE6 part-00021-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14862-1.c000.snappy.parquet    10.1 MB\n    \uD83D\uDCE6 part-00022-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14863-1.c000.snappy.parquet   986.9 KB\n    \uD83D\uDCE6 part-00023-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14864-1.c000.snappy.parquet    14.9 MB\n    \uD83D\uDCE6 part-00024-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14865-1.c000.snappy.parquet    10.7 MB\n    \uD83D\uDCE6 part-00025-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14866-1.c000.snappy.parquet    10.6 MB\n    \uD83D\uDCE6 part-00026-tid-1917038965717557701-1432fe30-9e36-4705-9432-6b6da9007cba-14867-1.c000.snappy.parquet     7.1 MB\n\n================================================================================\n\uD83D\uDCC1 df_joined_1Y_2015_clean.parquet\n================================================================================\n  Files: 18, Subdirs: 0, Total Size: 250.3 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_4510970318172860352                                   2.6 KB\n    \uD83D\uDCC4 _committed_5722328497295905326                                   1.3 KB\n    \uD83D\uDCC4 _committed_vacuum3109871291854638841                               96 B\n    \uD83D\uDCC4 _started_4510970318172860352                                        0 B\n    \uD83D\uDCE6 part-00000-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8607-1.c000.snappy.parquet    23.0 MB\n    \uD83D\uDCE6 part-00001-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8608-1.c000.snappy.parquet    20.6 MB\n    \uD83D\uDCE6 part-00002-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8609-1.c000.snappy.parquet    18.0 MB\n    \uD83D\uDCE6 part-00003-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8610-1.c000.snappy.parquet    21.0 MB\n    \uD83D\uDCE6 part-00004-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8611-1.c000.snappy.parquet    20.1 MB\n    \uD83D\uDCE6 part-00005-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8612-1.c000.snappy.parquet    21.8 MB\n    \uD83D\uDCE6 part-00006-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8613-1.c000.snappy.parquet    20.2 MB\n    \uD83D\uDCE6 part-00007-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8614-1.c000.snappy.parquet     8.1 MB\n    \uD83D\uDCE6 part-00008-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8615-1.c000.snappy.parquet    20.2 MB\n    \uD83D\uDCE6 part-00009-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8616-1.c000.snappy.parquet    20.9 MB\n    \uD83D\uDCE6 part-00010-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8617-1.c000.snappy.parquet    17.2 MB\n    \uD83D\uDCE6 part-00011-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8618-1.c000.snappy.parquet    19.2 MB\n    \uD83D\uDCE6 part-00012-tid-4510970318172860352-a77a51dd-b2a0-457c-9d46-3f1d97ca4067-8619-1.c000.snappy.parquet    20.2 MB\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_final_clean.parquet\n================================================================================\n  Files: 19, Subdirs: 0, Total Size: 241.3 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_1923300076755416352                                   1.6 KB\n    \uD83D\uDCC4 _started_1923300076755416352                                        0 B\n    \uD83D\uDCE6 part-00000-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6646-1.c000.snappy.parquet    20.1 MB\n    \uD83D\uDCE6 part-00001-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6647-1.c000.snappy.parquet    12.4 MB\n    \uD83D\uDCE6 part-00002-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6648-1.c000.snappy.parquet    17.2 MB\n    \uD83D\uDCE6 part-00003-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6649-1.c000.snappy.parquet    16.5 MB\n    \uD83D\uDCE6 part-00004-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6650-1.c000.snappy.parquet    17.9 MB\n    \uD83D\uDCE6 part-00005-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6651-1.c000.snappy.parquet    15.3 MB\n    \uD83D\uDCE6 part-00006-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6652-1.c000.snappy.parquet    17.7 MB\n    \uD83D\uDCE6 part-00007-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6653-1.c000.snappy.parquet    10.7 MB\n    \uD83D\uDCE6 part-00008-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6654-1.c000.snappy.parquet    18.3 MB\n    \uD83D\uDCE6 part-00009-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6655-1.c000.snappy.parquet    16.6 MB\n    \uD83D\uDCE6 part-00010-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6656-1.c000.snappy.parquet    18.9 MB\n    \uD83D\uDCE6 part-00011-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6657-1.c000.snappy.parquet    11.3 MB\n    \uD83D\uDCE6 part-00012-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6658-1.c000.snappy.parquet    10.5 MB\n    \uD83D\uDCE6 part-00013-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6659-1.c000.snappy.parquet    12.8 MB\n    \uD83D\uDCE6 part-00014-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6660-1.c000.snappy.parquet    14.1 MB\n    \uD83D\uDCE6 part-00015-tid-1923300076755416352-25a4d955-0ad6-4f6d-a67a-a4284ac1f709-6661-1.c000.snappy.parquet    11.0 MB\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_2015_final_clean.parquet\n================================================================================\n  Files: 14, Subdirs: 0, Total Size: 186.6 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_4803306999459019813                                   1.1 KB\n    \uD83D\uDCC4 _started_4803306999459019813                                        0 B\n    \uD83D\uDCE6 part-00000-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6270-1.c000.snappy.parquet    19.7 MB\n    \uD83D\uDCE6 part-00001-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6271-1.c000.snappy.parquet    19.9 MB\n    \uD83D\uDCE6 part-00002-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6272-1.c000.snappy.parquet    18.9 MB\n    \uD83D\uDCE6 part-00003-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6273-1.c000.snappy.parquet    18.0 MB\n    \uD83D\uDCE6 part-00004-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6274-1.c000.snappy.parquet    17.2 MB\n    \uD83D\uDCE6 part-00005-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6275-1.c000.snappy.parquet    17.6 MB\n    \uD83D\uDCE6 part-00006-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6276-1.c000.snappy.parquet    16.6 MB\n    \uD83D\uDCE6 part-00007-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6277-1.c000.snappy.parquet    16.3 MB\n    \uD83D\uDCE6 part-00008-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6278-1.c000.snappy.parquet    12.2 MB\n    \uD83D\uDCE6 part-00009-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6279-1.c000.snappy.parquet    17.6 MB\n    \uD83D\uDCE6 part-00010-tid-4803306999459019813-b068e416-01e6-4a43-93c3-7ea0ee57b660-6280-1.c000.snappy.parquet    12.4 MB\n\n================================================================================\n\uD83D\uDCC1 joined_3m_clean_imputed.parquet\n================================================================================\n  Files: 14, Subdirs: 0, Total Size: 95.7 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_4353081383368130900                                    933 B\n    \uD83D\uDCC4 _committed_5503753931250962352                                   1.8 KB\n    \uD83D\uDCC4 _started_4353081383368130900                                        0 B\n    \uD83D\uDCC4 _started_5503753931250962352                                        0 B\n    \uD83D\uDCE6 part-00000-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11418-1.c000.snappy.parquet    11.7 MB\n    \uD83D\uDCE6 part-00001-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11419-1.c000.snappy.parquet    11.8 MB\n    \uD83D\uDCE6 part-00002-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11420-1.c000.snappy.parquet    11.9 MB\n    \uD83D\uDCE6 part-00003-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11421-1.c000.snappy.parquet    11.7 MB\n    \uD83D\uDCE6 part-00004-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11422-1.c000.snappy.parquet    11.8 MB\n    \uD83D\uDCE6 part-00005-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11423-1.c000.snappy.parquet    11.7 MB\n    \uD83D\uDCE6 part-00006-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11424-1.c000.snappy.parquet    11.5 MB\n    \uD83D\uDCE6 part-00007-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11425-1.c000.snappy.parquet    11.9 MB\n    \uD83D\uDCE6 part-00008-tid-5503753931250962352-f4416007-9cd4-4a69-acf0-8f5582ceef56-11426-1.c000.snappy.parquet     1.6 MB\n\n================================================================================\n\uD83D\uDCC1 test_data_regression_scaled_2015\n================================================================================\n  Files: 27, Subdirs: 0, Total Size: 91.0 MB\n\n  Files:\n    \uD83D\uDCC4 _committed_2183565755248868542                                   4.0 KB\n    \uD83D\uDCC4 _committed_4432467752020416627                                   4.3 KB\n    \uD83D\uDCC4 _committed_8573367817191651793                                   2.5 KB\n    \uD83D\uDCC4 _started_4432467752020416627                                        0 B\n    \uD83D\uDCE6 part-00000-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2645-1.c000.snappy.parquet     3.6 MB\n    \uD83D\uDCE6 part-00001-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2642-1.c000.snappy.parquet     3.1 MB\n    \uD83D\uDCE6 part-00002-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2646-1.c000.snappy.parquet     4.3 MB\n    \uD83D\uDCE6 part-00003-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2643-1.c000.snappy.parquet     4.5 MB\n    \uD83D\uDCE6 part-00004-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2650-1.c000.snappy.parquet     2.8 MB\n    \uD83D\uDCE6 part-00005-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2647-1.c000.snappy.parquet     4.7 MB\n    \uD83D\uDCE6 part-00006-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2651-1.c000.snappy.parquet     3.7 MB\n    \uD83D\uDCE6 part-00007-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2648-1.c000.snappy.parquet     4.1 MB\n    \uD83D\uDCE6 part-00008-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2655-1.c000.snappy.parquet     3.8 MB\n    \uD83D\uDCE6 part-00009-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2652-1.c000.snappy.parquet     4.2 MB\n    \uD83D\uDCE6 part-00010-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2656-1.c000.snappy.parquet     3.8 MB\n    \uD83D\uDCE6 part-00011-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2653-1.c000.snappy.parquet     3.4 MB\n    \uD83D\uDCE6 part-00012-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2660-1.c000.snappy.parquet     4.4 MB\n    \uD83D\uDCE6 part-00013-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2661-1.c000.snappy.parquet     3.4 MB\n    \uD83D\uDCE6 part-00014-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2658-1.c000.snappy.parquet     7.6 MB\n    \uD83D\uDCE6 part-00015-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2657-1.c000.snappy.parquet     1.5 MB\n    \uD83D\uDCE6 part-00016-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2644-1.c000.snappy.parquet     5.0 MB\n    \uD83D\uDCE6 part-00017-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2649-1.c000.snappy.parquet     4.1 MB\n    \uD83D\uDCE6 part-00018-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2654-1.c000.snappy.parquet     4.6 MB\n    \uD83D\uDCE6 part-00019-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2659-1.c000.snappy.parquet     5.3 MB\n    \uD83D\uDCE6 part-00020-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2662-1.c000.snappy.parquet     3.5 MB\n    \uD83D\uDCE6 part-00021-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2663-1.c000.snappy.parquet     3.6 MB\n    \uD83D\uDCE6 part-00022-tid-4432467752020416627-bf0913bb-2761-4345-84cc-df92db3b90b8-2664-1.c000.snappy.parquet     2.3 MB\n\n================================================================================\n\uD83D\uDCC1 joined_3m_final_clean.parquet\n================================================================================\n  Files: 16, Subdirs: 0, Total Size: 78.5 MB\n\n  Files:\n    \uD83D\uDCC4 _SUCCESS                                                            0 B\n    \uD83D\uDCC4 _committed_7801359955208940763                                   1.3 KB\n    \uD83D\uDCC4 _started_7801359955208940763                                        0 B\n    \uD83D\uDCE6 part-00000-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12692-1.c000.snappy.parquet     6.1 MB\n    \uD83D\uDCE6 part-00001-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12688-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00002-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12689-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00003-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12685-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00004-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12686-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00005-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12691-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00006-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12687-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00007-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12684-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00008-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12681-1.c000.snappy.parquet     6.4 MB\n    \uD83D\uDCE6 part-00009-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12683-1.c000.snappy.parquet     6.3 MB\n    \uD83D\uDCE6 part-00010-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12682-1.c000.snappy.parquet     6.4 MB\n    \uD83D\uDCE6 part-00011-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12690-1.c000.snappy.parquet     6.2 MB\n    \uD83D\uDCE6 part-00012-tid-7801359955208940763-4c9dafdf-7094-484b-badb-f933276274e5-12693-1.c000.snappy.parquet     3.7 MB\n\n================================================================================\n\uD83D\uDCC1 charts\n================================================================================\n  Files: 37, Subdirs: 0, Total Size: 9.6 MB\n\n  Files:\n    \uD83D\uDDBC ML_pipeline_phase_2.png                                        171.9 KB\n    \uD83D\uDDBC ML_pipeline_phase_2_resized.png                                130.9 KB\n    \uD83D\uDDBC baseline_regression_cross_validation_result_visualization.png    41.5 KB\n    \uD83D\uDDBC carrier_performance_analysis.png                               989.5 KB\n    \uD83D\uDDBC carrier_performance_analysis_resized.png                       230.4 KB\n    \uD83D\uDDBC checkpoint1_missing_analysis.png                               614.8 KB\n    \uD83D\uDDBC checkpoint2_missing_analysis.png                               595.0 KB\n    \uD83D\uDDBC checkpoint3_feature_analysis.png                               115.7 KB\n    \uD83D\uDDBC checkpoint4_feature_analysis.png                               483.0 KB\n    \uD83D\uDDBC checkpoint5_final_analysis.png                                 711.3 KB\n    \uD83D\uDDBC checkpoint5_final_analysis_resized.png                         149.4 KB\n    \uD83D\uDDBC comprehensive_pipeline_analysis.png                            115.7 KB\n    \uD83D\uDDBC comprehensive_pipeline_analysis_resized.png                     96.5 KB\n    \uD83D\uDDBC cross_validation_versus_holdout_performance.png                 48.5 KB\n    \uD83D\uDDBC cumulative_feature_importance.png                               56.1 KB\n    \uD83D\uDDBC database_schema.png                                            213.6 KB\n    \uD83D\uDDBC database_schema_resized.png                                    134.9 KB\n    \uD83D\uDDBC feature_family_summary.png                                     367.5 KB\n    \uD83D\uDDBC feature_family_summary_resized.png                              82.1 KB\n    \uD83D\uDDBC final_verification_report.png                                  528.2 KB\n    \uD83D\uDDBC gannt_chart.png                                                625.7 KB\n    \uD83D\uDDBC gannt_chart_resized.png                                        240.8 KB\n    \uD83D\uDDBC geographic_patterns_analysis.png                               570.4 KB\n    \uD83D\uDDBC geographic_patterns_analysis_resized.png                       137.3 KB\n    \uD83D\uDDBC joins_diagram.png                                              203.2 KB\n    \uD83D\uDDBC joins_diagram_resized.png                                       61.2 KB\n    \uD83D\uDDBC leakage_diagram.png                                             67.6 KB\n    \uD83D\uDDBC leakage_diagram_resized.png                                     30.5 KB\n    \uD83D\uDDBC missing_data_before_after.png                                  275.2 KB\n    \uD83D\uDDBC missing_data_comprehensive_analysis.png                        578.2 KB\n    \uD83D\uDDBC missing_data_comprehensive_analysis_resized.png                125.5 KB\n    \uD83D\uDDBC regression_baseline_feature_Importance.png                     117.7 KB\n    \uD83D\uDDBC regression_feature_importance_distribution_across_models.png    34.7 KB\n    \uD83D\uDDBC regression_features_model_agreement.png                         72.4 KB\n    \uD83D\uDDBC regression_features_model_agreement_resized.png                 69.1 KB\n    \uD83D\uDDBC temporal_patterns_analysis.png                                 617.2 KB\n    \uD83D\uDDBC temporal_patterns_analysis_resized.png                         152.9 KB\n\n================================================================================\n\uD83D\uDCC1 Charts\n================================================================================\n  Files: 16, Subdirs: 2, Total Size: 839.7 KB\n\n  Subdirectories:\n    \uD83D\uDCC2 phase2/ (85 files, 19.6 MB)\n    \uD83D\uDCC2 phase2_eda/ (0 files, 0 B)\n\n  Files:\n    \uD83D\uDCF7 MIDS261_Final_Project_Phase2.jpg                                27.7 KB\n    \uD83D\uDCF7 MIDS261_Final_Project_Phase2_POC.jpg                            43.9 KB\n    \uD83D\uDDBC On-Time vs Delayed Flights (DEP_DEL15) 15 minutes delay or more.png    21.3 KB\n    \uD83D\uDDBC correlation_matrix.png                                         116.0 KB\n    \uD83D\uDDBC delay_rate_by_day_of_week.png                                   45.9 KB\n    \uD83D\uDDBC delay_rate_by_flight_volume_by_airline.png                      80.9 KB\n    \uD83D\uDDBC delay_rate_by_schedlued_departure_date.png                      33.4 KB\n    \uD83D\uDDBC delay_rate_by_visibility_bin.png                                17.4 KB\n    \uD83D\uDDBC delay_rate_vs_flight_volume_precipitation.png                   77.2 KB\n    \uD83D\uDDBC delay_rate_vs_flight_volume_temperature.png                     92.1 KB\n    \uD83D\uDDBC delay_rate_vs_flight_volume_wind_speed.png                      78.0 KB\n    \uD83D\uDDBC flight_count_by_operating_carrier.png                           22.1 KB\n    \uD83D\uDDBC missing_value_percentage_by_column.png                          19.4 KB\n    \uD83D\uDDBC top_20_airport_origin_by_flight_volume.png                      35.7 KB\n    \uD83D\uDDBC top_20_airports_by_delay_rate.png                               37.2 KB\n    \uD83D\uDDBC top_20_busiest_airports_sorted_by_delay_rate.png                91.7 KB\n\n================================================================================\n\uD83D\uDCC1 _checkpoints\n================================================================================\n  Files: 0, Subdirs: 1, Total Size: 0 B\n\n  Subdirectories:\n    \uD83D\uDCC2 a0c5b945-511e-4041-9717-1477bdedebc4/ (0 files, 0 B)\n\n\n================================================================================\n\uD83D\uDCC1 data_12M\n================================================================================\n  Files: 0, Subdirs: 11, Total Size: 0 B\n\n  Subdirectories:\n    \uD83D\uDCC2 df_joined_1Y_2015_baseline_test_data.parquet/ (38 files, 278.4 MB)\n    \uD83D\uDCC2 df_joined_1Y_2015_baseline_train_data.parquet/ (38 files, 863.3 MB)\n    \uD83D\uDCC2 df_joined_1Y_2015_clean.parquet/ (22 files, 255.2 MB)\n    \uD83D\uDCC2 df_joined_1Y_2015_features.parquet/ (20 files, 441.8 MB)\n    \uD83D\uDCC2 df_joined_1Y_2015_test_data.parquet/ (22 files, 163.8 MB)\n    \uD83D\uDCC2 df_joined_1Y_2015_train_data.parquet/ (22 files, 507.5 MB)\n    \uD83D\uDCC2 df_joined_1Y_features_plus_gf.parquet/ (13 files, 680.7 MB)\n    \uD83D\uDCC2 df_otpw_12M_features.parquet/ (12 files, 210.7 MB)\n    \uD83D\uDCC2 df_otpw_12M_test_data.parquet/ (14 files, 19.0 MB)\n    \uD83D\uDCC2 df_otpw_12M_train_data.parquet/ (14 files, 193.7 MB)\n    \uD83D\uDCC2 models/ (0 files, 0 B)\n\n\n================================================================================\n\uD83D\uDCC1 future_joins\n================================================================================\n  Files: 0, Subdirs: 2, Total Size: 0 B\n\n  Subdirectories:\n    \uD83D\uDCC2 BTS_OnTime/ (61 files, 13.16 GB)\n    \uD83D\uDCC2 NOAA/ (14439 files, 48.76 GB)\n\n\n================================================================================\n\uD83D\uDCC1 joined_1Y_feat_temp2.parquet\n================================================================================\n  Files: 1, Subdirs: 0, Total Size: 0 B\n\n  Files:\n    \uD83D\uDCC4 _started_4443791720215894344                                        0 B\n\n================================================================================\n\uD83D\uDCC1 mart\n================================================================================\n  Files: 0, Subdirs: 3, Total Size: 0 B\n\n  Subdirectories:\n    \uD83D\uDCC2 final_curated_intersect_otpw/ (4 files, 7.7 KB)\n    \uD83D\uDCC2 final_curated_not_in_otpw/ (26 files, 569.8 MB)\n    \uD83D\uDCC2 flights_asof_weather_full/ (0 files, 0 B)\n\n\n================================================================================\n\uD83D\uDCC1 plots_phase2\n================================================================================\n  (EMPTY - No files or subdirectories)\n\n================================================================================\n\uD83D\uDCC1 regression_results\n================================================================================\n  Files: 0, Subdirs: 1, Total Size: 0 B\n\n  Subdirectories:\n    \uD83D\uDCC2 phase2/ (1 files, 5.6 KB)\n\n\n================================================================================\n\uD83D\uDCC1 timeseries\n================================================================================\n  Files: 0, Subdirs: 5, Total Size: 0 B\n\n  Subdirectories:\n    \uD83D\uDCC2 ts_airport_carrier_hourly/ (2 files, 35 B)\n    \uD83D\uDCC2 ts_airport_carrier_hourly_base/ (1 files, 0 B)\n    \uD83D\uDCC2 ts_airport_hourly/ (2 files, 35 B)\n    \uD83D\uDCC2 ts_airport_hourly_base/ (1 files, 0 B)\n    \uD83D\uDCC2 wx_airport_hourly/ (2 files, 35 B)\n\n\n================================================================================\nCLEANUP CANDIDATES\n================================================================================\n\n EMPTY FOLDERS (1) - Safe to delete:\n  - plots_phase2\n\n  SMALL FOLDERS (8) - Less than 1 MB:\n  - Charts                                                       (839.7 KB)\n  - _checkpoints                                                 (0 B)\n  - data_12M                                                     (0 B)\n  - future_joins                                                 (0 B)\n  - joined_1Y_feat_temp2.parquet                                 (0 B)\n  - mart                                                         (0 B)\n  - regression_results                                           (0 B)\n  - timeseries                                                   (0 B)\n\n================================================================================\nCODE TO DELETE EMPTY FOLDERS\n================================================================================\n\nempty_folders = [\n    \"plots_phase2\",\n]\n\nfor folder in empty_folders:\n    path = f\"dbfs:/student-groups/Group_4_4/{folder}\"\n    try:\n        dbutils.fs.rm(path, recurse=True)\n        print(f\" Deleted: {folder}\")\n    except Exception as e:\n        print(f\" Failed: {folder} - {e}\")\n\n================================================================================\n ANALYSIS COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIST FILES AND SUBFILES WITHIN SPECIFIC FOLDERS\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DETAILED FOLDER CONTENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "folders_to_check = [\n",
    "    \"Charts\",\n",
    "    \"_checkpoints\",\n",
    "    \"charts\",\n",
    "    \"data_12M\",\n",
    "    \"df_joined_1Y_2015_clean.parquet\",\n",
    "    \"future_joins\",\n",
    "    \"joined_1Y_2015_features_clean.parquet\",\n",
    "    \"joined_1Y_2015_final_clean.parquet\",\n",
    "    \"joined_1Y_2015_final_feature_clean.parquet\",\n",
    "    \"joined_1Y_clean_imputed.parquet\",\n",
    "    \"joined_1Y_feat.parquet\",\n",
    "    \"joined_1Y_feat_2.parquet\",\n",
    "    \"joined_1Y_feat_temp.parquet\",\n",
    "    \"joined_1Y_feat_temp2.parquet\",\n",
    "    \"joined_1Y_feat_temp_2.parquet\",\n",
    "    \"joined_1Y_features_clean.parquet\",\n",
    "    \"joined_1Y_final_clean.parquet\",\n",
    "    \"joined_1Y_final_feature_clean.parquet\",\n",
    "    \"joined_1Y_final_feature_clean_with_removed_features\",\n",
    "    \"joined_3m_clean_imputed.parquet\",\n",
    "    \"joined_3m_final_clean.parquet\",\n",
    "    \"mart\",\n",
    "    \"plots_phase2\",\n",
    "    \"regression_results\",\n",
    "    \"temp2.parquet\",\n",
    "    \"temp3-4.parquet\",\n",
    "    \"test_data_regression_scaled_2015\",\n",
    "    \"timeseries\",\n",
    "    \"train_data_regression_scaled_2015\"\n",
    "]\n",
    "\n",
    "def get_size_str(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    elif size_bytes < 1024:\n",
    "        return f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024**2:\n",
    "        return f\"{size_bytes/1024:.1f} KB\"\n",
    "    elif size_bytes < 1024**3:\n",
    "        return f\"{size_bytes/(1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes/(1024**3):.2f} GB\"\n",
    "\n",
    "def list_folder_contents(folder_path, folder_name, indent=0):\n",
    "    \"\"\"List contents of a folder recursively\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    \n",
    "    try:\n",
    "        items = dbutils.fs.ls(folder_path)\n",
    "        \n",
    "        # Separate files and subdirectories\n",
    "        files = [item for item in items if not item.name.endswith('/')]\n",
    "        subdirs = [item for item in items if item.name.endswith('/')]\n",
    "        \n",
    "        # Sort both\n",
    "        files = sorted(files, key=lambda x: x.name)\n",
    "        subdirs = sorted(subdirs, key=lambda x: x.name)\n",
    "        \n",
    "        # Count and calculate size\n",
    "        total_size = 0\n",
    "        for f in files:\n",
    "            total_size = total_size + f.size\n",
    "        \n",
    "        # Return summary\n",
    "        return len(files), len(subdirs), total_size, files, subdirs\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0, 0, 0, [], []\n",
    "\n",
    "# ============================================================================\n",
    "# Analyze each folder\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nAnalyzing {len(folders_to_check)} folders...\\n\")\n",
    "\n",
    "folder_data = []\n",
    "total_all_size = 0\n",
    "\n",
    "for folder_name in folders_to_check:\n",
    "    folder_path = f\"{BASE_DIR}/{folder_name}\"\n",
    "    \n",
    "    num_files, num_subdirs, folder_size, files, subdirs = list_folder_contents(folder_path, folder_name)\n",
    "    \n",
    "    total_all_size = total_all_size + folder_size\n",
    "    \n",
    "    folder_data.append({\n",
    "        'name': folder_name,\n",
    "        'path': folder_path,\n",
    "        'num_files': num_files,\n",
    "        'num_subdirs': num_subdirs,\n",
    "        'size': folder_size,\n",
    "        'files': files,\n",
    "        'subdirs': subdirs\n",
    "    })\n",
    "\n",
    "# ============================================================================\n",
    "# Display Summary Table\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Sort by size (largest first)\n",
    "folder_data_sorted = sorted(folder_data, key=lambda x: x['size'], reverse=True)\n",
    "\n",
    "print(f\"{'#':<3} {'Folder Name':<65} {'Files':<6} {'Subdirs':<8} {'Size':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, folder in enumerate(folder_data_sorted, 1):\n",
    "    size_str = get_size_str(folder['size'])\n",
    "    \n",
    "    if folder['num_files'] == 0 and folder['num_subdirs'] == 0:\n",
    "        status = \" (EMPTY)\"\n",
    "    else:\n",
    "        status = \"\"\n",
    "    \n",
    "    print(f\"{i:<3} {folder['name']:<65} {folder['num_files']:<6} {folder['num_subdirs']:<8} {size_str:<12}{status}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'TOTAL':<69} {get_size_str(total_all_size):<12}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Detailed Contents of Each Folder\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CONTENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for folder in folder_data_sorted:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\uD83D\uDCC1 {folder['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if folder['num_files'] == 0 and folder['num_subdirs'] == 0:\n",
    "        print(\"  (EMPTY - No files or subdirectories)\")\n",
    "        continue\n",
    "    \n",
    "    size_str = get_size_str(folder['size'])\n",
    "    print(f\"  Files: {folder['num_files']}, Subdirs: {folder['num_subdirs']}, Total Size: {size_str}\")\n",
    "    print()\n",
    "    \n",
    "    # Show subdirectories first (if any)\n",
    "    if folder['subdirs']:\n",
    "        print(\"  Subdirectories:\")\n",
    "        for subdir in folder['subdirs']:\n",
    "            subdir_name = subdir.name.rstrip('/')\n",
    "            \n",
    "            # Try to get info about subdirectory\n",
    "            try:\n",
    "                sub_contents = dbutils.fs.ls(subdir.path)\n",
    "                sub_files = [x for x in sub_contents if not x.name.endswith('/')]\n",
    "                sub_size = 0\n",
    "                for sf in sub_files:\n",
    "                    sub_size = sub_size + sf.size\n",
    "                \n",
    "                print(f\"    \uD83D\uDCC2 {subdir_name}/ ({len(sub_files)} files, {get_size_str(sub_size)})\")\n",
    "            except:\n",
    "                print(f\"    \uD83D\uDCC2 {subdir_name}/ (error reading)\")\n",
    "        print()\n",
    "    \n",
    "    # Show files\n",
    "    if folder['files']:\n",
    "        print(\"  Files:\")\n",
    "        \n",
    "        # Limit to first 50 files to avoid overwhelming output\n",
    "        display_files = folder['files'][:50]\n",
    "        \n",
    "        for f in display_files:\n",
    "            size_str = get_size_str(f.size)\n",
    "            \n",
    "            # Determine file type\n",
    "            if f.name.endswith('.parquet'):\n",
    "                icon = \"\uD83D\uDCE6\"\n",
    "            elif f.name.endswith('.png'):\n",
    "                icon = \"\uD83D\uDDBC\"\n",
    "            elif f.name.endswith('.jpg') or f.name.endswith('.jpeg'):\n",
    "                icon = \"\uD83D\uDCF7\"\n",
    "            elif f.name.endswith('.csv'):\n",
    "                icon = \"\uD83D\uDCCA\"\n",
    "            elif f.name.endswith('.txt'):\n",
    "                icon = \"\uD83D\uDCC4\"\n",
    "            else:\n",
    "                icon = \"\uD83D\uDCC4\"\n",
    "            \n",
    "            print(f\"    {icon} {f.name:<60} {size_str:>10}\")\n",
    "        \n",
    "        if len(folder['files']) > 50:\n",
    "            print(f\"    ... and {len(folder['files']) - 50} more files\")\n",
    "\n",
    "# ============================================================================\n",
    "# Identify Empty or Unnecessary Folders\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP CANDIDATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "empty_folders = []\n",
    "small_folders = []  # < 1 MB\n",
    "\n",
    "for folder in folder_data:\n",
    "    if folder['num_files'] == 0 and folder['num_subdirs'] == 0:\n",
    "        empty_folders.append(folder['name'])\n",
    "    elif folder['size'] < 1024 * 1024:  # Less than 1 MB\n",
    "        small_folders.append((folder['name'], folder['size']))\n",
    "\n",
    "if empty_folders:\n",
    "    print(f\"\\n EMPTY FOLDERS ({len(empty_folders)}) - Safe to delete:\")\n",
    "    for name in empty_folders:\n",
    "        print(f\"  - {name}\")\n",
    "else:\n",
    "    print(\"\\n No empty folders found\")\n",
    "\n",
    "if small_folders:\n",
    "    print(f\"\\n  SMALL FOLDERS ({len(small_folders)}) - Less than 1 MB:\")\n",
    "    for name, size in small_folders:\n",
    "        print(f\"  - {name:<60} ({get_size_str(size)})\")\n",
    "else:\n",
    "    print(\"\\n No folders under 1 MB\")\n",
    "\n",
    "# ============================================================================\n",
    "# Generate deletion code for empty folders\n",
    "# ============================================================================\n",
    "if empty_folders:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CODE TO DELETE EMPTY FOLDERS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"empty_folders = [\")\n",
    "    for name in empty_folders:\n",
    "        print(f'    \"{name}\",')\n",
    "    print(\"]\")\n",
    "    print()\n",
    "    print(\"for folder in empty_folders:\")\n",
    "    print('    path = f\"dbfs:/student-groups/Group_4_4/{folder}\"')\n",
    "    print(\"    try:\")\n",
    "    print(\"        dbutils.fs.rm(path, recurse=True)\")\n",
    "    print(f'        print(f\" Deleted: {{folder}}\")')\n",
    "    print(\"    except Exception as e:\")\n",
    "    print(f'        print(f\" Failed: {{folder}} - {{e}}\")')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39699ef7-cdb5-4ea1-a2a1-b4ddf71706bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nDATABRICKS FOLDER CLEANUP\n================================================================================\n\nBase Directory: dbfs:/student-groups/Group_4_4\nFiles to remove: 15\n\n================================================================================\nSTEP 1: Verifying files to be deleted\n================================================================================\n Not found: joined_1Y_feat.parquet\n Not found: joined_1Y_feat_2.parquet\n Not found: joined_1Y_feat_temp.parquet\n Not found: joined_1Y_feat_temp2.parquet\n Not found: joined_1Y_feat_temp_2.parquet\n Not found: joined_1Y_features_clean.parquet\n Not found: joined_1Y_final_clean.parquet\n Not found: joined_1Y_final_feature_clean.parquet\n Not found: joined_3m_clean_imputed.parquet\n Not found: temp2.parquet\n Not found: temp3-4.parquet\n Not found: joined_1Y_clean_imputed.parquet\n Not found: joined_1Y_2015_features_clean.parquet\n Not found: joined_1Y_2015_final_clean.parquet\n Not found: joined_1Y_2015_final_feature_clean.parquet\n\n--------------------------------------------------------------------------------\nFiles found: 0/15\nFiles not found: 15\nTotal size to be freed: 0.00 GB\n--------------------------------------------------------------------------------\n\n================================================================================\nSTEP 2: Confirmation\n================================================================================\n\n  No files found to delete. Exiting.\n\n================================================================================\nSTEP 3: Deleting files\n================================================================================\n\n================================================================================\nCLEANUP SUMMARY\n================================================================================\n\n Successfully deleted: 0 directories\n Failed to delete: 0 directories\n\uD83D\uDCE6 Total space freed: 0.00 GB\n\n  Files not found (already deleted?): 15\n    - joined_1Y_feat.parquet\n    - joined_1Y_feat_2.parquet\n    - joined_1Y_feat_temp.parquet\n    - joined_1Y_feat_temp2.parquet\n    - joined_1Y_feat_temp_2.parquet\n    ... and 10 more\n\n================================================================================\nSTEP 5: Remaining files in directory\n================================================================================\n\nRemaining parquet directories: 36\n\nFirst 20 remaining files:\n 1. JOINED_1Y.parquet/                                           (  0.00 GB)\n 2. JOINED_1Y_2015.parquet/                                      (  0.00 GB)\n 3. JOINED_1Y_2019.parquet/                                      (  0.00 GB)\n 4. JOINED_3M.parquet/                                           (  0.00 GB)\n 5. JOINED_3M_2015.parquet/                                      (  0.00 GB)\n 6. JOINED_5Y_2015_2019.parquet/                                 (  0.00 GB)\n 7. checkpoint_1_initial_joined_2015.parquet/                    (  0.00 GB)\n 8. checkpoint_1_initial_joined_2019.parquet/                    (  0.00 GB)\n 9. checkpoint_2_cleaned_imputed_2015.parquet/                   (  0.00 GB)\n10. checkpoint_3_basic_features_2015.parquet/                    (  0.00 GB)\n11. checkpoint_3_basic_features_2019.parquet/                    (  0.00 GB)\n12. checkpoint_4_advanced_features_2015.parquet/                 (  0.00 GB)\n13. checkpoint_5_final_clean_2015.parquet/                       (  0.00 GB)\n14. checkpoint_5a_final_data_with_all_features_2015.parquet/     (  0.00 GB)\n15. checkpoint_6_regression_features_baseline_2015.parquet/      (  0.00 GB)\n16. checkpoint_7_regression_features_post_baseline_2015.parquet/ (  0.00 GB)\n17. df_joined_1Y_2015_clean.parquet/                             (  0.00 GB)\n18. df_otpw_12M_clean.parquet/                                   (  0.00 GB)\n19. df_otpw_3m.parquet/                                          (  0.00 GB)\n20. df_weather.parquet/                                          (  0.00 GB)\n\n... and 16 more files\n\n================================================================================\n CLEANUP COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP SCRIPT: Remove Temporary/Intermediate Files\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "# Files/directories to remove\n",
    "files_to_remove = [\n",
    "    # Intermediate feature versions\n",
    "    \"joined_1Y_feat.parquet\",\n",
    "    \"joined_1Y_feat_2.parquet\",\n",
    "    \"joined_1Y_feat_temp.parquet\",\n",
    "    \"joined_1Y_feat_temp2.parquet\",\n",
    "    \"joined_1Y_feat_temp_2.parquet\",\n",
    "    \"joined_1Y_features_clean.parquet\",\n",
    "    \"joined_1Y_final_clean.parquet\",\n",
    "    \"joined_1Y_final_feature_clean.parquet\",\n",
    "    \n",
    "    # 3-month data (not used in final)\n",
    "    \"joined_3m_clean_imputed.parquet\",\n",
    "    \n",
    "    # Temp files\n",
    "    \"temp2.parquet\",\n",
    "    \"temp3-4.parquet\",\n",
    "    \n",
    "    # Legacy files (missing year suffix)\n",
    "    \"joined_1Y_clean_imputed.parquet\",\n",
    "    \"joined_1Y_2015_features_clean.parquet\",\n",
    "    \"joined_1Y_2015_final_clean.parquet\",\n",
    "    \"joined_1Y_2015_final_feature_clean.parquet\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATABRICKS FOLDER CLEANUP\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBase Directory: {BASE_DIR}\")\n",
    "print(f\"Files to remove: {len(files_to_remove)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Verify files exist and get sizes\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: Verifying files to be deleted\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "files_found = []\n",
    "files_not_found = []\n",
    "total_size_bytes = 0\n",
    "\n",
    "for file_name in files_to_remove:\n",
    "    file_path = f\"{BASE_DIR}/{file_name}/\"\n",
    "    try:\n",
    "        # Check if file/directory exists\n",
    "        files = dbutils.fs.ls(file_path)\n",
    "        \n",
    "        # Calculate size\n",
    "        size = sum([f.size for f in files if f.name.endswith('.parquet')])\n",
    "        size_gb = size / (1024**3)\n",
    "        \n",
    "        files_found.append({\n",
    "            'name': file_name,\n",
    "            'path': file_path,\n",
    "            'size_gb': size_gb,\n",
    "            'size_bytes': size\n",
    "        })\n",
    "        total_size_bytes += size\n",
    "        \n",
    "        print(f\" Found: {file_name:50s} ({size_gb:>6.2f} GB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        files_not_found.append(file_name)\n",
    "        print(f\" Not found: {file_name}\")\n",
    "\n",
    "total_size_gb = total_size_bytes / (1024**3)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"Files found: {len(files_found)}/{len(files_to_remove)}\")\n",
    "print(f\"Files not found: {len(files_not_found)}\")\n",
    "print(f\"Total size to be freed: {total_size_gb:.2f} GB\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Confirmation prompt\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Confirmation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(files_found) == 0:\n",
    "    print(\"\\n  No files found to delete. Exiting.\")\n",
    "else:\n",
    "    print(f\"\\n  You are about to delete {len(files_found)} directories\")\n",
    "    print(f\"  This will free up approximately {total_size_gb:.2f} GB\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Uncomment the line below if you want to require manual confirmation\n",
    "    # confirm = input(\"Type 'DELETE' to proceed: \")\n",
    "    # if confirm != \"DELETE\":\n",
    "    #     print(\" Deletion cancelled\")\n",
    "    #     raise Exception(\"User cancelled deletion\")\n",
    "    \n",
    "    print(\" Proceeding with deletion...\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Delete files\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Deleting files\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "deleted_count = 0\n",
    "failed_count = 0\n",
    "deleted_size = 0\n",
    "\n",
    "for file_info in files_found:\n",
    "    try:\n",
    "        print(f\"\\nDeleting: {file_info['name']}\")\n",
    "        dbutils.fs.rm(file_info['path'], recurse=True)\n",
    "        deleted_count += 1\n",
    "        deleted_size += file_info['size_bytes']\n",
    "        print(f\"   Deleted successfully ({file_info['size_gb']:.2f} GB freed)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "        print(f\"   Failed to delete: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n Successfully deleted: {deleted_count} directories\")\n",
    "print(f\" Failed to delete: {failed_count} directories\")\n",
    "print(f\"\uD83D\uDCE6 Total space freed: {deleted_size / (1024**3):.2f} GB\")\n",
    "\n",
    "if files_not_found:\n",
    "    print(f\"\\n  Files not found (already deleted?): {len(files_not_found)}\")\n",
    "    for fname in files_not_found[:5]:  # Show first 5\n",
    "        print(f\"    - {fname}\")\n",
    "    if len(files_not_found) > 5:\n",
    "        print(f\"    ... and {len(files_not_found) - 5} more\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Show remaining files in directory\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: Remaining files in directory\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "remaining_files = dbutils.fs.ls(BASE_DIR)\n",
    "remaining_parquet = [f for f in remaining_files if f.name.endswith('.parquet/')]\n",
    "\n",
    "print(f\"\\nRemaining parquet directories: {len(remaining_parquet)}\")\n",
    "print(\"\\nFirst 20 remaining files:\")\n",
    "for i, f in enumerate(remaining_parquet[:20], 1):\n",
    "    size_gb = f.size / (1024**3) if f.size > 0 else 0\n",
    "    print(f\"{i:2d}. {f.name:60s} ({size_gb:>6.2f} GB)\")\n",
    "\n",
    "if len(remaining_parquet) > 20:\n",
    "    print(f\"\\n... and {len(remaining_parquet) - 20} more files\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" CLEANUP COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd9db5e-76b0-4465-8597-d6797f4c4fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nANALYZING REMAINING FILES\n================================================================================\n\nTotal parquet directories: 36\n\n================================================================================\nFILE SIZES\n================================================================================\n\nAll files (sorted by size):\n\n 1. JOINED_5Y_2015_2019.parquet                                            2.21 GB\n 2. checkpoint_4_advanced_features_2015.parquet                            1.13 GB\n 3. df_weather.parquet                                                     1.07 GB\n 4. joined_1Y_feat_2.parquet                                             793.43 MB\n 5. checkpoint_5a_final_data_with_all_features_2015.parquet              611.88 MB\n 6. checkpoint_5_final_clean_2015.parquet                                611.88 MB\n 7. joined_1Y_feat.parquet                                               521.75 MB\n 8. JOINED_1Y.parquet                                                    489.29 MB\n 9. JOINED_1Y_2019.parquet                                               489.22 MB\n10. temp2.parquet                                                        479.33 MB\n11. joined_1Y_final_feature_clean.parquet                                408.80 MB\n12. temp3-4.parquet                                                      404.48 MB\n13. checkpoint_1_initial_joined_2019.parquet                             402.77 MB\n14. checkpoint_1_initial_joined_2015.parquet                             390.91 MB\n15. JOINED_1Y_2015.parquet                                               389.54 MB\n16. joined_1Y_feat_temp.parquet                                          356.24 MB\n17. joined_1Y_feat_temp_2.parquet                                        352.39 MB\n18. joined_1Y_features_clean.parquet                                     343.33 MB\n19. joined_1Y_2015_final_feature_clean.parquet                           315.98 MB\n20. joined_1Y_2015_features_clean.parquet                                273.19 MB\n21. checkpoint_3_basic_features_2015.parquet                             268.79 MB\n22. checkpoint_3_basic_features_2019.parquet                             268.75 MB\n23. joined_1Y_clean_imputed.parquet                                      265.35 MB\n24. df_joined_1Y_2015_clean.parquet                                      250.35 MB\n25. joined_1Y_final_clean.parquet                                        241.28 MB\n26. checkpoint_2_cleaned_imputed_2015.parquet                            193.63 MB\n27. joined_1Y_2015_final_clean.parquet                                   186.57 MB\n28. df_otpw_12M_clean.parquet                                            116.96 MB\n29. df_otpw_3m.parquet                                                   108.16 MB\n30. joined_3m_clean_imputed.parquet                                       95.66 MB\n31. JOINED_3M_2015.parquet                                                86.74 MB\n32. JOINED_3M.parquet                                                     86.68 MB\n33. joined_3m_final_clean.parquet                                         78.51 MB\n34. checkpoint_6_regression_features_baseline_2015.parquet                 0.00 MB\n35. checkpoint_7_regression_features_post_baseline_2015.parquet            0.00 MB\n36. joined_1Y_feat_temp2.parquet                                           0.00 MB\n--------------------------------------------------------------------------------\nTOTAL                                                                  14.06 GB\n\n================================================================================\nCATEGORIZATION\n================================================================================\n\n KEEP (Essential - 7 files expected):\n--------------------------------------------------------------------------------\n  KEEP TOTAL:                                                           0.00 GB\n\n CAN REMOVE (36 files):\n--------------------------------------------------------------------------------\n  JOINED_5Y_2015_2019.parquet                                           2.21 GB\n  checkpoint_4_advanced_features_2015.parquet                           1.13 GB\n  df_weather.parquet                                                    1.07 GB\n  joined_1Y_feat_2.parquet                                            793.43 MB\n  checkpoint_5a_final_data_with_all_features_2015.parquet             611.88 MB\n  checkpoint_5_final_clean_2015.parquet                               611.88 MB\n  joined_1Y_feat.parquet                                              521.75 MB\n  JOINED_1Y.parquet                                                   489.29 MB\n  JOINED_1Y_2019.parquet                                              489.22 MB\n  temp2.parquet                                                       479.33 MB\n  joined_1Y_final_feature_clean.parquet                               408.80 MB\n  temp3-4.parquet                                                     404.48 MB\n  checkpoint_1_initial_joined_2019.parquet                            402.77 MB\n  checkpoint_1_initial_joined_2015.parquet                            390.91 MB\n  JOINED_1Y_2015.parquet                                              389.54 MB\n  joined_1Y_feat_temp.parquet                                         356.24 MB\n  joined_1Y_feat_temp_2.parquet                                       352.39 MB\n  joined_1Y_features_clean.parquet                                    343.33 MB\n  joined_1Y_2015_final_feature_clean.parquet                          315.98 MB\n  joined_1Y_2015_features_clean.parquet                               273.19 MB\n  checkpoint_3_basic_features_2015.parquet                            268.79 MB\n  checkpoint_3_basic_features_2019.parquet                            268.75 MB\n  joined_1Y_clean_imputed.parquet                                     265.35 MB\n  df_joined_1Y_2015_clean.parquet                                     250.35 MB\n  joined_1Y_final_clean.parquet                                       241.28 MB\n  checkpoint_2_cleaned_imputed_2015.parquet                           193.63 MB\n  joined_1Y_2015_final_clean.parquet                                  186.57 MB\n  df_otpw_12M_clean.parquet                                           116.96 MB\n  df_otpw_3m.parquet                                                  108.16 MB\n  joined_3m_clean_imputed.parquet                                      95.66 MB\n  JOINED_3M_2015.parquet                                               86.74 MB\n  JOINED_3M.parquet                                                    86.68 MB\n  joined_3m_final_clean.parquet                                        78.51 MB\n  checkpoint_6_regression_features_baseline_2015.parquet                0.00 MB\n  checkpoint_7_regression_features_post_baseline_2015.parquet           0.00 MB\n  joined_1Y_feat_temp2.parquet                                          0.00 MB\n  REMOVE TOTAL (space to free):                                        14.06 GB\n\n================================================================================\nDELETION CODE\n================================================================================\n\n# Copy and run this to delete files:\n\nfiles_to_delete = [\n    \"JOINED_5Y_2015_2019.parquet\",  # 2.21 GB\n    \"checkpoint_4_advanced_features_2015.parquet\",  # 1.13 GB\n    \"df_weather.parquet\",  # 1.07 GB\n    \"joined_1Y_feat_2.parquet\",  # 793.43 MB\n    \"checkpoint_5a_final_data_with_all_features_2015.parquet\",  # 611.88 MB\n    \"checkpoint_5_final_clean_2015.parquet\",  # 611.88 MB\n    \"joined_1Y_feat.parquet\",  # 521.75 MB\n    \"JOINED_1Y.parquet\",  # 489.29 MB\n    \"JOINED_1Y_2019.parquet\",  # 489.22 MB\n    \"temp2.parquet\",  # 479.33 MB\n    \"joined_1Y_final_feature_clean.parquet\",  # 408.80 MB\n    \"temp3-4.parquet\",  # 404.48 MB\n    \"checkpoint_1_initial_joined_2019.parquet\",  # 402.77 MB\n    \"checkpoint_1_initial_joined_2015.parquet\",  # 390.91 MB\n    \"JOINED_1Y_2015.parquet\",  # 389.54 MB\n    \"joined_1Y_feat_temp.parquet\",  # 356.24 MB\n    \"joined_1Y_feat_temp_2.parquet\",  # 352.39 MB\n    \"joined_1Y_features_clean.parquet\",  # 343.33 MB\n    \"joined_1Y_2015_final_feature_clean.parquet\",  # 315.98 MB\n    \"joined_1Y_2015_features_clean.parquet\",  # 273.19 MB\n    \"checkpoint_3_basic_features_2015.parquet\",  # 268.79 MB\n    \"checkpoint_3_basic_features_2019.parquet\",  # 268.75 MB\n    \"joined_1Y_clean_imputed.parquet\",  # 265.35 MB\n    \"df_joined_1Y_2015_clean.parquet\",  # 250.35 MB\n    \"joined_1Y_final_clean.parquet\",  # 241.28 MB\n    \"checkpoint_2_cleaned_imputed_2015.parquet\",  # 193.63 MB\n    \"joined_1Y_2015_final_clean.parquet\",  # 186.57 MB\n    \"df_otpw_12M_clean.parquet\",  # 116.96 MB\n    \"df_otpw_3m.parquet\",  # 108.16 MB\n    \"joined_3m_clean_imputed.parquet\",  # 95.66 MB\n    \"JOINED_3M_2015.parquet\",  # 86.74 MB\n    \"JOINED_3M.parquet\",  # 86.68 MB\n    \"joined_3m_final_clean.parquet\",  # 78.51 MB\n    \"checkpoint_6_regression_features_baseline_2015.parquet\",  # 0.00 MB\n    \"checkpoint_7_regression_features_post_baseline_2015.parquet\",  # 0.00 MB\n    \"joined_1Y_feat_temp2.parquet\",  # 0.00 MB\n]\n\ndeleted_count = 0\nfreed_bytes = 0\nfor file_name in files_to_delete:\n    try:\n        path = f\"dbfs:/student-groups/Group_4_4/{file_name}.parquet\"\n        dbutils.fs.rm(path, recurse=True)\n        print(f\" Deleted: {file_name}\")\n        deleted_count = deleted_count + 1\n    except Exception as e:\n        print(f\" Failed: {file_name} - {e}\")\n\nprint(f\"\\n Deleted {deleted_count} directories\")\n\n================================================================================\n ANALYSIS COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SIMPLE FILE SIZE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "joined_BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING REMAINING FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all parquet directories\n",
    "all_files = dbutils.fs.ls(joined_BASE_DIR)\n",
    "parquet_dirs = [f for f in all_files if f.name.endswith('.parquet/')]\n",
    "\n",
    "print(f\"\\nTotal parquet directories: {len(parquet_dirs)}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Get sizes for each directory\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"FILE SIZES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "file_data = []\n",
    "\n",
    "for directory in parquet_dirs:\n",
    "    dir_name = directory.name.rstrip('/')\n",
    "    \n",
    "    try:\n",
    "        # Get contents of directory\n",
    "        contents = dbutils.fs.ls(directory.path)\n",
    "        \n",
    "        # Calculate total size manually\n",
    "        total_bytes = 0\n",
    "        for item in contents:\n",
    "            if not item.name.endswith('/'):  # It's a file, not a subdirectory\n",
    "                total_bytes = total_bytes + item.size\n",
    "        \n",
    "        # Convert to GB and MB\n",
    "        size_gb = total_bytes / (1024 * 1024 * 1024)\n",
    "        size_mb = total_bytes / (1024 * 1024)\n",
    "        \n",
    "        # Store as tuple to avoid PySpark issues\n",
    "        file_data.append((dir_name, total_bytes, size_gb, size_mb))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {dir_name}: {str(e)}\")\n",
    "        file_data.append((dir_name, 0, 0, 0))\n",
    "\n",
    "# Sort by size (largest first) - manual sort to avoid PySpark\n",
    "file_data_sorted = sorted(file_data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display\n",
    "print(\"\\nAll files (sorted by size):\\n\")\n",
    "total_storage_bytes = 0\n",
    "\n",
    "for i, (name, size_bytes, size_gb, size_mb) in enumerate(file_data_sorted, 1):\n",
    "    total_storage_bytes = total_storage_bytes + size_bytes\n",
    "    \n",
    "    if size_gb > 1:\n",
    "        size_display = f\"{size_gb:.2f} GB\"\n",
    "    else:\n",
    "        size_display = f\"{size_mb:.2f} MB\"\n",
    "    \n",
    "    print(f\"{i:2d}. {name:65s} {size_display:>12s}\")\n",
    "\n",
    "total_gb = total_storage_bytes / (1024 * 1024 * 1024)\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TOTAL':67s} {total_gb:>8.2f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# Categorize files\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "keep = []\n",
    "remove = []\n",
    "\n",
    "for name, size_bytes, size_gb, size_mb in file_data_sorted:\n",
    "    \n",
    "    # Files to KEEP\n",
    "    if name in ['checkpoint_5_final_clean_2015', \n",
    "                'checkpoint_5a_final_data_with_all_features_2015',\n",
    "                'checkpoint_6_regression_features_baseline_2015',\n",
    "                'checkpoint_7_regression_features_post_baseline_2015',\n",
    "                'JOINED_1Y_2015', \n",
    "                'JOINED_1Y_2019',\n",
    "                '2015_final_feature_engineered_data_with_dep_delay']:\n",
    "        keep.append((name, size_bytes, size_gb, size_mb))\n",
    "    \n",
    "    # Files to REMOVE\n",
    "    else:\n",
    "        remove.append((name, size_bytes, size_gb, size_mb))\n",
    "\n",
    "# Display KEEP files\n",
    "print(\"\\n KEEP (Essential - 7 files expected):\")\n",
    "print(\"-\" * 80)\n",
    "keep_total = 0\n",
    "for name, size_bytes, size_gb, size_mb in keep:\n",
    "    keep_total = keep_total + size_bytes\n",
    "    if size_gb > 1:\n",
    "        print(f\"  {name:65s} {size_gb:>8.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  {name:65s} {size_mb:>8.2f} MB\")\n",
    "print(f\"  {'KEEP TOTAL:':65s} {keep_total/(1024**3):>8.2f} GB\")\n",
    "\n",
    "# Display REMOVE files\n",
    "print(\"\\n CAN REMOVE ({} files):\".format(len(remove)))\n",
    "print(\"-\" * 80)\n",
    "remove_total = 0\n",
    "for name, size_bytes, size_gb, size_mb in remove:\n",
    "    remove_total = remove_total + size_bytes\n",
    "    if size_gb > 1:\n",
    "        print(f\"  {name:65s} {size_gb:>8.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  {name:65s} {size_mb:>8.2f} MB\")\n",
    "print(f\"  {'REMOVE TOTAL (space to free):':65s} {remove_total/(1024**3):>8.2f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# Generate deletion list\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DELETION CODE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n# Copy and run this to delete files:\\n\")\n",
    "print(\"files_to_delete = [\")\n",
    "for name, size_bytes, size_gb, size_mb in remove:\n",
    "    if size_gb > 1:\n",
    "        comment = f\"{size_gb:.2f} GB\"\n",
    "    else:\n",
    "        comment = f\"{size_mb:.2f} MB\"\n",
    "    print(f'    \"{name}\",  # {comment}')\n",
    "print(\"]\\n\")\n",
    "\n",
    "print(\"deleted_count = 0\")\n",
    "print(\"freed_bytes = 0\")\n",
    "print(\"for file_name in files_to_delete:\")\n",
    "print(\"    try:\")\n",
    "print('        path = f\"dbfs:/student-groups/Group_4_4/{file_name}.parquet\"')\n",
    "print(\"        dbutils.fs.rm(path, recurse=True)\")\n",
    "print('        print(f\" Deleted: {file_name}\")')\n",
    "print(\"        deleted_count = deleted_count + 1\")\n",
    "print(\"    except Exception as e:\")\n",
    "print('        print(f\" Failed: {file_name} - {e}\")')\n",
    "print(\"\")\n",
    "print('print(f\"\\\\n Deleted {deleted_count} directories\")')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996db6b5-656d-4b24-8af4-d0b0fd6d8b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deleted: joined_1Y_feat_2.parquet\n Deleted: joined_1Y_feat.parquet\n Deleted: temp2.parquet\n Deleted: joined_1Y_final_feature_clean.parquet\n Deleted: temp3-4.parquet\n Deleted: joined_1Y_feat_temp.parquet\n Deleted: joined_1Y_feat_temp_2.parquet\n Deleted: joined_1Y_features_clean.parquet\n Deleted: joined_1Y_2015_final_feature_clean.parquet\n Deleted: joined_1Y_2015_features_clean.parquet\n Deleted: joined_1Y_clean_imputed.parquet\n Deleted: joined_1Y_final_clean.parquet\n Deleted: joined_1Y_2015_final_clean.parquet\n Deleted: joined_1Y_feat_temp2.parquet\n\n Deleted 14 directories\n"
     ]
    }
   ],
   "source": [
    "files_to_delete = [\n",
    "    \"joined_1Y_feat_2.parquet\",  # 793.43 MB\n",
    "    \"joined_1Y_feat.parquet\",  # 521.75 MB\n",
    "    \"temp2.parquet\",  # 479.33 MB\n",
    "    \"joined_1Y_final_feature_clean.parquet\",  # 408.80 MB\n",
    "    \"temp3-4.parquet\",  # 404.48 MB\n",
    "    \"joined_1Y_feat_temp.parquet\",  # 356.24 MB\n",
    "    \"joined_1Y_feat_temp_2.parquet\",  # 352.39 MB\n",
    "    \"joined_1Y_features_clean.parquet\",  # 343.33 MB\n",
    "    \"joined_1Y_2015_final_feature_clean.parquet\",  # 315.98 MB\n",
    "    \"joined_1Y_2015_features_clean.parquet\",  # 273.19 MB\n",
    "    \"joined_1Y_clean_imputed.parquet\",  # 265.35 MB\n",
    "    \"joined_1Y_final_clean.parquet\",  # 241.28 MB\n",
    "    \"joined_1Y_2015_final_clean.parquet\",  # 186.57 MB\n",
    "    \"joined_1Y_feat_temp2.parquet\",  # 0.00 MB\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "freed_bytes = 0\n",
    "for file_name in files_to_delete:\n",
    "    try:\n",
    "        path = f\"dbfs:/student-groups/Group_4_4/{file_name}.parquet\"\n",
    "        dbutils.fs.rm(path, recurse=True)\n",
    "        print(f\" Deleted: {file_name}\")\n",
    "        deleted_count = deleted_count + 1\n",
    "    except Exception as e:\n",
    "        print(f\" Failed: {file_name} - {e}\")\n",
    "\n",
    "print(f\"\\n Deleted {deleted_count} directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed79ae44-eda1-440e-baad-ce3539154164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nMOVING PNG FILES TO Charts FOLDER\n================================================================================\n\nStep 1: Checking Charts directory...\n Charts directory exists: dbfs:/student-groups/Group_4_4/charts\n\nStep 2: Moving 37 PNG files...\n--------------------------------------------------------------------------------\n   Moved: leakage_diagram_resized.png\n   Moved: regression_feature_importance_distribution_across_models.png\n   Moved: baseline_regression_cross_validation_result_visualization.png\n   Moved: cross_validation_versus_holdout_performance.png\n   Moved: cumulative_feature_importance.png\n   Moved: joins_diagram_resized.png\n   Moved: leakage_diagram.png\n   Moved: regression_features_model_agreement_resized.png\n   Moved: regression_features_model_agreement.png\n   Moved: feature_family_summary_resized.png\n   Moved: comprehensive_pipeline_analysis_resized.png\n   Moved: checkpoint3_feature_analysis.png\n   Moved: comprehensive_pipeline_analysis.png\n   Moved: regression_baseline_feature_Importance.png\n   Moved: missing_data_comprehensive_analysis_resized.png\n   Moved: ML_pipeline_phase_2_resized.png\n   Moved: database_schema_resized.png\n   Moved: geographic_patterns_analysis_resized.png\n   Moved: checkpoint5_final_analysis_resized.png\n   Moved: temporal_patterns_analysis_resized.png\n   Moved: ML_pipeline_phase_2.png\n   Moved: joins_diagram.png\n   Moved: database_schema.png\n   Moved: carrier_performance_analysis_resized.png\n   Moved: gannt_chart_resized.png\n   Moved: missing_data_before_after.png\n   Moved: feature_family_summary.png\n   Moved: checkpoint4_feature_analysis.png\n   Moved: final_verification_report.png\n   Moved: geographic_patterns_analysis.png\n   Moved: missing_data_comprehensive_analysis.png\n   Moved: checkpoint2_missing_analysis.png\n   Moved: checkpoint1_missing_analysis.png\n   Moved: temporal_patterns_analysis.png\n   Moved: gannt_chart.png\n   Moved: checkpoint5_final_analysis.png\n   Moved: carrier_performance_analysis.png\n\n================================================================================\nSUMMARY\n================================================================================\n\n Files moved to Charts/: 37\n Files already in Charts/: 0\n Files not found: 0\n Files failed to move: 0\n\nTotal processed: 37\n\n================================================================================\nVERIFICATION: Charts Directory Contents\n================================================================================\n\nTotal PNG files in Charts/: 37\n\nFirst 10 files:\n   1. ML_pipeline_phase_2.png\n   2. ML_pipeline_phase_2_resized.png\n   3. baseline_regression_cross_validation_result_visualization.png\n   4. carrier_performance_analysis.png\n   5. carrier_performance_analysis_resized.png\n   6. checkpoint1_missing_analysis.png\n   7. checkpoint2_missing_analysis.png\n   8. checkpoint3_feature_analysis.png\n   9. checkpoint4_feature_analysis.png\n  10. checkpoint5_final_analysis.png\n  ... and 27 more files\n\n================================================================================\n MOVE COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MOVE ALL PNG FILES TO Charts FOLDER\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "CHARTS_DIR = f\"{BASE_DIR}/charts\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOVING PNG FILES TO Charts FOLDER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# List of all PNG files\n",
    "png_files = [\n",
    "    \"leakage_diagram_resized.png\",\n",
    "    \"regression_feature_importance_distribution_across_models.png\",\n",
    "    \"baseline_regression_cross_validation_result_visualization.png\",\n",
    "    \"cross_validation_versus_holdout_performance.png\",\n",
    "    \"cumulative_feature_importance.png\",\n",
    "    \"joins_diagram_resized.png\",\n",
    "    \"leakage_diagram.png\",\n",
    "    \"regression_features_model_agreement_resized.png\",\n",
    "    \"regression_features_model_agreement.png\",\n",
    "    \"feature_family_summary_resized.png\",\n",
    "    \"comprehensive_pipeline_analysis_resized.png\",\n",
    "    \"checkpoint3_feature_analysis.png\",\n",
    "    \"comprehensive_pipeline_analysis.png\",\n",
    "    \"regression_baseline_feature_Importance.png\",\n",
    "    \"missing_data_comprehensive_analysis_resized.png\",\n",
    "    \"ML_pipeline_phase_2_resized.png\",\n",
    "    \"database_schema_resized.png\",\n",
    "    \"geographic_patterns_analysis_resized.png\",\n",
    "    \"checkpoint5_final_analysis_resized.png\",\n",
    "    \"temporal_patterns_analysis_resized.png\",\n",
    "    \"ML_pipeline_phase_2.png\",\n",
    "    \"joins_diagram.png\",\n",
    "    \"database_schema.png\",\n",
    "    \"carrier_performance_analysis_resized.png\",\n",
    "    \"gannt_chart_resized.png\",\n",
    "    \"missing_data_before_after.png\",\n",
    "    \"feature_family_summary.png\",\n",
    "    \"checkpoint4_feature_analysis.png\",\n",
    "    \"final_verification_report.png\",\n",
    "    \"geographic_patterns_analysis.png\",\n",
    "    \"missing_data_comprehensive_analysis.png\",\n",
    "    \"checkpoint2_missing_analysis.png\",\n",
    "    \"checkpoint1_missing_analysis.png\",\n",
    "    \"temporal_patterns_analysis.png\",\n",
    "    \"gannt_chart.png\",\n",
    "    \"checkpoint5_final_analysis.png\",\n",
    "    \"carrier_performance_analysis.png\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Ensure Charts directory exists\n",
    "# ============================================================================\n",
    "print(f\"\\nStep 1: Checking Charts directory...\")\n",
    "try:\n",
    "    dbutils.fs.ls(CHARTS_DIR)\n",
    "    print(f\" Charts directory exists: {CHARTS_DIR}\")\n",
    "except:\n",
    "    print(f\"  Creating Charts directory: {CHARTS_DIR}\")\n",
    "    dbutils.fs.mkdirs(CHARTS_DIR)\n",
    "    print(f\" Charts directory created\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Move PNG files\n",
    "# ============================================================================\n",
    "print(f\"\\nStep 2: Moving {len(png_files)} PNG files...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "moved_count = 0\n",
    "already_in_charts = 0\n",
    "not_found = 0\n",
    "failed = 0\n",
    "\n",
    "for png_file in png_files:\n",
    "    source_path = f\"{BASE_DIR}/{png_file}\"\n",
    "    dest_path = f\"{CHARTS_DIR}/{png_file}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists in source location\n",
    "        try:\n",
    "            dbutils.fs.ls(source_path)\n",
    "            file_exists_in_source = True\n",
    "        except:\n",
    "            file_exists_in_source = False\n",
    "        \n",
    "        # Check if file already in Charts\n",
    "        try:\n",
    "            dbutils.fs.ls(dest_path)\n",
    "            file_exists_in_dest = True\n",
    "        except:\n",
    "            file_exists_in_dest = False\n",
    "        \n",
    "        if file_exists_in_dest and not file_exists_in_source:\n",
    "            # Already moved\n",
    "            already_in_charts += 1\n",
    "            print(f\"   Already in Charts: {png_file}\")\n",
    "        \n",
    "        elif file_exists_in_source:\n",
    "            # Move the file (copy then delete)\n",
    "            dbutils.fs.cp(source_path, dest_path)\n",
    "            dbutils.fs.rm(source_path)\n",
    "            moved_count += 1\n",
    "            print(f\"   Moved: {png_file}\")\n",
    "        \n",
    "        else:\n",
    "            # File not found in either location\n",
    "            not_found += 1\n",
    "            print(f\"   Not found: {png_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(f\"   Failed to move {png_file}: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n Files moved to Charts/: {moved_count}\")\n",
    "print(f\" Files already in Charts/: {already_in_charts}\")\n",
    "print(f\" Files not found: {not_found}\")\n",
    "print(f\" Files failed to move: {failed}\")\n",
    "print(f\"\\nTotal processed: {len(png_files)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Verify Charts directory contents\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION: Charts Directory Contents\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    charts_contents = dbutils.fs.ls(CHARTS_DIR)\n",
    "    png_in_charts = [f.name for f in charts_contents if f.name.endswith('.png')]\n",
    "    \n",
    "    print(f\"\\nTotal PNG files in Charts/: {len(png_in_charts)}\")\n",
    "    print(\"\\nFirst 10 files:\")\n",
    "    for i, filename in enumerate(sorted(png_in_charts)[:10], 1):\n",
    "        print(f\"  {i:2d}. {filename}\")\n",
    "    \n",
    "    if len(png_in_charts) > 10:\n",
    "        print(f\"  ... and {len(png_in_charts) - 10} more files\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Charts directory: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" MOVE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc84b3f-4043-4465-986d-d2a6f06d00ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nMOVING CSV FILES TO csvs_1Y FOLDER\n================================================================================\n\nStep 1: Checking csvs_1Y directory...\n  Creating csvs_1Y directory: dbfs:/student-groups/Group_4_4/csvs_1Y\n csvs_1Y directory created\n\nStep 2: Moving 35 CSV files...\n--------------------------------------------------------------------------------\n   Moved: quarterly_delay_analysis.csv\n   Moved: presentation_statistics.csv\n   Moved: raw_vs_derived_features.csv\n   Moved: day_of_week_analysis.csv\n   Moved: quality_metrics_summary.csv\n   Moved: cv_summary.csv\n   Moved: feature_evolution_summary.csv\n   Moved: final_feature_summary.csv\n   Moved: final_dataset_profile.csv\n   Moved: holdout_results.csv\n   Moved: dataset_sizes_summary.csv\n   Moved: cv_vs_holdout_comparison.csv\n   Moved: feature_families_summary.csv\n   Moved: top_predictors_summary.csv\n   Moved: hourly_delay_analysis.csv\n   Moved: cv_results.csv\n   Moved: airport_performance_top30.csv\n   Moved: non_numerical_feature_handling.csv\n   Moved: carrier_performance_metrics.csv\n   Moved: state_level_analysis.csv\n   Moved: feature_transformations.csv\n   Moved: regression_resultscv_regression_baseline_results.csv\n   Moved: cv_regression_results.csv\n   Moved: cv_regression_results_with.csv\n   Moved: missing_data_comparison.csv\n   Moved: appendix_b_column_classification_final.csv\n   Moved: appendix_b2_column_classification_2015.csv\n   Moved: appendix_b1_column_classification_2015.csv\n   Moved: appendix_b3_column_classification_2015.csv\n   Moved: comprehensive_data_dictionary.csv\n   Moved: appendix_b4_column_classification_2015.csv\n   Moved: appendix_b5_column_classification_2015.csv\n   Moved: cv_regression_feature_importance.csv\n   Moved: regression_resultscv_regression_baseline_feature_importance.csv\n   Moved: airport-zones.csv\n\n================================================================================\nSUMMARY\n================================================================================\n\n Files moved to csvs_1Y/: 35\n Files already in csvs_1Y/: 0\n Files not found: 0\n Files failed to move: 0\n\nTotal processed: 35\n\n================================================================================\nVERIFICATION: csvs_1Y Directory Contents\n================================================================================\n\nTotal CSV files in csvs_1Y/: 35\n\nAll files:\n   1. airport-zones.csv\n   2. airport_performance_top30.csv\n   3. appendix_b1_column_classification_2015.csv\n   4. appendix_b2_column_classification_2015.csv\n   5. appendix_b3_column_classification_2015.csv\n   6. appendix_b4_column_classification_2015.csv\n   7. appendix_b5_column_classification_2015.csv\n   8. appendix_b_column_classification_final.csv\n   9. carrier_performance_metrics.csv\n  10. comprehensive_data_dictionary.csv\n  11. cv_regression_feature_importance.csv\n  12. cv_regression_results.csv\n  13. cv_regression_results_with.csv\n  14. cv_results.csv\n  15. cv_summary.csv\n  16. cv_vs_holdout_comparison.csv\n  17. dataset_sizes_summary.csv\n  18. day_of_week_analysis.csv\n  19. feature_evolution_summary.csv\n  20. feature_families_summary.csv\n  21. feature_transformations.csv\n  22. final_dataset_profile.csv\n  23. final_feature_summary.csv\n  24. holdout_results.csv\n  25. hourly_delay_analysis.csv\n  26. missing_data_comparison.csv\n  27. non_numerical_feature_handling.csv\n  28. presentation_statistics.csv\n  29. quality_metrics_summary.csv\n  30. quarterly_delay_analysis.csv\n  31. raw_vs_derived_features.csv\n  32. regression_resultscv_regression_baseline_feature_importance.csv\n  33. regression_resultscv_regression_baseline_results.csv\n  34. state_level_analysis.csv\n  35. top_predictors_summary.csv\n\n================================================================================\n MOVE COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MOVE ALL CSV FILES TO csvs_1Y FOLDER\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "CSV_DIR = f\"{BASE_DIR}/csvs_1Y\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOVING CSV FILES TO csvs_1Y FOLDER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# List of all CSV files\n",
    "csv_files = [\n",
    "    \"quarterly_delay_analysis.csv\",\n",
    "    \"presentation_statistics.csv\",\n",
    "    \"raw_vs_derived_features.csv\",\n",
    "    \"day_of_week_analysis.csv\",\n",
    "    \"quality_metrics_summary.csv\",\n",
    "    \"cv_summary.csv\",\n",
    "    \"feature_evolution_summary.csv\",\n",
    "    \"final_feature_summary.csv\",\n",
    "    \"final_dataset_profile.csv\",\n",
    "    \"holdout_results.csv\",\n",
    "    \"dataset_sizes_summary.csv\",\n",
    "    \"cv_vs_holdout_comparison.csv\",\n",
    "    \"feature_families_summary.csv\",\n",
    "    \"top_predictors_summary.csv\",\n",
    "    \"hourly_delay_analysis.csv\",\n",
    "    \"cv_results.csv\",\n",
    "    \"airport_performance_top30.csv\",\n",
    "    \"non_numerical_feature_handling.csv\",\n",
    "    \"carrier_performance_metrics.csv\",\n",
    "    \"state_level_analysis.csv\",\n",
    "    \"feature_transformations.csv\",\n",
    "    \"regression_resultscv_regression_baseline_results.csv\",\n",
    "    \"cv_regression_results.csv\",\n",
    "    \"cv_regression_results_with.csv\",\n",
    "    \"missing_data_comparison.csv\",\n",
    "    \"appendix_b_column_classification_final.csv\",\n",
    "    \"appendix_b2_column_classification_2015.csv\",\n",
    "    \"appendix_b1_column_classification_2015.csv\",\n",
    "    \"appendix_b3_column_classification_2015.csv\",\n",
    "    \"comprehensive_data_dictionary.csv\",\n",
    "    \"appendix_b4_column_classification_2015.csv\",\n",
    "    \"appendix_b5_column_classification_2015.csv\",\n",
    "    \"cv_regression_feature_importance.csv\",\n",
    "    \"regression_resultscv_regression_baseline_feature_importance.csv\",\n",
    "    \"airport-zones.csv\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Ensure csvs_1Y directory exists\n",
    "# ============================================================================\n",
    "print(f\"\\nStep 1: Checking csvs_1Y directory...\")\n",
    "try:\n",
    "    dbutils.fs.ls(CSV_DIR)\n",
    "    print(f\" csvs_1Y directory exists: {CSV_DIR}\")\n",
    "except:\n",
    "    print(f\"  Creating csvs_1Y directory: {CSV_DIR}\")\n",
    "    dbutils.fs.mkdirs(CSV_DIR)\n",
    "    print(f\" csvs_1Y directory created\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Move CSV files\n",
    "# ============================================================================\n",
    "print(f\"\\nStep 2: Moving {len(csv_files)} CSV files...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "moved_count = 0\n",
    "already_in_csvs = 0\n",
    "not_found = 0\n",
    "failed = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    source_path = f\"{BASE_DIR}/{csv_file}\"\n",
    "    dest_path = f\"{CSV_DIR}/{csv_file}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists in source location\n",
    "        try:\n",
    "            dbutils.fs.ls(source_path)\n",
    "            file_exists_in_source = True\n",
    "        except:\n",
    "            file_exists_in_source = False\n",
    "        \n",
    "        # Check if file already in csvs_1Y\n",
    "        try:\n",
    "            dbutils.fs.ls(dest_path)\n",
    "            file_exists_in_dest = True\n",
    "        except:\n",
    "            file_exists_in_dest = False\n",
    "        \n",
    "        if file_exists_in_dest and not file_exists_in_source:\n",
    "            # Already moved\n",
    "            already_in_csvs += 1\n",
    "            print(f\"   Already in csvs_1Y: {csv_file}\")\n",
    "        \n",
    "        elif file_exists_in_source:\n",
    "            # Move the file (copy then delete)\n",
    "            dbutils.fs.cp(source_path, dest_path)\n",
    "            dbutils.fs.rm(source_path)\n",
    "            moved_count += 1\n",
    "            print(f\"   Moved: {csv_file}\")\n",
    "        \n",
    "        else:\n",
    "            # File not found in either location\n",
    "            not_found += 1\n",
    "            print(f\"   Not found: {csv_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(f\"   Failed to move {csv_file}: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n Files moved to csvs_1Y/: {moved_count}\")\n",
    "print(f\" Files already in csvs_1Y/: {already_in_csvs}\")\n",
    "print(f\" Files not found: {not_found}\")\n",
    "print(f\" Files failed to move: {failed}\")\n",
    "print(f\"\\nTotal processed: {len(csv_files)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Verify csvs_1Y directory contents\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION: csvs_1Y Directory Contents\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    csv_contents = dbutils.fs.ls(CSV_DIR)\n",
    "    csvs_in_folder = [f.name for f in csv_contents if f.name.endswith('.csv')]\n",
    "    \n",
    "    print(f\"\\nTotal CSV files in csvs_1Y/: {len(csvs_in_folder)}\")\n",
    "    print(\"\\nAll files:\")\n",
    "    for i, filename in enumerate(sorted(csvs_in_folder), 1):\n",
    "        print(f\"  {i:2d}. {filename}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading csvs_1Y directory: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" MOVE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8eb0ff4-e22a-40cc-8c60-f4fca19270c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nMOVING FILES TO APPROPRIATE FOLDERS\n================================================================================\n\nFiles to move:\n  JPG files  Charts/: 2\n  TXT files  csvs_1Y/: 9\n  CSV files  csvs_1Y/: 1\n\nEnsuring directories exist...\n Charts directory exists\n csvs_1Y directory exists\n\n================================================================================\nMOVING JPG FILES TO Charts/\n================================================================================\n   Moved to Charts/: MIDS261_Final_Project_Phase2.jpg\n   Moved to Charts/: MIDS261_Final_Project_Phase2_POC.jpg\n\n================================================================================\nMOVING TXT FILES TO csvs_1Y/\n================================================================================\n   Moved to csvs_1Y/: checkpoint1_analysis_report.txt\n   Moved to csvs_1Y/: checkpoint2_analysis_report.txt\n   Moved to csvs_1Y/: checkpoint3_analysis_report.txt\n   Moved to csvs_1Y/: checkpoint4_analysis_report.txt\n   Moved to csvs_1Y/: checkpoint5_analysis_report.txt\n   Moved to csvs_1Y/: checkpoint_summary.txt\n   Moved to csvs_1Y/: final_comprehensive_report.txt\n   Moved to csvs_1Y/: final_verification_summary.txt\n   Moved to csvs_1Y/: missing_data_analysis_report.txt\n\n================================================================================\nMOVING CSV FILES TO csvs_1Y/\n================================================================================\n   Moved to csvs_1Y/: table_descriptions_summary.csv\n\n================================================================================\nSUMMARY\n================================================================================\n\nJPG FILES  Charts/:\n   Moved: 2\n   Already there: 0\n   Not found: 0\n\nTXT FILES  csvs_1Y/:\n   Moved: 9\n   Already there: 0\n   Not found: 0\n\nCSV FILES  csvs_1Y/:\n   Moved: 1\n   Already there: 0\n   Not found: 0\n\nTOTAL FILES MOVED: 12\n\n================================================================================\n MOVE COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MOVE JPG, TXT, AND CSV FILES TO APPROPRIATE FOLDERS\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "CHARTS_DIR = f\"{BASE_DIR}/Charts\"\n",
    "CSV_DIR = f\"{BASE_DIR}/csvs_1Y\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOVING FILES TO APPROPRIATE FOLDERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "files = [\n",
    "    \"MIDS261_Final_Project_Phase2.jpg\",\n",
    "    \"MIDS261_Final_Project_Phase2_POC.jpg\",\n",
    "    \"checkpoint1_analysis_report.txt\",\n",
    "    \"checkpoint2_analysis_report.txt\",\n",
    "    \"checkpoint3_analysis_report.txt\",\n",
    "    \"checkpoint4_analysis_report.txt\",\n",
    "    \"checkpoint5_analysis_report.txt\",\n",
    "    \"checkpoint_summary.txt\",\n",
    "    \"final_comprehensive_report.txt\",\n",
    "    \"final_verification_summary.txt\",\n",
    "    \"missing_data_analysis_report.txt\",\n",
    "    \"table_descriptions_summary.csv\",\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Categorize files by destination\n",
    "# ============================================================================\n",
    "jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "txt_files = [f for f in files if f.endswith('.txt')]\n",
    "csv_files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "print(f\"\\nFiles to move:\")\n",
    "print(f\"  JPG files  Charts/: {len(jpg_files)}\")\n",
    "print(f\"  TXT files  csvs_1Y/: {len(txt_files)}\")\n",
    "print(f\"  CSV files  csvs_1Y/: {len(csv_files)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Ensure directories exist\n",
    "# ============================================================================\n",
    "print(\"\\nEnsuring directories exist...\")\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(CHARTS_DIR)\n",
    "    print(f\" Charts directory exists\")\n",
    "except:\n",
    "    dbutils.fs.mkdirs(CHARTS_DIR)\n",
    "    print(f\" Charts directory created\")\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(CSV_DIR)\n",
    "    print(f\" csvs_1Y directory exists\")\n",
    "except:\n",
    "    dbutils.fs.mkdirs(CSV_DIR)\n",
    "    print(f\" csvs_1Y directory created\")\n",
    "\n",
    "# ============================================================================\n",
    "# Move JPG files to Charts\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MOVING JPG FILES TO Charts/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "jpg_moved = 0\n",
    "jpg_already_there = 0\n",
    "jpg_not_found = 0\n",
    "\n",
    "for jpg_file in jpg_files:\n",
    "    source_path = f\"{BASE_DIR}/{jpg_file}\"\n",
    "    dest_path = f\"{CHARTS_DIR}/{jpg_file}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if exists in source\n",
    "        try:\n",
    "            dbutils.fs.ls(source_path)\n",
    "            exists_in_source = True\n",
    "        except:\n",
    "            exists_in_source = False\n",
    "        \n",
    "        # Check if exists in destination\n",
    "        try:\n",
    "            dbutils.fs.ls(dest_path)\n",
    "            exists_in_dest = True\n",
    "        except:\n",
    "            exists_in_dest = False\n",
    "        \n",
    "        if exists_in_dest and not exists_in_source:\n",
    "            jpg_already_there += 1\n",
    "            print(f\"   Already in Charts/: {jpg_file}\")\n",
    "        elif exists_in_source:\n",
    "            dbutils.fs.cp(source_path, dest_path)\n",
    "            dbutils.fs.rm(source_path)\n",
    "            jpg_moved += 1\n",
    "            print(f\"   Moved to Charts/: {jpg_file}\")\n",
    "        else:\n",
    "            jpg_not_found += 1\n",
    "            print(f\"   Not found: {jpg_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Failed: {jpg_file} - {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Move TXT files to csvs_1Y\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MOVING TXT FILES TO csvs_1Y/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "txt_moved = 0\n",
    "txt_already_there = 0\n",
    "txt_not_found = 0\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    source_path = f\"{BASE_DIR}/{txt_file}\"\n",
    "    dest_path = f\"{CSV_DIR}/{txt_file}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if exists in source\n",
    "        try:\n",
    "            dbutils.fs.ls(source_path)\n",
    "            exists_in_source = True\n",
    "        except:\n",
    "            exists_in_source = False\n",
    "        \n",
    "        # Check if exists in destination\n",
    "        try:\n",
    "            dbutils.fs.ls(dest_path)\n",
    "            exists_in_dest = True\n",
    "        except:\n",
    "            exists_in_dest = False\n",
    "        \n",
    "        if exists_in_dest and not exists_in_source:\n",
    "            txt_already_there += 1\n",
    "            print(f\"   Already in csvs_1Y/: {txt_file}\")\n",
    "        elif exists_in_source:\n",
    "            dbutils.fs.cp(source_path, dest_path)\n",
    "            dbutils.fs.rm(source_path)\n",
    "            txt_moved += 1\n",
    "            print(f\"   Moved to csvs_1Y/: {txt_file}\")\n",
    "        else:\n",
    "            txt_not_found += 1\n",
    "            print(f\"   Not found: {txt_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Failed: {txt_file} - {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Move CSV files to csvs_1Y\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MOVING CSV FILES TO csvs_1Y/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "csv_moved = 0\n",
    "csv_already_there = 0\n",
    "csv_not_found = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    source_path = f\"{BASE_DIR}/{csv_file}\"\n",
    "    dest_path = f\"{CSV_DIR}/{csv_file}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if exists in source\n",
    "        try:\n",
    "            dbutils.fs.ls(source_path)\n",
    "            exists_in_source = True\n",
    "        except:\n",
    "            exists_in_source = False\n",
    "        \n",
    "        # Check if exists in destination\n",
    "        try:\n",
    "            dbutils.fs.ls(dest_path)\n",
    "            exists_in_dest = True\n",
    "        except:\n",
    "            exists_in_dest = False\n",
    "        \n",
    "        if exists_in_dest and not exists_in_source:\n",
    "            csv_already_there += 1\n",
    "            print(f\"   Already in csvs_1Y/: {csv_file}\")\n",
    "        elif exists_in_source:\n",
    "            dbutils.fs.cp(source_path, dest_path)\n",
    "            dbutils.fs.rm(source_path)\n",
    "            csv_moved += 1\n",
    "            print(f\"   Moved to csvs_1Y/: {csv_file}\")\n",
    "        else:\n",
    "            csv_not_found += 1\n",
    "            print(f\"   Not found: {csv_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Failed: {csv_file} - {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nJPG FILES  Charts/:\")\n",
    "print(f\"   Moved: {jpg_moved}\")\n",
    "print(f\"   Already there: {jpg_already_there}\")\n",
    "print(f\"   Not found: {jpg_not_found}\")\n",
    "\n",
    "print(f\"\\nTXT FILES  csvs_1Y/:\")\n",
    "print(f\"   Moved: {txt_moved}\")\n",
    "print(f\"   Already there: {txt_already_there}\")\n",
    "print(f\"   Not found: {txt_not_found}\")\n",
    "\n",
    "print(f\"\\nCSV FILES  csvs_1Y/:\")\n",
    "print(f\"   Moved: {csv_moved}\")\n",
    "print(f\"   Already there: {csv_already_there}\")\n",
    "print(f\"   Not found: {csv_not_found}\")\n",
    "\n",
    "print(f\"\\nTOTAL FILES MOVED: {jpg_moved + txt_moved + csv_moved}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" MOVE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a4f6e9-a82f-4d26-816c-5fe4c823a5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nDELETING FOLDERS AND CONTENTS\n================================================================================\n\nFolders to delete: 18\n\n================================================================================\nSTEP 1: Checking folders before deletion\n================================================================================\n\n Found: _checkpoints                                                        0 files,        0 B\n Found: df_joined_1Y_2015_clean.parquet                                    18 files,   250.3 MB\n Found: joined_1Y_2015_features_clean.parquet                              25 files,   273.2 MB\n Found: joined_1Y_2015_final_clean.parquet                                 14 files,   186.6 MB\n Found: joined_1Y_2015_final_feature_clean.parquet                         26 files,   316.0 MB\n Found: joined_1Y_clean_imputed.parquet                                    30 files,   265.4 MB\n Found: joined_1Y_feat.parquet                                             14 files,   521.8 MB\n Found: joined_1Y_feat_2.parquet                                           30 files,   793.4 MB\n Found: joined_1Y_feat_temp.parquet                                        14 files,   356.2 MB\n Found: joined_1Y_feat_temp2.parquet                                        1 files,        0 B\n Found: joined_1Y_feat_temp_2.parquet                                      14 files,   352.4 MB\n Found: joined_1Y_features_clean.parquet                                   12 files,   343.3 MB\n Found: joined_1Y_final_clean.parquet                                      19 files,   241.3 MB\n Found: joined_1Y_final_feature_clean.parquet                              21 files,   408.8 MB\n Found: joined_1Y_final_feature_clean_with_removed_features                53 files,    1.14 GB\n Found: plots_phase2                                                        0 files,        0 B\n Found: temp2.parquet                                                      39 files,   479.3 MB\n Found: temp3-4.parquet                                                    29 files,   404.5 MB\n\n--------------------------------------------------------------------------------\nFolders found: 18/18\nFolders not found: 0\nTotal space to be freed: 6.21 GB\n--------------------------------------------------------------------------------\n\n================================================================================\nSTEP 2: Confirmation\n================================================================================\n\n  You are about to PERMANENTLY DELETE 18 folders\n  This will free up approximately 6.21 GB\n\n Proceeding with deletion...\n\n================================================================================\nSTEP 3: Deleting folders\n================================================================================\n\nDeleting: _checkpoints\n   Deleted successfully (0 B freed)\nDeleting: df_joined_1Y_2015_clean.parquet\n   Deleted successfully (250.3 MB freed)\nDeleting: joined_1Y_2015_features_clean.parquet\n   Deleted successfully (273.2 MB freed)\nDeleting: joined_1Y_2015_final_clean.parquet\n   Deleted successfully (186.6 MB freed)\nDeleting: joined_1Y_2015_final_feature_clean.parquet\n   Deleted successfully (316.0 MB freed)\nDeleting: joined_1Y_clean_imputed.parquet\n   Deleted successfully (265.4 MB freed)\nDeleting: joined_1Y_feat.parquet\n   Deleted successfully (521.8 MB freed)\nDeleting: joined_1Y_feat_2.parquet\n   Deleted successfully (793.4 MB freed)\nDeleting: joined_1Y_feat_temp.parquet\n   Deleted successfully (356.2 MB freed)\nDeleting: joined_1Y_feat_temp2.parquet\n   Deleted successfully (0 B freed)\nDeleting: joined_1Y_feat_temp_2.parquet\n   Deleted successfully (352.4 MB freed)\nDeleting: joined_1Y_features_clean.parquet\n   Deleted successfully (343.3 MB freed)\nDeleting: joined_1Y_final_clean.parquet\n   Deleted successfully (241.3 MB freed)\nDeleting: joined_1Y_final_feature_clean.parquet\n   Deleted successfully (408.8 MB freed)\nDeleting: joined_1Y_final_feature_clean_with_removed_features\n   Deleted successfully (1.14 GB freed)\nDeleting: plots_phase2\n   Deleted successfully (0 B freed)\nDeleting: temp2.parquet\n   Deleted successfully (479.3 MB freed)\nDeleting: temp3-4.parquet\n   Deleted successfully (404.5 MB freed)\n\n================================================================================\nDELETION SUMMARY\n================================================================================\n\n Successfully deleted: 18 folders\n Failed to delete: 0 folders\n\uD83D\uDCE6 Total space freed: 6.21 GB\n\n================================================================================\nSTEP 5: Verification\n================================================================================\n\nChecking if folders were deleted...\n   Confirmed deleted: _checkpoints\n   Confirmed deleted: df_joined_1Y_2015_clean.parquet\n   Confirmed deleted: joined_1Y_2015_features_clean.parquet\n   Confirmed deleted: joined_1Y_2015_final_clean.parquet\n   Confirmed deleted: joined_1Y_2015_final_feature_clean.parquet\n   Confirmed deleted: joined_1Y_clean_imputed.parquet\n   Confirmed deleted: joined_1Y_feat.parquet\n   Confirmed deleted: joined_1Y_feat_2.parquet\n   Confirmed deleted: joined_1Y_feat_temp.parquet\n   Confirmed deleted: joined_1Y_feat_temp2.parquet\n   Confirmed deleted: joined_1Y_feat_temp_2.parquet\n   Confirmed deleted: joined_1Y_features_clean.parquet\n   Confirmed deleted: joined_1Y_final_clean.parquet\n   Confirmed deleted: joined_1Y_final_feature_clean.parquet\n   Confirmed deleted: joined_1Y_final_feature_clean_with_removed_features\n   Confirmed deleted: plots_phase2\n   Confirmed deleted: temp2.parquet\n   Confirmed deleted: temp3-4.parquet\n\n All 18 folders successfully deleted and verified\n\n================================================================================\nSTEP 6: Remaining folders in directory\n================================================================================\n\nRemaining folders: 34\n\nFirst 20 remaining folders:\n   1. 2015_final_feature_engineered_data_with_dep_delay/\n   2. Charts/\n   3. JOINED_1Y.parquet/\n   4. JOINED_1Y_2015.parquet/\n   5. JOINED_1Y_2019.parquet/\n   6. JOINED_3M.parquet/\n   7. JOINED_3M_2015.parquet/\n   8. JOINED_5Y_2015_2019.parquet/\n   9. charts/\n  10. checkpoint_1_initial_joined_2015.parquet/\n  11. checkpoint_1_initial_joined_2019.parquet/\n  12. checkpoint_2_cleaned_imputed_2015.parquet/\n  13. checkpoint_3_basic_features_2015.parquet/\n  14. checkpoint_3_basic_features_2019.parquet/\n  15. checkpoint_4_advanced_features_2015.parquet/\n  16. checkpoint_5_final_clean_2015.parquet/\n  17. checkpoint_5a_final_data_with_all_features_2015.parquet/\n  18. checkpoint_6_regression_features_baseline_2015.parquet/\n  19. checkpoint_7_regression_features_post_baseline_2015.parquet/\n  20. csvs_1Y/\n\n  ... and 14 more folders\n\n================================================================================\n DELETION COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DELETE SPECIFIED FOLDERS AND ALL THEIR CONTENTS\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DELETING FOLDERS AND CONTENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "folders_to_delete = [\n",
    "    \"_checkpoints\",\n",
    "    \"df_joined_1Y_2015_clean.parquet\",\n",
    "    \"joined_1Y_2015_features_clean.parquet\",\n",
    "    \"joined_1Y_2015_final_clean.parquet\",\n",
    "    \"joined_1Y_2015_final_feature_clean.parquet\",\n",
    "    \"joined_1Y_clean_imputed.parquet\",\n",
    "    \"joined_1Y_feat.parquet\",\n",
    "    \"joined_1Y_feat_2.parquet\",\n",
    "    \"joined_1Y_feat_temp.parquet\",\n",
    "    \"joined_1Y_feat_temp2.parquet\",\n",
    "    \"joined_1Y_feat_temp_2.parquet\",\n",
    "    \"joined_1Y_features_clean.parquet\",\n",
    "    \"joined_1Y_final_clean.parquet\",\n",
    "    \"joined_1Y_final_feature_clean.parquet\",\n",
    "    \"joined_1Y_final_feature_clean_with_removed_features\",\n",
    "    \"plots_phase2\",\n",
    "    \"temp2.parquet\",\n",
    "    \"temp3-4.parquet\"\n",
    "]\n",
    "\n",
    "print(f\"\\nFolders to delete: {len(folders_to_delete)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Check which folders exist and get their sizes\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: Checking folders before deletion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def get_size_str(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    elif size_bytes < 1024:\n",
    "        return f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024**2:\n",
    "        return f\"{size_bytes/1024:.1f} KB\"\n",
    "    elif size_bytes < 1024**3:\n",
    "        return f\"{size_bytes/(1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes/(1024**3):.2f} GB\"\n",
    "\n",
    "folders_found = []\n",
    "folders_not_found = []\n",
    "total_size_to_free = 0\n",
    "\n",
    "print()\n",
    "for folder_name in folders_to_delete:\n",
    "    folder_path = f\"{BASE_DIR}/{folder_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if folder exists\n",
    "        dbutils.fs.ls(folder_path)\n",
    "        \n",
    "        # Calculate folder size\n",
    "        contents = dbutils.fs.ls(folder_path)\n",
    "        folder_size = 0\n",
    "        file_count = 0\n",
    "        \n",
    "        for item in contents:\n",
    "            if not item.name.endswith('/'):\n",
    "                folder_size = folder_size + item.size\n",
    "                file_count = file_count + 1\n",
    "        \n",
    "        total_size_to_free = total_size_to_free + folder_size\n",
    "        \n",
    "        folders_found.append({\n",
    "            'name': folder_name,\n",
    "            'path': folder_path,\n",
    "            'size': folder_size,\n",
    "            'files': file_count\n",
    "        })\n",
    "        \n",
    "        size_str = get_size_str(folder_size)\n",
    "        print(f\" Found: {folder_name:65s} {file_count:>3} files, {size_str:>10s}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        folders_not_found.append(folder_name)\n",
    "        print(f\" Not found: {folder_name}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"Folders found: {len(folders_found)}/{len(folders_to_delete)}\")\n",
    "print(f\"Folders not found: {len(folders_not_found)}\")\n",
    "print(f\"Total space to be freed: {get_size_str(total_size_to_free)}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Confirmation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Confirmation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(folders_found) == 0:\n",
    "    print(\"\\n  No folders found to delete. Exiting.\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" NOTHING TO DELETE\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"\\n  You are about to PERMANENTLY DELETE {len(folders_found)} folders\")\n",
    "    print(f\"  This will free up approximately {get_size_str(total_size_to_free)}\")\n",
    "    print(\"\\n Proceeding with deletion...\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 3: Delete folders\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: Deleting folders\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    deleted_count = 0\n",
    "    failed_count = 0\n",
    "    total_freed = 0\n",
    "\n",
    "    for folder in folders_found:\n",
    "        try:\n",
    "            print(f\"Deleting: {folder['name']}\")\n",
    "            dbutils.fs.rm(folder['path'], recurse=True)\n",
    "            deleted_count += 1\n",
    "            total_freed = total_freed + folder['size']\n",
    "            print(f\"   Deleted successfully ({get_size_str(folder['size'])} freed)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            print(f\"   Failed to delete: {str(e)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 4: Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DELETION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\n Successfully deleted: {deleted_count} folders\")\n",
    "    print(f\" Failed to delete: {failed_count} folders\")\n",
    "    print(f\"\uD83D\uDCE6 Total space freed: {get_size_str(total_freed)}\")\n",
    "\n",
    "    if folders_not_found:\n",
    "        print(f\"\\n  Folders not found (already deleted?): {len(folders_not_found)}\")\n",
    "        for fname in folders_not_found[:10]:\n",
    "            print(f\"    - {fname}\")\n",
    "        if len(folders_not_found) > 10:\n",
    "            print(f\"    ... and {len(folders_not_found) - 10} more\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 5: Verify deletion\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: Verification\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nChecking if folders were deleted...\")\n",
    "    still_exist = []\n",
    "\n",
    "    for folder in folders_found:\n",
    "        try:\n",
    "            dbutils.fs.ls(folder['path'])\n",
    "            still_exist.append(folder['name'])\n",
    "            print(f\"    Still exists: {folder['name']}\")\n",
    "        except:\n",
    "            print(f\"   Confirmed deleted: {folder['name']}\")\n",
    "\n",
    "    if still_exist:\n",
    "        print(f\"\\n  WARNING: {len(still_exist)} folders still exist!\")\n",
    "    else:\n",
    "        print(f\"\\n All {deleted_count} folders successfully deleted and verified\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 6: Show remaining folders\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 6: Remaining folders in directory\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        remaining = dbutils.fs.ls(BASE_DIR)\n",
    "        remaining_folders = [f for f in remaining if f.name.endswith('/')]\n",
    "        \n",
    "        print(f\"\\nRemaining folders: {len(remaining_folders)}\")\n",
    "        print(\"\\nFirst 20 remaining folders:\")\n",
    "        for i, f in enumerate(sorted(remaining_folders, key=lambda x: x.name)[:20], 1):\n",
    "            print(f\"  {i:2d}. {f.name}\")\n",
    "        \n",
    "        if len(remaining_folders) > 20:\n",
    "            print(f\"\\n  ... and {len(remaining_folders) - 20} more folders\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing remaining folders: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" DELETION COMPLETE\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73f35ab-d13b-4991-a208-d0393ba9c361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nCONSOLIDATING charts/ INTO Charts/\n================================================================================\n\n================================================================================\nSTEP 1: Checking directories\n================================================================================\n Source directory exists: charts/\n Destination directory exists: Charts/\n\n================================================================================\nSTEP 2: Listing files in charts/ directory\n================================================================================\n\nFound in charts/:\n  Files: 37\n  Subdirectories: 0\n\nFiles to move:\n  - ML_pipeline_phase_2.png                                        171.9 KB\n  - ML_pipeline_phase_2_resized.png                                130.9 KB\n  - baseline_regression_cross_validation_result_visualization.png    41.5 KB\n  - carrier_performance_analysis.png                               989.5 KB\n  - carrier_performance_analysis_resized.png                       230.4 KB\n  - checkpoint1_missing_analysis.png                               614.8 KB\n  - checkpoint2_missing_analysis.png                               595.0 KB\n  - checkpoint3_feature_analysis.png                               115.7 KB\n  - checkpoint4_feature_analysis.png                               483.0 KB\n  - checkpoint5_final_analysis.png                                 711.3 KB\n  - checkpoint5_final_analysis_resized.png                         149.4 KB\n  - comprehensive_pipeline_analysis.png                            115.7 KB\n  - comprehensive_pipeline_analysis_resized.png                     96.5 KB\n  - cross_validation_versus_holdout_performance.png                 48.5 KB\n  - cumulative_feature_importance.png                               56.1 KB\n  - database_schema.png                                            213.6 KB\n  - database_schema_resized.png                                    134.9 KB\n  - feature_family_summary.png                                     367.5 KB\n  - feature_family_summary_resized.png                              82.1 KB\n  - final_verification_report.png                                  528.2 KB\n  - gannt_chart.png                                                625.7 KB\n  - gannt_chart_resized.png                                        240.8 KB\n  - geographic_patterns_analysis.png                               570.4 KB\n  - geographic_patterns_analysis_resized.png                       137.3 KB\n  - joins_diagram.png                                              203.2 KB\n  - joins_diagram_resized.png                                       61.2 KB\n  - leakage_diagram.png                                             67.6 KB\n  - leakage_diagram_resized.png                                     30.5 KB\n  - missing_data_before_after.png                                  275.2 KB\n  - missing_data_comprehensive_analysis.png                        578.2 KB\n  - missing_data_comprehensive_analysis_resized.png                125.5 KB\n  - regression_baseline_feature_Importance.png                     117.7 KB\n  - regression_feature_importance_distribution_across_models.png    34.7 KB\n  - regression_features_model_agreement.png                         72.4 KB\n  - regression_features_model_agreement_resized.png                 69.1 KB\n  - temporal_patterns_analysis.png                                 617.2 KB\n  - temporal_patterns_analysis_resized.png                         152.9 KB\n\nTotal size to move: 9.6 MB\n\n================================================================================\nSTEP 3: Moving files from charts/ to Charts/\n================================================================================\n\n   Moved: ML_pipeline_phase_2.png\n   Moved: ML_pipeline_phase_2_resized.png\n   Moved: baseline_regression_cross_validation_result_visualization.png\n   Moved: carrier_performance_analysis.png\n   Moved: carrier_performance_analysis_resized.png\n   Moved: checkpoint1_missing_analysis.png\n   Moved: checkpoint2_missing_analysis.png\n   Moved: checkpoint3_feature_analysis.png\n   Moved: checkpoint4_feature_analysis.png\n   Moved: checkpoint5_final_analysis.png\n   Moved: checkpoint5_final_analysis_resized.png\n   Moved: comprehensive_pipeline_analysis.png\n   Moved: comprehensive_pipeline_analysis_resized.png\n   Moved: cross_validation_versus_holdout_performance.png\n   Moved: cumulative_feature_importance.png\n   Moved: database_schema.png\n   Moved: database_schema_resized.png\n   Moved: feature_family_summary.png\n   Moved: feature_family_summary_resized.png\n   Moved: final_verification_report.png\n   Moved: gannt_chart.png\n   Moved: gannt_chart_resized.png\n   Moved: geographic_patterns_analysis.png\n   Moved: geographic_patterns_analysis_resized.png\n   Moved: joins_diagram.png\n   Moved: joins_diagram_resized.png\n   Moved: leakage_diagram.png\n   Moved: leakage_diagram_resized.png\n   Moved: missing_data_before_after.png\n   Moved: missing_data_comprehensive_analysis.png\n   Moved: missing_data_comprehensive_analysis_resized.png\n   Moved: regression_baseline_feature_Importance.png\n   Moved: regression_feature_importance_distribution_across_models.png\n   Moved: regression_features_model_agreement.png\n   Moved: regression_features_model_agreement_resized.png\n   Moved: temporal_patterns_analysis.png\n   Moved: temporal_patterns_analysis_resized.png\n\nMove summary:\n   Moved: 37\n   Skipped (already exist): 0\n   Failed: 0\n\n================================================================================\nSTEP 4: Verifying Charts/ directory\n================================================================================\n\nContents of Charts/ directory:\n  Files: 53\n  Subdirectories: 2\n\nFirst 10 files in Charts/:\n   1. MIDS261_Final_Project_Phase2.jpg                                27.7 KB\n   2. MIDS261_Final_Project_Phase2_POC.jpg                            43.9 KB\n   3. ML_pipeline_phase_2.png                                        171.9 KB\n   4. ML_pipeline_phase_2_resized.png                                130.9 KB\n   5. On-Time vs Delayed Flights (DEP_DEL15) 15 minutes delay or more.png    21.3 KB\n   6. baseline_regression_cross_validation_result_visualization.png    41.5 KB\n   7. carrier_performance_analysis.png                               989.5 KB\n   8. carrier_performance_analysis_resized.png                       230.4 KB\n   9. checkpoint1_missing_analysis.png                               614.8 KB\n  10. checkpoint2_missing_analysis.png                               595.0 KB\n  ... and 43 more files\n\n================================================================================\nSTEP 5: Deleting charts/ directory\n================================================================================\n\nDeleting: dbfs:/student-groups/Group_4_4/charts\n charts/ directory deleted successfully\n Confirmed: charts/ directory no longer exists\n\n================================================================================\nFINAL SUMMARY\n================================================================================\n\n Files moved from charts/ to Charts/: 37\n Files already in Charts/ (skipped): 0\n Files that failed to move: 0\n\n charts/ directory has been deleted\n All files consolidated in Charts/ directory\n\n================================================================================\n CONSOLIDATION COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MOVE FILES FROM charts/ TO Charts/ AND DELETE charts/\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "SOURCE_DIR = f\"{BASE_DIR}/charts\"\n",
    "DEST_DIR = f\"{BASE_DIR}/Charts\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONSOLIDATING charts/ INTO Charts/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def get_size_str(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    elif size_bytes < 1024:\n",
    "        return f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024**2:\n",
    "        return f\"{size_bytes/1024:.1f} KB\"\n",
    "    elif size_bytes < 1024**3:\n",
    "        return f\"{size_bytes/(1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes/(1024**3):.2f} GB\"\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Check both directories exist\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: Checking directories\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(SOURCE_DIR)\n",
    "    source_exists = True\n",
    "    print(f\" Source directory exists: charts/\")\n",
    "except:\n",
    "    source_exists = False\n",
    "    print(f\" Source directory NOT found: charts/\")\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(DEST_DIR)\n",
    "    dest_exists = True\n",
    "    print(f\" Destination directory exists: Charts/\")\n",
    "except:\n",
    "    dest_exists = False\n",
    "    print(f\" Destination directory NOT found: Charts/\")\n",
    "    print(f\"  Creating Charts/ directory...\")\n",
    "    dbutils.fs.mkdirs(DEST_DIR)\n",
    "    print(f\" Charts/ directory created\")\n",
    "\n",
    "if not source_exists:\n",
    "    print(\"\\n  Source directory 'charts/' does not exist. Nothing to move.\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" OPERATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    # ========================================================================\n",
    "    # Step 2: List files in charts/ directory\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: Listing files in charts/ directory\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        source_contents = dbutils.fs.ls(SOURCE_DIR)\n",
    "        \n",
    "        # Separate files and subdirectories\n",
    "        files_to_move = [item for item in source_contents if not item.name.endswith('/')]\n",
    "        subdirs = [item for item in source_contents if item.name.endswith('/')]\n",
    "        \n",
    "        print(f\"\\nFound in charts/:\")\n",
    "        print(f\"  Files: {len(files_to_move)}\")\n",
    "        print(f\"  Subdirectories: {len(subdirs)}\")\n",
    "        \n",
    "        if len(files_to_move) == 0 and len(subdirs) == 0:\n",
    "            print(\"\\n charts/ directory is empty\")\n",
    "        else:\n",
    "            print(\"\\nFiles to move:\")\n",
    "            total_size = 0\n",
    "            for f in files_to_move:\n",
    "                total_size = total_size + f.size\n",
    "                size_str = get_size_str(f.size)\n",
    "                print(f\"  - {f.name:60s} {size_str:>10s}\")\n",
    "            \n",
    "            if subdirs:\n",
    "                print(\"\\nSubdirectories (will be moved recursively):\")\n",
    "                for d in subdirs:\n",
    "                    print(f\"  - {d.name}\")\n",
    "            \n",
    "            print(f\"\\nTotal size to move: {get_size_str(total_size)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading charts/ directory: {e}\")\n",
    "        files_to_move = []\n",
    "        subdirs = []\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 3: Move files from charts/ to Charts/\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: Moving files from charts/ to Charts/\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if len(files_to_move) == 0 and len(subdirs) == 0:\n",
    "        print(\"\\nNo files to move (charts/ is empty)\")\n",
    "        moved_count = 0\n",
    "        failed_count = 0\n",
    "    else:\n",
    "        print()\n",
    "        moved_count = 0\n",
    "        failed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        # Move files\n",
    "        for file_item in files_to_move:\n",
    "            source_path = file_item.path\n",
    "            dest_path = f\"{DEST_DIR}/{file_item.name}\"\n",
    "            \n",
    "            try:\n",
    "                # Check if file already exists in destination\n",
    "                try:\n",
    "                    dbutils.fs.ls(dest_path)\n",
    "                    file_exists = True\n",
    "                except:\n",
    "                    file_exists = False\n",
    "                \n",
    "                if file_exists:\n",
    "                    print(f\"   Already exists in Charts/: {file_item.name}\")\n",
    "                    skipped_count += 1\n",
    "                else:\n",
    "                    # Copy file to Charts/\n",
    "                    dbutils.fs.cp(source_path, dest_path)\n",
    "                    moved_count += 1\n",
    "                    print(f\"   Moved: {file_item.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                print(f\"   Failed to move {file_item.name}: {e}\")\n",
    "        \n",
    "        # Move subdirectories\n",
    "        for subdir_item in subdirs:\n",
    "            source_path = subdir_item.path\n",
    "            subdir_name = subdir_item.name.rstrip('/')\n",
    "            dest_path = f\"{DEST_DIR}/{subdir_name}\"\n",
    "            \n",
    "            try:\n",
    "                # Check if subdirectory already exists in destination\n",
    "                try:\n",
    "                    dbutils.fs.ls(dest_path)\n",
    "                    subdir_exists = True\n",
    "                except:\n",
    "                    subdir_exists = False\n",
    "                \n",
    "                if subdir_exists:\n",
    "                    print(f\"   Subdirectory already exists in Charts/: {subdir_name}/\")\n",
    "                    skipped_count += 1\n",
    "                else:\n",
    "                    # Copy subdirectory recursively to Charts/\n",
    "                    dbutils.fs.cp(source_path, dest_path, recurse=True)\n",
    "                    moved_count += 1\n",
    "                    print(f\"   Moved subdirectory: {subdir_name}/\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                print(f\"   Failed to move subdirectory {subdir_name}/: {e}\")\n",
    "        \n",
    "        print(f\"\\nMove summary:\")\n",
    "        print(f\"   Moved: {moved_count}\")\n",
    "        print(f\"   Skipped (already exist): {skipped_count}\")\n",
    "        print(f\"   Failed: {failed_count}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 4: Verify Charts/ directory\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: Verifying Charts/ directory\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        dest_contents = dbutils.fs.ls(DEST_DIR)\n",
    "        files_in_charts = [item for item in dest_contents if not item.name.endswith('/')]\n",
    "        subdirs_in_charts = [item for item in dest_contents if item.name.endswith('/')]\n",
    "        \n",
    "        print(f\"\\nContents of Charts/ directory:\")\n",
    "        print(f\"  Files: {len(files_in_charts)}\")\n",
    "        print(f\"  Subdirectories: {len(subdirs_in_charts)}\")\n",
    "        \n",
    "        if len(files_in_charts) > 0:\n",
    "            print(f\"\\nFirst 10 files in Charts/:\")\n",
    "            for i, f in enumerate(sorted(files_in_charts, key=lambda x: x.name)[:10], 1):\n",
    "                size_str = get_size_str(f.size)\n",
    "                print(f\"  {i:2d}. {f.name:60s} {size_str:>10s}\")\n",
    "            \n",
    "            if len(files_in_charts) > 10:\n",
    "                print(f\"  ... and {len(files_in_charts) - 10} more files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Charts/ directory: {e}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 5: Delete charts/ directory\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: Deleting charts/ directory\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nDeleting: {SOURCE_DIR}\")\n",
    "        dbutils.fs.rm(SOURCE_DIR, recurse=True)\n",
    "        print(f\" charts/ directory deleted successfully\")\n",
    "        \n",
    "        # Verify deletion\n",
    "        try:\n",
    "            dbutils.fs.ls(SOURCE_DIR)\n",
    "            print(f\"  WARNING: charts/ directory still exists!\")\n",
    "        except:\n",
    "            print(f\" Confirmed: charts/ directory no longer exists\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Failed to delete charts/ directory: {e}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Step 6: Final Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\n Files moved from charts/ to Charts/: {moved_count}\")\n",
    "    print(f\" Files already in Charts/ (skipped): {skipped_count}\")\n",
    "    print(f\" Files that failed to move: {failed_count}\")\n",
    "    print(f\"\\n charts/ directory has been deleted\")\n",
    "    print(f\" All files consolidated in Charts/ directory\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" CONSOLIDATION COMPLETE\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db9b675-7dca-40f5-b103-68771f315212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nDELETING FOLDERS AND ALL SUBFILES\n================================================================================\n\nFolders to delete: 3\n\n================================================================================\nSTEP 1: Checking folders before deletion\n================================================================================\n\n Found: JOINED_1Y.parquet                                    26 files,   489.3 MB\n Found: JOINED_3M.parquet                                    12 files,    86.7 MB\n Found: mart                                                 5 subdirs,   30 files,   569.8 MB\n\n--------------------------------------------------------------------------------\nFolders found: 3/3\nFolders not found: 0\nTotal space to be freed: 1.12 GB\n--------------------------------------------------------------------------------\n\n================================================================================\nSTEP 2: Confirmation\n================================================================================\n\n  You are about to PERMANENTLY DELETE 3 folders\n  This includes ALL subfolders and files\n  This will free up approximately 1.12 GB\n\n Proceeding with deletion...\n\n================================================================================\nSTEP 3: Deleting folders\n================================================================================\n\nDeleting: JOINED_1Y.parquet\n  Removing 26 files across 0 subdirectories...\n   Deleted successfully (489.3 MB freed)\nDeleting: JOINED_3M.parquet\n  Removing 12 files across 0 subdirectories...\n   Deleted successfully (86.7 MB freed)\nDeleting: mart\n  Removing 30 files across 5 subdirectories...\n   Deleted successfully (569.8 MB freed)\n\n================================================================================\nDELETION SUMMARY\n================================================================================\n\n Successfully deleted: 3 folders\n Failed to delete: 0 folders\n\uD83D\uDCE6 Total space freed: 1.12 GB\n\n================================================================================\nSTEP 5: Verification\n================================================================================\n\nVerifying deletion...\n   Confirmed deleted: JOINED_1Y.parquet\n   Confirmed deleted: JOINED_3M.parquet\n   Confirmed deleted: mart\n\n All 3 folders successfully deleted and verified\n\n================================================================================\nSTEP 6: Remaining folders\n================================================================================\n\nRemaining folders in base directory: 30\n\nFirst 20 remaining folders:\n   1. 2015_final_feature_engineered_data_with_dep_delay/\n   2. Charts/\n   3. JOINED_1Y_2015.parquet/\n   4. JOINED_1Y_2019.parquet/\n   5. JOINED_3M_2015.parquet/\n   6. JOINED_5Y_2015_2019.parquet/\n   7. checkpoint_1_initial_joined_2015.parquet/\n   8. checkpoint_1_initial_joined_2019.parquet/\n   9. checkpoint_2_cleaned_imputed_2015.parquet/\n  10. checkpoint_3_basic_features_2015.parquet/\n  11. checkpoint_3_basic_features_2019.parquet/\n  12. checkpoint_4_advanced_features_2015.parquet/\n  13. checkpoint_5_final_clean_2015.parquet/\n  14. checkpoint_5a_final_data_with_all_features_2015.parquet/\n  15. checkpoint_6_regression_features_baseline_2015.parquet/\n  16. checkpoint_7_regression_features_post_baseline_2015.parquet/\n  17. csvs_1Y/\n  18. data_12M/\n  19. df_otpw_12M_clean.parquet/\n  20. df_otpw_3m.parquet/\n\n  ... and 10 more folders\n\n================================================================================\n DELETION COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DELETE SPECIFIED FOLDERS AND ALL THEIR CONTENTS\n",
    "# ============================================================================\n",
    "\n",
    "BASE_DIR = \"dbfs:/student-groups/Group_4_4\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DELETING FOLDERS AND ALL SUBFILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "folders_to_delete = [\n",
    "    \"JOINED_1Y.parquet\",\n",
    "    \"JOINED_3M.parquet\",\n",
    "    \"mart\"\n",
    "]\n",
    "\n",
    "print(f\"\\nFolders to delete: {len(folders_to_delete)}\")\n",
    "\n",
    "def get_size_str(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    elif size_bytes < 1024:\n",
    "        return f\"{size_bytes} B\"\n",
    "    elif size_bytes < 1024**2:\n",
    "        return f\"{size_bytes/1024:.1f} KB\"\n",
    "    elif size_bytes < 1024**3:\n",
    "        return f\"{size_bytes/(1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{size_bytes/(1024**3):.2f} GB\"\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Check folders and calculate sizes\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: Checking folders before deletion\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "folders_found = []\n",
    "folders_not_found = []\n",
    "total_size_to_free = 0\n",
    "\n",
    "for folder_name in folders_to_delete:\n",
    "    folder_path = f\"{BASE_DIR}/{folder_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if folder exists\n",
    "        contents = dbutils.fs.ls(folder_path)\n",
    "        \n",
    "        # Calculate folder size and count files/subdirs\n",
    "        folder_size = 0\n",
    "        file_count = 0\n",
    "        subdir_count = 0\n",
    "        \n",
    "        def count_recursive(path):\n",
    "            \"\"\"Recursively count files and calculate total size\"\"\"\n",
    "            size = 0\n",
    "            files = 0\n",
    "            subdirs = 0\n",
    "            \n",
    "            try:\n",
    "                items = dbutils.fs.ls(path)\n",
    "                for item in items:\n",
    "                    if item.name.endswith('/'):\n",
    "                        subdirs += 1\n",
    "                        sub_size, sub_files, sub_subdirs = count_recursive(item.path)\n",
    "                        size += sub_size\n",
    "                        files += sub_files\n",
    "                        subdirs += sub_subdirs\n",
    "                    else:\n",
    "                        files += 1\n",
    "                        size += item.size\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return size, files, subdirs\n",
    "        \n",
    "        folder_size, file_count, subdir_count = count_recursive(folder_path)\n",
    "        total_size_to_free = total_size_to_free + folder_size\n",
    "        \n",
    "        folders_found.append({\n",
    "            'name': folder_name,\n",
    "            'path': folder_path,\n",
    "            'size': folder_size,\n",
    "            'files': file_count,\n",
    "            'subdirs': subdir_count\n",
    "        })\n",
    "        \n",
    "        size_str = get_size_str(folder_size)\n",
    "        \n",
    "        if subdir_count > 0:\n",
    "            print(f\" Found: {folder_name:50s} {subdir_count:>3} subdirs, {file_count:>4} files, {size_str:>10s}\")\n",
    "        else:\n",
    "            print(f\" Found: {folder_name:50s} {file_count:>4} files, {size_str:>10s}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        folders_not_found.append(folder_name)\n",
    "        print(f\" Not found: {folder_name}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"Folders found: {len(folders_found)}/{len(folders_to_delete)}\")\n",
    "print(f\"Folders not found: {len(folders_not_found)}\")\n",
    "print(f\"Total space to be freed: {get_size_str(total_size_to_free)}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Confirmation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Confirmation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(folders_found) == 0:\n",
    "    print(\"\\n  No folders found to delete. Exiting.\")\n",
    "else:\n",
    "    print(f\"\\n  You are about to PERMANENTLY DELETE {len(folders_found)} folders\")\n",
    "    print(f\"  This includes ALL subfolders and files\")\n",
    "    print(f\"  This will free up approximately {get_size_str(total_size_to_free)}\")\n",
    "    print(\"\\n Proceeding with deletion...\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Step 3: Delete folders\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: Deleting folders\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    deleted_count = 0\n",
    "    failed_count = 0\n",
    "    total_freed = 0\n",
    "    \n",
    "    for folder in folders_found:\n",
    "        try:\n",
    "            print(f\"Deleting: {folder['name']}\")\n",
    "            print(f\"  Removing {folder['files']} files across {folder['subdirs']} subdirectories...\")\n",
    "            \n",
    "            dbutils.fs.rm(folder['path'], recurse=True)\n",
    "            \n",
    "            deleted_count += 1\n",
    "            total_freed = total_freed + folder['size']\n",
    "            print(f\"   Deleted successfully ({get_size_str(folder['size'])} freed)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            print(f\"   Failed to delete: {str(e)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Step 4: Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DELETION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n Successfully deleted: {deleted_count} folders\")\n",
    "    print(f\" Failed to delete: {failed_count} folders\")\n",
    "    print(f\"\uD83D\uDCE6 Total space freed: {get_size_str(total_freed)}\")\n",
    "    \n",
    "    if folders_not_found:\n",
    "        print(f\"\\n  Folders not found (already deleted?): {len(folders_not_found)}\")\n",
    "        for fname in folders_not_found:\n",
    "            print(f\"    - {fname}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Step 5: Verify deletion\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: Verification\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nVerifying deletion...\")\n",
    "    still_exist = []\n",
    "    \n",
    "    for folder in folders_found:\n",
    "        try:\n",
    "            dbutils.fs.ls(folder['path'])\n",
    "            still_exist.append(folder['name'])\n",
    "            print(f\"    Still exists: {folder['name']}\")\n",
    "        except:\n",
    "            print(f\"   Confirmed deleted: {folder['name']}\")\n",
    "    \n",
    "    if still_exist:\n",
    "        print(f\"\\n  WARNING: {len(still_exist)} folders still exist!\")\n",
    "    else:\n",
    "        print(f\"\\n All {deleted_count} folders successfully deleted and verified\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Step 6: Show remaining folders\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 6: Remaining folders\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        remaining = dbutils.fs.ls(BASE_DIR)\n",
    "        remaining_folders = [f for f in remaining if f.name.endswith('/')]\n",
    "        \n",
    "        print(f\"\\nRemaining folders in base directory: {len(remaining_folders)}\")\n",
    "        \n",
    "        if len(remaining_folders) <= 20:\n",
    "            print(\"\\nAll remaining folders:\")\n",
    "            for i, f in enumerate(sorted(remaining_folders, key=lambda x: x.name), 1):\n",
    "                print(f\"  {i:2d}. {f.name}\")\n",
    "        else:\n",
    "            print(\"\\nFirst 20 remaining folders:\")\n",
    "            for i, f in enumerate(sorted(remaining_folders, key=lambda x: x.name)[:20], 1):\n",
    "                print(f\"  {i:2d}. {f.name}\")\n",
    "            print(f\"\\n  ... and {len(remaining_folders) - 20} more folders\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing remaining folders: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" DELETION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a439c5-e190-44dc-995a-8ab77037f5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Team_4_4_Folder_Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}