{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b4df04-02cb-4580-b7bf-87ee0db3758c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "df = spark.read.parquet(f\"{folder_path}/otpw_12m.parquet\")\n",
    "\n",
    "# binary target label we are predicting\n",
    "df = df.withColumn(\"label\", when(col(\"DEP_DELAY\").cast(\"double\") > 15, 1).otherwise(0))\n",
    "\n",
    "# average delay rates\n",
    "carrier_delay = df.groupBy(\"OP_UNIQUE_CARRIER\").agg(F.mean(\"label\").alias(\"CARRIER_DELAY_RATE\"))\n",
    "origin_delay = df.groupBy(\"ORIGIN\").agg(F.mean(\"label\").alias(\"ORIGIN_DELAY_RATE\"))\n",
    "dest_delay = df.groupBy(\"DEST\").agg(F.mean(\"label\").alias(\"DEST_DELAY_RATE\"))\n",
    "\n",
    "df = df.join(carrier_delay, on=\"OP_UNIQUE_CARRIER\", how=\"left\")\n",
    "df = df.join(origin_delay, on=\"ORIGIN\", how=\"left\")\n",
    "df = df.join(dest_delay, on=\"DEST\", how=\"left\")\n",
    "\n",
    "# time features\n",
    "df = df.withColumn(\"DEP_HOUR\", (col(\"CRS_DEP_TIME\").cast(\"int\") / 100).cast(\"int\"))\n",
    "df = df.withColumn(\"IS_WEEKEND\", when(col(\"DAY_OF_WEEK\").isin(\"6\", \"7\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_WINTER\", when(col(\"MONTH\").isin(\"12\", \"1\", \"2\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_SPRING\", when(col(\"MONTH\").isin(\"3\", \"4\", \"5\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_SUMMER\", when(col(\"MONTH\").isin(\"6\", \"7\", \"8\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_FALL\", when(col(\"MONTH\").isin(\"9\", \"10\", \"11\"), 1).otherwise(0))\n",
    "holiday_dates = [\n",
    "    \"2015-01-01\",  # New Year's Day\n",
    "    \"2015-01-19\",  # Martin Luther King Jr. Day\n",
    "    \"2015-02-14\",  # Valentine's Day\n",
    "    \"2015-02-16\",  # Presidents Day\n",
    "    \"2015-05-25\",  # Memorial Day\n",
    "    \"2015-07-04\",  # Independence Day\n",
    "    \"2015-09-07\",  # Labor Day\n",
    "    \"2015-11-26\",  # Thanksgiving\n",
    "    \"2015-12-25\"   # Christmas\n",
    "    \"2015-12-31\"   # New Year's Eve\n",
    "]\n",
    "df = df.withColumn(\n",
    "    \"IS_HOLIDAY\",\n",
    "    when(col(\"FL_DATE\").isin(holiday_dates), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# window definitions\n",
    "w_tailnum = W.Window.partitionBy(\"TAIL_NUM\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\")\n",
    "w_airport_history = W.Window.partitionBy(\"ORIGIN\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-sys.maxsize, -1)\n",
    "w_dest_history = W.Window.partitionBy(\"DEST\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-sys.maxsize, -1)\n",
    "\n",
    "df = df.withColumn(\"Prev_TaxiIn\", F.lag(\"TAXI_IN\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_TaxiOut\", F.lag(\"TAXI_OUT\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_ArrDelay\", F.lag(\"ARR_DELAY\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_ArrTime\", F.lag(\"ARR_TIME\").over(w_tailnum))\n",
    "df = df.withColumn(\"Turnaround_Time\", (col(\"CRS_DEP_TIME\").cast(\"int\") - col(\"Prev_ArrTime\").cast(\"int\")))\n",
    "\n",
    "# convert times to timestamp for rolling window logic\n",
    "df = df.withColumn(\"DEP_DATETIME\", F.concat_ws(\" \", col(\"FL_DATE\"), F.format_string(\"%04d\", col(\"CRS_DEP_TIME\").cast(\"int\"))))\n",
    "df = df.withColumn(\"DEP_TIMESTAMP\", F.unix_timestamp(\"DEP_DATETIME\", \"yyyy-MM-dd HHmm\"))\n",
    "\n",
    "# delayed or cancelled flights at airport within prior 2 hours\n",
    "w_airport_2h = (\n",
    "    W.Window.partitionBy(\"ORIGIN\")\n",
    "    .orderBy(\"DEP_TIMESTAMP\")\n",
    "    .rangeBetween(-7200, -1)  # last 2 hours\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Num_airport_wide_delays\",\n",
    "    F.sum(F.when(col(\"DEP_DELAY\").cast(\"double\") > 15, 1).otherwise(0)).over(w_airport_2h)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Num_airport_wide_cancelations\",\n",
    "    F.sum(F.when(col(\"CANCELLED\").cast(\"double\") == 1, 1).otherwise(0)).over(w_airport_2h)\n",
    ")\n",
    "\n",
    "# flights arriving at origin 2 hours before scheduled departure\n",
    "df = df.withColumn(\n",
    "    \"Oncoming_flights\",\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            (col(\"ARR_TIME\").cast(\"int\") >= (col(\"CRS_DEP_TIME\").cast(\"int\") - 200)) &\n",
    "            (col(\"ARR_TIME\").cast(\"int\") <= col(\"CRS_DEP_TIME\").cast(\"int\")), 1\n",
    "        ).otherwise(0)\n",
    "    ).over(W.Window.partitionBy(\"ORIGIN\", \"FL_DATE\"))\n",
    ")\n",
    "\n",
    "# rolling on time arrival and departure percentages\n",
    "df = df.withColumn(\n",
    "    \"OntimeArrivalPct\",\n",
    "    F.avg(F.when(col(\"ARR_DELAY\").cast(\"double\") <= 0, 1).otherwise(0)).over(w_dest_history)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"OntimeDeparturePct\",\n",
    "    F.avg(F.when(col(\"DEP_DELAY\").cast(\"double\") <= 0, 1).otherwise(0)).over(w_airport_history)\n",
    ")\n",
    "\n",
    "base_features = [\n",
    "    \"YEAR\", \"QUARTER\", \"MONTH\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"DISTANCE\",\n",
    "    \"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\",\n",
    "    \"ORIGIN_WAC\", \"DEST_WAC\",\n",
    "    \"HourlyPrecipitation\", \"HourlyVisibility\", \"HourlyWindSpeed\",\n",
    "    \"HourlyWindGustSpeed\", \"HourlyWindDirection\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\", \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\",\n",
    "    \"HourlyPresentWeatherType\"\n",
    "]\n",
    "engineered_features = [\n",
    "    \"DEP_HOUR\", \"IS_WEEKEND\", \"IS_WINTER\", \"IS_SPRING\", \"IS_SUMMER\", \"IS_FALL\", \"IS_HOLIDAY\",\n",
    "    \"CARRIER_DELAY_RATE\", \"ORIGIN_DELAY_RATE\", \"DEST_DELAY_RATE\",\n",
    "    \"Prev_TaxiIn\", \"Prev_TaxiOut\", \"Prev_ArrDelay\", \"Turnaround_Time\",\n",
    "    \"Num_airport_wide_delays\", \"Num_airport_wide_cancelations\", \"Oncoming_flights\",\n",
    "    \"OntimeArrivalPct\", \"OntimeDeparturePct\"\n",
    "]\n",
    "all_features = base_features + engineered_features\n",
    "\n",
    "for c in all_features:\n",
    "    df = df.withColumn(c + \"_dbl\", col(c).cast(\"double\"))\n",
    "\n",
    "# drop columns with too many nulls\n",
    "null_threshold = 1000000\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "sparse_cols = [col for col, count in null_counts.items() if count > null_threshold]\n",
    "df = df.drop(*sparse_cols)\n",
    "\n",
    "# drop rows with nulls\n",
    "df = df.dropna()\n",
    "\n",
    "final_cols = [c + \"_dbl\" for c in all_features if (c not in sparse_cols)]\n",
    "df_model = df.select([\"label\"] + final_cols)\n",
    "df_model = df_model.orderBy(\"FL_DATE\", \"CRS_DEP_TIME\", \"ORIGIN\", \"DEST\")\n",
    "df_model = df_model.cache()\n",
    "df_model.count()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=final_cols, outputCol=\"features\")\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=5,\n",
    "    maxIter=20,\n",
    "    stepSize=0.1,\n",
    "    subsamplingRate=1.0,\n",
    "    featureSubsetStrategy=\"all\",\n",
    "    seed=31\n",
    ")\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "model = pipeline.fit(df_model)\n",
    "\n",
    "importances = model.stages[-1].featureImportances.toArray()\n",
    "feature_tuples = [(feature, float(importance)) for feature, importance in zip(final_cols, importances)]\n",
    "schema = StructType([\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"importance\", DoubleType(), False)\n",
    "])\n",
    "feature_importance_df = spark.createDataFrame(feature_tuples, schema).orderBy(col(\"importance\").desc())\n",
    "\n",
    "display(feature_importance_df)\n",
    "\n",
    "pandas_df = feature_importance_df.toPandas()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(pandas_df[\"feature\"], pandas_df[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"GBTClassifier Feature Importances\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d241201-e81b-4546-af0f-5228e6a5bb42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic regression baseline model\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "import sys\n",
    "\n",
    "df = spark.read.parquet(f\"{folder_path}/otpw_12m.parquet\")\n",
    "\n",
    "df = df.withColumn(\"label\", when(col(\"DEP_DELAY\").cast(\"double\") > 15, 1).otherwise(0))\n",
    "\n",
    "carrier_delay = df.groupBy(\"OP_UNIQUE_CARRIER\").agg(F.mean(\"label\").alias(\"CARRIER_DELAY_RATE\"))\n",
    "origin_delay = df.groupBy(\"ORIGIN\").agg(F.mean(\"label\").alias(\"ORIGIN_DELAY_RATE\"))\n",
    "dest_delay = df.groupBy(\"DEST\").agg(F.mean(\"label\").alias(\"DEST_DELAY_RATE\"))\n",
    "df = df.join(carrier_delay, on=\"OP_UNIQUE_CARRIER\", how=\"left\")\n",
    "df = df.join(origin_delay, on=\"ORIGIN\", how=\"left\")\n",
    "df = df.join(dest_delay, on=\"DEST\", how=\"left\")\n",
    "\n",
    "df = df.withColumn(\"DEP_HOUR\", (col(\"CRS_DEP_TIME\").cast(\"int\") / 100).cast(\"int\"))\n",
    "df = df.withColumn(\"IS_WEEKEND\", when(col(\"DAY_OF_WEEK\").isin(\"6\", \"7\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_WINTER\", when(col(\"MONTH\").isin(\"12\", \"1\", \"2\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_SPRING\", when(col(\"MONTH\").isin(\"3\", \"4\", \"5\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_SUMMER\", when(col(\"MONTH\").isin(\"6\", \"7\", \"8\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_FALL\", when(col(\"MONTH\").isin(\"9\", \"10\", \"11\"), 1).otherwise(0))\n",
    "holiday_dates = [\n",
    "    \"2015-01-01\",  # New Year's Day\n",
    "    \"2015-01-19\",  # Martin Luther King Jr. Day\n",
    "    \"2015-02-14\",  # Valentine's Day\n",
    "    \"2015-02-16\",  # Presidents Day\n",
    "    \"2015-05-25\",  # Memorial Day\n",
    "    \"2015-07-04\",  # Independence Day\n",
    "    \"2015-09-07\",  # Labor Day\n",
    "    \"2015-11-26\",  # Thanksgiving\n",
    "    \"2015-12-25\",  # Christmas\n",
    "    \"2015-12-31\"   # New Year's Eve\n",
    "]\n",
    "df = df.withColumn(\n",
    "    \"IS_HOLIDAY\",\n",
    "    when(col(\"FL_DATE\").isin(holiday_dates), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "w_tailnum = W.Window.partitionBy(\"TAIL_NUM\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\")\n",
    "w_airport_history = W.Window.partitionBy(\"ORIGIN\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-sys.maxsize, -1)\n",
    "w_dest_history = W.Window.partitionBy(\"DEST\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-sys.maxsize, -1)\n",
    "\n",
    "df = df.withColumn(\"Prev_TaxiIn\", F.lag(\"TAXI_IN\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_TaxiOut\", F.lag(\"TAXI_OUT\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_ArrDelay\", F.lag(\"ARR_DELAY\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_ArrTime\", F.lag(\"ARR_TIME\").over(w_tailnum))\n",
    "df = df.withColumn(\"Turnaround_Time\", (col(\"CRS_DEP_TIME\").cast(\"int\") - col(\"Prev_ArrTime\").cast(\"int\")))\n",
    "\n",
    "df = df.withColumn(\"DEP_DATETIME\", F.concat_ws(\" \", col(\"FL_DATE\"), F.format_string(\"%04d\", col(\"CRS_DEP_TIME\").cast(\"int\"))))\n",
    "df = df.withColumn(\"DEP_TIMESTAMP\", F.unix_timestamp(\"DEP_DATETIME\", \"yyyy-MM-dd HHmm\"))\n",
    "w_airport_2h = (\n",
    "    W.Window.partitionBy(\"ORIGIN\")\n",
    "    .orderBy(\"DEP_TIMESTAMP\")\n",
    "    .rangeBetween(-7200, -1)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Num_airport_wide_delays\",\n",
    "    F.sum(F.when(col(\"DEP_DELAY\").cast(\"double\") > 15, 1).otherwise(0)).over(w_airport_2h)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Num_airport_wide_cancelations\",\n",
    "    F.sum(F.when(col(\"CANCELLED\").cast(\"double\") == 1, 1).otherwise(0)).over(w_airport_2h)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Oncoming_flights\",\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            (col(\"ARR_TIME\").cast(\"int\") >= (col(\"CRS_DEP_TIME\").cast(\"int\") - 200)) &\n",
    "            (col(\"ARR_TIME\").cast(\"int\") <= col(\"CRS_DEP_TIME\").cast(\"int\")), 1\n",
    "        ).otherwise(0)\n",
    "    ).over(W.Window.partitionBy(\"ORIGIN\", \"FL_DATE\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"OntimeArrivalPct\",\n",
    "    F.avg(F.when(col(\"ARR_DELAY\").cast(\"double\") <= 0, 1).otherwise(0)).over(w_dest_history)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"OntimeDeparturePct\",\n",
    "    F.avg(F.when(col(\"DEP_DELAY\").cast(\"double\") <= 0, 1).otherwise(0)).over(w_airport_history)\n",
    ")\n",
    "\n",
    "base_features = [\n",
    "    \"YEAR\", \"QUARTER\", \"MONTH\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"DISTANCE\",\n",
    "    \"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\",\n",
    "    \"ORIGIN_WAC\", \"DEST_WAC\",\n",
    "    \"HourlyPrecipitation\", \"HourlyVisibility\", \"HourlyWindSpeed\",\n",
    "    \"HourlyWindGustSpeed\", \"HourlyWindDirection\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\", \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\",\n",
    "    \"HourlyPresentWeatherType\"\n",
    "]\n",
    "engineered_features = [\n",
    "    \"DEP_HOUR\", \"IS_WEEKEND\", \"IS_WINTER\", \"IS_SPRING\", \"IS_SUMMER\", \"IS_FALL\", \"IS_HOLIDAY\",\n",
    "    \"CARRIER_DELAY_RATE\", \"ORIGIN_DELAY_RATE\", \"DEST_DELAY_RATE\",\n",
    "    \"Prev_TaxiIn\", \"Prev_TaxiOut\", \"Prev_ArrDelay\", \"Turnaround_Time\",\n",
    "    \"Num_airport_wide_delays\", \"Num_airport_wide_cancelations\", \"Oncoming_flights\",\n",
    "    \"OntimeArrivalPct\", \"OntimeDeparturePct\"\n",
    "]\n",
    "all_features = base_features + engineered_features\n",
    "\n",
    "for c in all_features:\n",
    "    df = df.withColumn(c + \"_dbl\", col(c).cast(\"double\"))\n",
    "\n",
    "# Drop columns with too many nulls\n",
    "null_threshold = 1000000\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "sparse_cols = [col for col, count in null_counts.items() if count > null_threshold]\n",
    "df = df.drop(*sparse_cols)\n",
    "\n",
    "# Drop rows with nulls\n",
    "df = df.dropna()\n",
    "\n",
    "# Categorical columns to index\n",
    "categorical_cols = [\"ORIGIN\", \"DEST\", \"OP_UNIQUE_CARRIER\"]\n",
    "\n",
    "# Define StringIndexers for categoricals\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid='keep') for c in categorical_cols\n",
    "]\n",
    "\n",
    "# Final feature list\n",
    "numeric_features = [c + \"_dbl\" for c in all_features if (c not in sparse_cols)]\n",
    "feature_cols = [f\"{c}_idx\" for c in categorical_cols] + numeric_features\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# First 3 quarters for training, last quarter for test\n",
    "train_df = df.filter(col(\"MONTH\").between(1, 9))\n",
    "test_df = df.filter(col(\"MONTH\").between(10, 12))\n",
    "\n",
    "# Define logistic regression and pipeline\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "\n",
    "# Train\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate overall AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Baseline Logistic Regression AUC: {auc:.3f}\")\n",
    "\n",
    "pdf = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "cm = confusion_matrix(pdf[\"label\"], pdf[\"prediction\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Not Delayed\", \"Delayed\"], yticklabels=[\"Not Delayed\", \"Delayed\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1 for Delayed class (label=1)\n",
    "precision_delayed = precision_score(pdf[\"label\"], pdf[\"prediction\"], pos_label=1)\n",
    "recall_delayed = recall_score(pdf[\"label\"], pdf[\"prediction\"], pos_label=1)\n",
    "f1_delayed = f1_score(pdf[\"label\"], pdf[\"prediction\"], pos_label=1)\n",
    "\n",
    "print(f\"Precision (Delayed class): {precision_delayed:.3f}\")\n",
    "print(f\"Recall (Delayed class): {recall_delayed:.3f}\")\n",
    "print(f\"F1 (Delayed class): {f1_delayed:.3f}\")\n",
    "\n",
    "# Full classification report for both classes\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(pdf[\"label\"], pdf[\"prediction\"], target_names=[\"Not Delayed\", \"Delayed\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ae47bd-c504-46e9-979b-3153e573415c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic regression with 3 month blocking time series cross-validation and grid search for parameter selection for first 3/4 of data for training, last 1/4 for test \n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "df = spark.read.parquet(f\"{folder_path}/otpw_12m.parquet\")\n",
    "df = df.withColumn(\"label\", when(col(\"DEP_DELAY\").cast(\"double\") > 15, 1).otherwise(0))\n",
    "carrier_delay = df.groupBy(\"OP_UNIQUE_CARRIER\").agg(F.mean(\"label\").alias(\"CARRIER_DELAY_RATE\"))\n",
    "origin_delay = df.groupBy(\"ORIGIN\").agg(F.mean(\"label\").alias(\"ORIGIN_DELAY_RATE\"))\n",
    "dest_delay = df.groupBy(\"DEST\").agg(F.mean(\"label\").alias(\"DEST_DELAY_RATE\"))\n",
    "df = df.join(carrier_delay, on=\"OP_UNIQUE_CARRIER\", how=\"left\")\n",
    "df = df.join(origin_delay, on=\"ORIGIN\", how=\"left\")\n",
    "df = df.join(dest_delay, on=\"DEST\", how=\"left\")\n",
    "df = df.withColumn(\"DEP_HOUR\", (col(\"CRS_DEP_TIME\").cast(\"int\") / 100).cast(\"int\"))\n",
    "df = df.withColumn(\"IS_WEEKEND\", when(col(\"DAY_OF_WEEK\").isin(\"6\", \"7\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_WINTER\", when(col(\"MONTH\").isin(\"12\", \"1\", \"2\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_SPRING\", when(col(\"MONTH\").isin(\"3\", \"4\", \"5\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_SUMMER\", when(col(\"MONTH\").isin(\"6\", \"7\", \"8\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"IS_FALL\", when(col(\"MONTH\").isin(\"9\", \"10\", \"11\"), 1).otherwise(0))\n",
    "holiday_dates = [\n",
    "    \"2015-01-01\", \"2015-01-19\", \"2015-02-14\", \"2015-02-16\", \"2015-05-25\",\n",
    "    \"2015-07-04\", \"2015-09-07\", \"2015-11-26\", \"2015-12-25\", \"2015-12-31\"\n",
    "]\n",
    "df = df.withColumn(\"IS_HOLIDAY\", when(col(\"FL_DATE\").isin(holiday_dates), 1).otherwise(0))\n",
    "w_tailnum = W.Window.partitionBy(\"TAIL_NUM\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\")\n",
    "w_airport_history = W.Window.partitionBy(\"ORIGIN\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-sys.maxsize, -1)\n",
    "w_dest_history = W.Window.partitionBy(\"DEST\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-sys.maxsize, -1)\n",
    "df = df.withColumn(\"Prev_TaxiIn\", F.lag(\"TAXI_IN\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_TaxiOut\", F.lag(\"TAXI_OUT\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_ArrDelay\", F.lag(\"ARR_DELAY\").over(w_tailnum))\n",
    "df = df.withColumn(\"Prev_ArrTime\", F.lag(\"ARR_TIME\").over(w_tailnum))\n",
    "df = df.withColumn(\"Turnaround_Time\", (col(\"CRS_DEP_TIME\").cast(\"int\") - col(\"Prev_ArrTime\").cast(\"int\")))\n",
    "df = df.withColumn(\"DEP_DATETIME\", F.concat_ws(\" \", col(\"FL_DATE\"), F.format_string(\"%04d\", col(\"CRS_DEP_TIME\").cast(\"int\"))))\n",
    "df = df.withColumn(\"DEP_TIMESTAMP\", F.unix_timestamp(\"DEP_DATETIME\", \"yyyy-MM-dd HHmm\"))\n",
    "w_airport_2h = (W.Window.partitionBy(\"ORIGIN\").orderBy(\"DEP_TIMESTAMP\").rangeBetween(-7200, -1))\n",
    "df = df.withColumn(\"Num_airport_wide_delays\", F.sum(F.when(col(\"DEP_DELAY\").cast(\"double\") > 15, 1).otherwise(0)).over(w_airport_2h))\n",
    "df = df.withColumn(\"Num_airport_wide_cancelations\", F.sum(F.when(col(\"CANCELLED\").cast(\"double\") == 1, 1).otherwise(0)).over(w_airport_2h))\n",
    "df = df.withColumn(\"Oncoming_flights\", F.sum(F.when((col(\"ARR_TIME\").cast(\"int\") >= (col(\"CRS_DEP_TIME\").cast(\"int\") - 200)) & (col(\"ARR_TIME\").cast(\"int\") <= col(\"CRS_DEP_TIME\").cast(\"int\")), 1).otherwise(0)).over(W.Window.partitionBy(\"ORIGIN\", \"FL_DATE\")))\n",
    "df = df.withColumn(\"OntimeArrivalPct\", F.avg(F.when(col(\"ARR_DELAY\").cast(\"double\") <= 0, 1).otherwise(0)).over(w_dest_history))\n",
    "df = df.withColumn(\"OntimeDeparturePct\", F.avg(F.when(col(\"DEP_DELAY\").cast(\"double\") <= 0, 1).otherwise(0)).over(w_airport_history))\n",
    "\n",
    "base_features = [\n",
    "    \"YEAR\", \"QUARTER\", \"MONTH\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"DISTANCE\",\n",
    "    \"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\",\n",
    "    \"ORIGIN_WAC\", \"DEST_WAC\",\n",
    "    \"HourlyPrecipitation\", \"HourlyVisibility\", \"HourlyWindSpeed\",\n",
    "    \"HourlyWindGustSpeed\", \"HourlyWindDirection\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\", \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\",\n",
    "    \"HourlyPresentWeatherType\"\n",
    "]\n",
    "engineered_features = [\n",
    "    \"DEP_HOUR\", \"IS_WEEKEND\", \"IS_WINTER\", \"IS_SPRING\", \"IS_SUMMER\", \"IS_FALL\", \"IS_HOLIDAY\",\n",
    "    \"CARRIER_DELAY_RATE\", \"ORIGIN_DELAY_RATE\", \"DEST_DELAY_RATE\",\n",
    "    \"Prev_TaxiIn\", \"Prev_TaxiOut\", \"Prev_ArrDelay\", \"Turnaround_Time\",\n",
    "    \"Num_airport_wide_delays\", \"Num_airport_wide_cancelations\", \"Oncoming_flights\",\n",
    "    \"OntimeArrivalPct\", \"OntimeDeparturePct\"\n",
    "]\n",
    "all_features = base_features + engineered_features\n",
    "\n",
    "for c in all_features:\n",
    "    df = df.withColumn(c + \"_dbl\", col(c).cast(\"double\"))\n",
    "\n",
    "null_threshold = 1000000\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "sparse_cols = [col for col, count in null_counts.items() if count > null_threshold]\n",
    "df = df.drop(*sparse_cols)\n",
    "df = df.dropna()\n",
    "\n",
    "categorical_cols = [\"ORIGIN\", \"DEST\", \"OP_UNIQUE_CARRIER\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid='keep') for c in categorical_cols\n",
    "]\n",
    "numeric_features = [c + \"_dbl\" for c in all_features if (c not in sparse_cols)]\n",
    "feature_cols = [f\"{c}_idx\" for c in categorical_cols] + numeric_features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "train_df = df.filter(col(\"MONTH\").between(1, 9))\n",
    "test_df = df.filter(col(\"MONTH\").between(10, 12))\n",
    "\n",
    "\n",
    "blocks = [(1, 3), (4, 6), (7, 9)]\n",
    "\n",
    "param_grid = [\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.0},   # Ridge\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.5},   # Elastic Net\n",
    "    {\"regParam\": 0.1,  \"elasticNetParam\": 0.0},   # Ridge\n",
    "    {\"regParam\": 0.1,  \"elasticNetParam\": 0.5},   # Elastic Net\n",
    "    {\"regParam\": 1.0,  \"elasticNetParam\": 0.0},   # Ridge\n",
    "    {\"regParam\": 1.0,  \"elasticNetParam\": 0.5},   # Elastic Net\n",
    "]\n",
    "\n",
    "all_grid_results = []\n",
    "for params in param_grid:\n",
    "    cv_results = []\n",
    "    for i in range(1, len(blocks)):\n",
    "        train_months = [month for b in blocks[:i] for month in range(b[0], b[1] + 1)]\n",
    "        val_months = list(range(blocks[i][0], blocks[i][1] + 1))\n",
    "\n",
    "        cv_train = train_df.filter(col(\"MONTH\").isin(train_months))\n",
    "        cv_val = train_df.filter(col(\"MONTH\").isin(val_months))\n",
    "\n",
    "        if cv_train.count() == 0 or cv_val.count() == 0:\n",
    "            continue\n",
    "\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\",\n",
    "                               regParam=params[\"regParam\"],\n",
    "                               elasticNetParam=params[\"elasticNetParam\"])\n",
    "        pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "\n",
    "        cv_model = pipeline.fit(cv_train)\n",
    "        cv_pred = cv_model.transform(cv_val)\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(cv_pred)\n",
    "\n",
    "        pdf = cv_pred.select(\"label\", \"prediction\").toPandas()\n",
    "        if len(pdf) == 0:\n",
    "            continue\n",
    "\n",
    "        precision_delayed = precision_score(pdf[\"label\"], pdf[\"prediction\"], pos_label=1)\n",
    "        recall_delayed = recall_score(pdf[\"label\"], pdf[\"prediction\"], pos_label=1)\n",
    "        f1_delayed = f1_score(pdf[\"label\"], pdf[\"prediction\"], pos_label=1)\n",
    "        accuracy = (pdf[\"label\"] == pdf[\"prediction\"]).mean()\n",
    "\n",
    "        cv_results.append({\n",
    "            \"regParam\": params[\"regParam\"],\n",
    "            \"elasticNetParam\": params[\"elasticNetParam\"],\n",
    "            \"AUC\": auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision_delayed\": precision_delayed,\n",
    "            \"recall_delayed\": recall_delayed,\n",
    "            \"f1_delayed\": f1_delayed\n",
    "        })\n",
    "    if len(cv_results) > 0:\n",
    "        df_cv_results = pd.DataFrame(cv_results)\n",
    "        avg_auc = df_cv_results[\"AUC\"].mean()\n",
    "        avg_f1 = df_cv_results[\"f1_delayed\"].mean()\n",
    "        all_grid_results.append({\n",
    "            \"regParam\": params[\"regParam\"],\n",
    "            \"elasticNetParam\": params[\"elasticNetParam\"],\n",
    "            \"mean_AUC\": avg_auc,\n",
    "            \"mean_f1\": avg_f1\n",
    "        })\n",
    "\n",
    "grid_results_df = pd.DataFrame(all_grid_results)\n",
    "print(\"\\nGrid search CV results:\")\n",
    "print(grid_results_df.sort_values(\"mean_f1\", ascending=False))\n",
    "\n",
    "# Select best params\n",
    "best_row = grid_results_df.loc[grid_results_df[\"mean_f1\"].idxmax()]\n",
    "best_reg = best_row[\"regParam\"]\n",
    "best_elastic = best_row[\"elasticNetParam\"]\n",
    "print(f\"\\nBest parameters: regParam={best_reg}, elasticNetParam={best_elastic}\")\n",
    "\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\",\n",
    "                       regParam=best_reg,\n",
    "                       elasticNetParam=best_elastic)\n",
    "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "final_model = pipeline.fit(train_df)\n",
    "final_predictions = final_model.transform(test_df)\n",
    "final_pdf = final_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "final_auc = evaluator.evaluate(final_predictions)\n",
    "precision_delayed = precision_score(final_pdf[\"label\"], final_pdf[\"prediction\"], pos_label=1)\n",
    "recall_delayed = recall_score(final_pdf[\"label\"], final_pdf[\"prediction\"], pos_label=1)\n",
    "f1_delayed = f1_score(final_pdf[\"label\"], final_pdf[\"prediction\"], pos_label=1)\n",
    "accuracy = (final_pdf[\"label\"] == final_pdf[\"prediction\"]).mean()\n",
    "\n",
    "print(\"\\nFinal holdout Q4 results:\")\n",
    "print(f\"AUC: {final_auc:.3f}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision (Delayed): {precision_delayed:.3f}\")\n",
    "print(f\"Recall (Delayed): {recall_delayed:.3f}\")\n",
    "print(f\"F1 (Delayed): {f1_delayed:.3f}\")\n",
    "\n",
    "print(\"\\nClassification report for Q4 test set:\\n\")\n",
    "print(classification_report(final_pdf[\"label\"], final_pdf[\"prediction\"], target_names=[\"Not Delayed\", \"Delayed\"]))\n",
    "\n",
    "cm = confusion_matrix(final_pdf[\"label\"], final_pdf[\"prediction\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Not Delayed\", \"Delayed\"], yticklabels=[\"Not Delayed\", \"Delayed\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Q4 Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d964f78-0f9c-4d84-93c5-feca4634f43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def plot_correlation_heatmap(\n",
    "    df, \n",
    "    columns, \n",
    "    sample_size=5000, \n",
    "    title=\"Correlation Matrix Heatmap\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a correlation matrix heatmap for the given Spark DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (Spark DataFrame): Input DataFrame, feature-engineered and all columns numeric.\n",
    "        columns (list): List of column names (strings) to include in the correlation matrix.\n",
    "        sample_size (int): How many rows to sample for speed (default 5000).\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # remove rows with nulls in selected columns and sample\n",
    "    df_corr_sample = df.select(columns).dropna().limit(sample_size)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=columns, outputCol=\"features_corr\")\n",
    "    df_corr_vec = assembler.transform(df_corr_sample)\n",
    "\n",
    "    corr = Correlation.corr(df_corr_vec, \"features_corr\", \"pearson\").head()[0].toArray()\n",
    "\n",
    "    # Convert result to Pandas DataFrame for plotting\n",
    "    corr_df = pd.DataFrame(corr, columns=columns, index=columns)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(1.5*len(columns), 1.2*len(columns)))\n",
    "    sns.heatmap(\n",
    "        corr_df,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"size\": 6},\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.7}\n",
    "    )\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after all feature engineering\n",
    "# corr_cols = [\n",
    "#     \"label_dbl\", \"DEP_HOUR_dbl\", \"CARRIER_DELAY_RATE_dbl\", \"ORIGIN_DELAY_RATE_dbl\",\n",
    "#     \"Prev_TaxiIn_dbl\", \"Prev_TaxiOut_dbl\", \"Prev_ArrDelay_dbl\", \"Turnaround_Time_dbl\"\n",
    "#     # ... add any more _dbl numeric columns you want\n",
    "# ]\n",
    "\n",
    "# plot_correlation_heatmap(df, corr_cols, sample_size=5000, title=\"My Flight Delay Feature Correlation Heatmap\")\n",
    "\n",
    "def plot_gbt_feature_importances(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    label_col=\"label\",\n",
    "    maxDepth=5,\n",
    "    maxIter=20,\n",
    "    stepSize=0.1,\n",
    "    subsamplingRate=1.0,\n",
    "    featureSubsetStrategy=\"all\",\n",
    "    seed=31,\n",
    "    top_n=20,\n",
    "    figsize=(12,6)\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit a GBTClassifier and plot feature importances.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Spark DataFrame with features already engineered (numeric)\n",
    "        feature_cols (list): List of column names (strings) to use as features\n",
    "        label_col (str): Column name for label (default: \"label\")\n",
    "        maxDepth, maxIter, stepSize, subsamplingRate, featureSubsetStrategy, seed: GBTClassifier params\n",
    "        top_n (int): Plot the top_n most important features\n",
    "        figsize (tuple): Matplotlib figure size\n",
    "    \"\"\"\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    gbt = GBTClassifier(\n",
    "        labelCol=label_col,\n",
    "        featuresCol=\"features\",\n",
    "        maxDepth=maxDepth,\n",
    "        maxIter=maxIter,\n",
    "        stepSize=stepSize,\n",
    "        subsamplingRate=subsamplingRate,\n",
    "        featureSubsetStrategy=featureSubsetStrategy,\n",
    "        seed=seed\n",
    "    )\n",
    "    pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "    # fit GBT\n",
    "    model = pipeline.fit(df)\n",
    "\n",
    "    # extract importances\n",
    "    importances = model.stages[-1].featureImportances.toArray()\n",
    "    feature_tuples = [(feature, float(importance)) for feature, importance in zip(feature_cols, importances)]\n",
    "\n",
    "    # create dataframe for sorting and plotting\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", StringType(), False),\n",
    "        StructField(\"importance\", DoubleType(), False)\n",
    "    ])\n",
    "    feature_importance_df = df.sql_ctx.createDataFrame(feature_tuples, schema).orderBy(col(\"importance\").desc())\n",
    "\n",
    "    # toPandas for plotting\n",
    "    pandas_df = feature_importance_df.toPandas().head(top_n)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(pandas_df[\"feature\"], pandas_df[\"importance\"])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(f\"GBTClassifier Feature Importances (Top {top_n})\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return feature_importance_df\n",
    "\n",
    "# Example usage after feature engineering:\n",
    "# feature_cols = [\"DEP_HOUR_dbl\", \"Prev_ArrDelay_dbl\", ...] \n",
    "# feature_importance_df = plot_gbt_feature_importances(df_model, feature_cols, label_col=\"label\", top_n=15)\n",
    "\n",
    "def run_logreg_baseline(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    label_col=\"label\",\n",
    "    split_col=\"MONTH\",\n",
    "    train_months=range(1,10),\n",
    "    test_months=range(10,13),\n",
    "    experiment_name=\"/Users/<your-username>/flight-delay-logreg\",  # set your Databricks user or shared experiment path\n",
    "    run_name=\"logreg_baseline\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train, evaluate, and log a logistic regression baseline on a feature-engineered Spark DataFrame.\n",
    "    Saves the model as a Databricks experiment with MLflow.\n",
    "    \"\"\"\n",
    "    # set experiment (creates if doesn't exist)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col)\n",
    "        pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "        # train/test split by time\n",
    "        train_df = df.filter(col(split_col).isin(train_months))\n",
    "        test_df = df.filter(col(split_col).isin(test_months))\n",
    "\n",
    "        # train model\n",
    "        model = pipeline.fit(train_df)\n",
    "        # save model with MLflow\n",
    "        mlflow.spark.log_model(model, \"logreg-model\")\n",
    "\n",
    "        # predict with test data\n",
    "        predictions = model.transform(test_df)\n",
    "        pdf = predictions.select(label_col, \"prediction\").toPandas()\n",
    "\n",
    "        # metrics\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        precision_delayed = precision_score(pdf[label_col], pdf[\"prediction\"], pos_label=1)\n",
    "        recall_delayed = recall_score(pdf[label_col], pdf[\"prediction\"], pos_label=1)\n",
    "        f1_delayed = f1_score(pdf[label_col], pdf[\"prediction\"], pos_label=1)\n",
    "        accuracy = (pdf[label_col] == pdf[\"prediction\"]).mean()\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Precision_Delayed\", precision_delayed)\n",
    "        mlflow.log_metric(\"Recall_Delayed\", recall_delayed)\n",
    "        mlflow.log_metric(\"F1_Delayed\", f1_delayed)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "\n",
    "        # confusion matrix plot\n",
    "        cm = confusion_matrix(pdf[label_col], pdf[\"prediction\"])\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                    xticklabels=[\"Not Delayed\", \"Delayed\"], yticklabels=[\"Not Delayed\", \"Delayed\"])\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "\n",
    "        # print metrics\n",
    "        print(f\"AUC: {auc:.3f}\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision (Delayed class): {precision_delayed:.3f}\")\n",
    "        print(f\"Recall (Delayed class): {recall_delayed:.3f}\")\n",
    "        print(f\"F1 (Delayed class): {f1_delayed:.3f}\")\n",
    "\n",
    "        print(\"\\nClassification report:\\n\")\n",
    "        print(classification_report(pdf[label_col], pdf[\"prediction\"], target_names=[\"Not Delayed\", \"Delayed\"]))\n",
    "\n",
    "        return model\n",
    "\n",
    "# Usage Example:\n",
    "# feature_cols = [\"DEP_HOUR\", \"Prev_ArrDelay\", ...] # numeric, feature-engineered columns\n",
    "# model = run_logreg_baseline(\n",
    "#     df=my_featured_df,\n",
    "#     feature_cols=feature_cols,\n",
    "#     label_col=\"label\",\n",
    "#     experiment_name=\"/Users/<your-username>/flight-delay-logreg\"\n",
    "# )\n",
    "\n",
    "def run_blocked_ts_logreg_cv(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    label_col=\"label\",\n",
    "    month_col=\"MONTH\",\n",
    "    blocks=[(1, 3), (4, 6), (7, 9)],\n",
    "    test_months=range(10, 13),\n",
    "    param_grid=[\n",
    "        {\"regParam\": 0.01, \"elasticNetParam\": 0.0},   # Ridge\n",
    "        {\"regParam\": 0.01, \"elasticNetParam\": 0.5},   # Elastic Net\n",
    "        {\"regParam\": 0.1,  \"elasticNetParam\": 0.0},   # Ridge\n",
    "        {\"regParam\": 0.1,  \"elasticNetParam\": 0.5},   # Elastic Net\n",
    "        {\"regParam\": 1.0,  \"elasticNetParam\": 0.0},   # Ridge\n",
    "        {\"regParam\": 1.0,  \"elasticNetParam\": 0.5},   # Elastic Net\n",
    "    ],\n",
    "    experiment_name=\"/Users/<your-username>/flight-delay-logreg-cv\",  # change to your workspace path\n",
    "    run_name=\"blocked_ts_logreg_cv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Time series blocked cross-validated logistic regression with grid search.\n",
    "    Assumes input Spark DataFrame has all numeric features, already indexed.\n",
    "    Logs model and results to MLflow (Databricks Experiments).\n",
    "    \"\"\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        all_grid_results = []\n",
    "\n",
    "        for params in param_grid:\n",
    "            cv_results = []\n",
    "            for i in range(1, len(blocks)):\n",
    "                train_months = [month for b in blocks[:i] for month in range(b[0], b[1] + 1)]\n",
    "                val_months = list(range(blocks[i][0], blocks[i][1] + 1))\n",
    "\n",
    "                cv_train = df.filter(col(month_col).isin(train_months))\n",
    "                cv_val = df.filter(col(month_col).isin(val_months))\n",
    "\n",
    "                if cv_train.count() == 0 or cv_val.count() == 0:\n",
    "                    continue\n",
    "\n",
    "                lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col,\n",
    "                                       regParam=params[\"regParam\"],\n",
    "                                       elasticNetParam=params[\"elasticNetParam\"])\n",
    "                pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "                cv_model = pipeline.fit(cv_train)\n",
    "                cv_pred = cv_model.transform(cv_val)\n",
    "\n",
    "                evaluator = BinaryClassificationEvaluator(labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "                auc = evaluator.evaluate(cv_pred)\n",
    "                pdf = cv_pred.select(label_col, \"prediction\").toPandas()\n",
    "                if len(pdf) == 0:\n",
    "                    continue\n",
    "\n",
    "                precision_delayed = precision_score(pdf[label_col], pdf[\"prediction\"], pos_label=1)\n",
    "                recall_delayed = recall_score(pdf[label_col], pdf[\"prediction\"], pos_label=1)\n",
    "                f1_delayed = f1_score(pdf[label_col], pdf[\"prediction\"], pos_label=1)\n",
    "                accuracy = (pdf[label_col] == pdf[\"prediction\"]).mean()\n",
    "\n",
    "                cv_results.append({\n",
    "                    \"regParam\": params[\"regParam\"],\n",
    "                    \"elasticNetParam\": params[\"elasticNetParam\"],\n",
    "                    \"AUC\": auc,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"precision_delayed\": precision_delayed,\n",
    "                    \"recall_delayed\": recall_delayed,\n",
    "                    \"f1_delayed\": f1_delayed\n",
    "                })\n",
    "            if len(cv_results) > 0:\n",
    "                df_cv_results = pd.DataFrame(cv_results)\n",
    "                avg_auc = df_cv_results[\"AUC\"].mean()\n",
    "                avg_f1 = df_cv_results[\"f1_delayed\"].mean()\n",
    "                all_grid_results.append({\n",
    "                    \"regParam\": params[\"regParam\"],\n",
    "                    \"elasticNetParam\": params[\"elasticNetParam\"],\n",
    "                    \"mean_AUC\": avg_auc,\n",
    "                    \"mean_f1\": avg_f1\n",
    "                })\n",
    "\n",
    "        grid_results_df = pd.DataFrame(all_grid_results)\n",
    "        print(\"\\nGrid search CV results:\")\n",
    "        print(grid_results_df.sort_values(\"mean_f1\", ascending=False))\n",
    "\n",
    "        # log grid search results as artifact\n",
    "        grid_results_df.to_csv(\"grid_search_cv_results.csv\", index=False)\n",
    "        mlflow.log_artifact(\"grid_search_cv_results.csv\")\n",
    "\n",
    "        # select best params\n",
    "        best_row = grid_results_df.loc[grid_results_df[\"mean_f1\"].idxmax()]\n",
    "        best_reg = best_row[\"regParam\"]\n",
    "        best_elastic = best_row[\"elasticNetParam\"]\n",
    "        print(f\"\\nBest parameters: regParam={best_reg}, elasticNetParam={best_elastic}\")\n",
    "\n",
    "        mlflow.log_param(\"best_regParam\", best_reg)\n",
    "        mlflow.log_param(\"best_elasticNetParam\", best_elastic)\n",
    "\n",
    "        # train on full training data (first 3/4 of year)\n",
    "        train_months = [month for b in blocks for month in range(b[0], b[1] + 1)]\n",
    "        train_df = df.filter(col(month_col).isin(train_months))\n",
    "        test_df = df.filter(col(month_col).isin(test_months))\n",
    "\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col,\n",
    "                               regParam=best_reg,\n",
    "                               elasticNetParam=best_elastic)\n",
    "        pipeline = Pipeline(stages=[assembler, lr])\n",
    "        final_model = pipeline.fit(train_df)\n",
    "        mlflow.spark.log_model(final_model, \"final-model\")\n",
    "        \n",
    "        # predict with test data (last 1/4 of year)\n",
    "        final_predictions = final_model.transform(test_df)\n",
    "        final_pdf = final_predictions.select(label_col, \"prediction\").toPandas()\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "        final_auc = evaluator.evaluate(final_predictions)\n",
    "        precision_delayed = precision_score(final_pdf[label_col], final_pdf[\"prediction\"], pos_label=1)\n",
    "        recall_delayed = recall_score(final_pdf[label_col], final_pdf[\"prediction\"], pos_label=1)\n",
    "        f1_delayed = f1_score(final_pdf[label_col], final_pdf[\"prediction\"], pos_label=1)\n",
    "        accuracy = (final_pdf[label_col] == final_pdf[\"prediction\"]).mean()\n",
    "\n",
    "        mlflow.log_metric(\"AUC\", final_auc)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"Precision_Delayed\", precision_delayed)\n",
    "        mlflow.log_metric(\"Recall_Delayed\", recall_delayed)\n",
    "        mlflow.log_metric(\"F1_Delayed\", f1_delayed)\n",
    "\n",
    "        print(\"\\nFinal holdout Q4 results:\")\n",
    "        print(f\"AUC: {final_auc:.3f}\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision (Delayed): {precision_delayed:.3f}\")\n",
    "        print(f\"Recall (Delayed): {recall_delayed:.3f}\")\n",
    "        print(f\"F1 (Delayed): {f1_delayed:.3f}\")\n",
    "\n",
    "        print(\"\\nClassification report for Q4 test set:\\n\")\n",
    "        print(classification_report(final_pdf[label_col], final_pdf[\"prediction\"], target_names=[\"Not Delayed\", \"Delayed\"]))\n",
    "\n",
    "        # confusion matrix plot\n",
    "        cm = confusion_matrix(final_pdf[label_col], final_pdf[\"prediction\"])\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                    xticklabels=[\"Not Delayed\", \"Delayed\"], yticklabels=[\"Not Delayed\", \"Delayed\"])\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix (Q4 Test Set)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        \n",
    "        return final_model, grid_results_df\n",
    "\n",
    "# Example usage:\n",
    "# final_model, grid_results_df = run_blocked_ts_logreg_cv(\n",
    "#     df=your_df,\n",
    "#     feature_cols=your_feature_cols,\n",
    "#     label_col=\"label\",\n",
    "#     month_col=\"MONTH\",\n",
    "#     experiment_name=\"/Users/your-user/flight-delay-logreg-cv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eee2ff20-140c-421c-b6f2-ca2b94eee1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd704ce-0f84-4a6a-9394-e9e73a3f4622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c352798-43d8-4f2e-a93c-957a5a0dbe8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93e0ffc7-f926-44e0-9025-e908a2b3cfb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Exclude target and date column\n",
    "exclude_columns = {\"delay_class\", \"sched_depart_date_time_UTC\", \"FL_DATE\"}\n",
    "column_types = dict(df_clean.dtypes)\n",
    "\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "\n",
    "# Separate features by type\n",
    "for col_name, col_type in column_types.items():\n",
    "    if col_name in exclude_columns:\n",
    "        continue\n",
    "        \n",
    "    if col_type in ['int', 'bigint', 'double', 'float']:\n",
    "        categorical_int_cols = {\n",
    "            'DAY_OF_WEEK', 'MONTH', 'QUARTER', 'DISTANCE_GROUP', 'YEAR', 'season'\n",
    "            'ORIGIN_WAC', 'origin_type', 'dest_type', 'DEST_WAC', 'is_weekend', 'is_peak_hour', 'distance_category'\n",
    "            'departure_dayofweek', 'departure_month', 'hour_category', 'is_holiday_window', 'STATION', 'departure_hour'\n",
    "        }\n",
    "        if col_name in categorical_int_cols:\n",
    "            categorical_features.append(col_name)\n",
    "        else:\n",
    "            numerical_features.append(col_name)\n",
    "    elif col_type == 'string':\n",
    "        categorical_features.append(col_name)\n",
    "\n",
    "# Build categorical stages\n",
    "indexers = [StringIndexer(inputCol=f, outputCol=f+\"_indexed\", handleInvalid=\"keep\") for f in categorical_features]\n",
    "encoders = [OneHotEncoder(inputCol=f+\"_indexed\", outputCol=f+\"_encoded\", dropLast=True) for f in categorical_features]\n",
    "\n",
    "# Assemble categorical features\n",
    "categorical_assembler = VectorAssembler(\n",
    "    inputCols=[f+\"_encoded\" for f in categorical_features],\n",
    "    outputCol=\"categorical_features_vector\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Assemble numerical features\n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=numerical_features,\n",
    "    outputCol=\"numerical_features_vector\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numerical_features_vector\",\n",
    "    outputCol=\"scaled_numerical_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# Combine all stages into one pipeline\n",
    "pipeline_stages = indexers + encoders + [categorical_assembler, numerical_assembler, scaler]\n",
    "\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit and transform on the same DataFrame\n",
    "model = pipeline.fit(df_clean)\n",
    "df_transformed = model.transform(df_clean)\n",
    "\n",
    "# Combine categorical and scaled numerical features into final feature vector\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"categorical_features_vector\", \"scaled_numerical_features\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_final = final_assembler.transform(df_transformed)\n",
    "\n",
    "df_modeling = df_final.select(\"features\", \"delay_class\", \"sched_depart_date_time_UTC\", \"FL_DATE\")\n",
    "\n",
    "# Check feature sizes\n",
    "total_features = df_modeling.select(\"features\").first()[0].size\n",
    "categorical_size = total_features - len(numerical_features)\n",
    "\n",
    "print(f\"Total feature vector size: {total_features}\")\n",
    "print(f\"Categorical features (one-hot): {categorical_size}\")\n",
    "print(f\"Numerical features (scaled): {len(numerical_features)}\")\n",
    "print(f\"Dataset ready for PCA and modeling\")\n",
    "row_count = df_modeling.count()\n",
    "print(f\"Total rows in final dataset: {row_count}\")\n",
    "\n",
    "df_modeling.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3477a8db-788c-4b45-a91c-897f20b3c0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# Fit PCA with many components to find variance threshold\n",
    "pca_full = PCA(k=124, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_full_model = pca_full.fit(df_modeling)\n",
    "\n",
    "# Get explained variance ratios\n",
    "explained_variance = pca_full_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Components needed for 95% variance: {components_95}\")\n",
    "\n",
    "# Fit PCA with optimal number of components\n",
    "pca_optimal = PCA(k=components_95, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_optimal_model = pca_optimal.fit(df_modeling)\n",
    "df_final_pca = pca_optimal_model.transform(df_modeling)\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"Optimal PCA components: {components_95}\")\n",
    "print(f\"Variance explained: {cumulative_variance[components_95-1]:.3f}\")\n",
    "\n",
    "final_components = components_95\n",
    "\n",
    "# Select final columns for modeling\n",
    "df_model_ready = df_final_pca.select(\"pca_features\", \"delay_class\", \"sched_depart_date_time_UTC\").withColumnRenamed(\"pca_features\", \"features\")\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"Features: {final_components} PCA components\")\n",
    "print(f\"Target: delay_class (6 classes)\")\n",
    "print(f\"Rows: {df_model_ready.count()}\")\n",
    "\n",
    "# Show class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "df_model_ready.groupBy(\"delay_class\").count().orderBy(\"delay_class\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "099dac52-abcc-448b-b1c5-4d00bf0c41f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939d6252-8d70-4774-8f2c-6294850c4a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "658adcbd-a28d-4f1d-ae92-fd629d021cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ONE-HOT ENCODING FOR CATEGORICAL FEATURES\n",
    "print(\"=== One-Hot Encoding Categorical Features ===\")\n",
    "\n",
    "# Create StringIndexers for all categorical features\n",
    "indexers = []\n",
    "for feature in categorical_features:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=feature, \n",
    "        outputCol=f\"{feature}_indexed\",\n",
    "        handleInvalid=\"keep\"  # Handle any unseen categories\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "\n",
    "# Create OneHotEncoders for all indexed features\n",
    "encoders = []\n",
    "for feature in categorical_features:\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=f\"{feature}_indexed\",\n",
    "        outputCol=f\"{feature}_encoded\",\n",
    "        dropLast=True  # Avoid multicollinearity\n",
    "    )\n",
    "    encoders.append(encoder)\n",
    "\n",
    "# Assemble all one-hot encoded features into a single vector\n",
    "categorical_encoded_cols = [f\"{feature}_encoded\" for feature in categorical_features]\n",
    "categorical_assembler = VectorAssembler(\n",
    "    inputCols=categorical_encoded_cols,\n",
    "    outputCol=\"categorical_features_vector\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(stages=indexers + encoders + [categorical_assembler])\n",
    "\n",
    "print(f\"Created categorical pipeline with {len(categorical_features)} features\")\n",
    "print(f\"Pipeline stages: {len(indexers)} indexers + {len(encoders)} encoders + 1 assembler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e2d92e-c2a0-42d9-95af-678a52da50a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCALING FOR NUMERICAL FEATURES  \n",
    "print(\"=== Scaling Numerical Features ===\")\n",
    "\n",
    "# Assemble numerical features into a single vector\n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=numerical_features,\n",
    "    outputCol=\"numerical_features_vector\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Create StandardScaler (centers around 0, scales to unit variance)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numerical_features_vector\",\n",
    "    outputCol=\"scaled_numerical_features\",\n",
    "    withMean=True,  # Center around 0 \n",
    "    withStd=True    # Scale to unit variance\n",
    ")\n",
    "\n",
    "# Create pipeline for numerical features\n",
    "numerical_pipeline = Pipeline(stages=[numerical_assembler, scaler])\n",
    "\n",
    "print(f\"Created numerical pipeline with {len(numerical_features)} features\")\n",
    "print(\"StandardScaler: withMean=True, withStd=True (good for PCA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83c9599-9388-4b7e-82e1-041315745257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMBINE BOTH PIPELINES AND FIT\n",
    "print(\"=== Fitting Complete Feature Pipeline ===\")\n",
    "\n",
    "# Fit categorical pipeline\n",
    "print(\"Fitting categorical pipeline...\")\n",
    "categorical_model = categorical_pipeline.fit(df_clean)\n",
    "df_categorical = categorical_model.transform(df_clean)\n",
    "\n",
    "# Fit numerical pipeline  \n",
    "print(\"Fitting numerical pipeline...\")\n",
    "numerical_model = numerical_pipeline.fit(df_clean)\n",
    "df_numerical = numerical_model.transform(df_clean)\n",
    "\n",
    "# Combine the results\n",
    "# We need to join the categorical and numerical transformations\n",
    "df_processed = df_categorical.select(\n",
    "    \"DEP_DELAY_GROUP\",  # Keep target variable\n",
    "    \"categorical_features_vector\"\n",
    ").join(\n",
    "    df_numerical.select(\n",
    "        \"DEP_DELAY_GROUP\",  # For join key\n",
    "        \"scaled_numerical_features\"\n",
    "    ),\n",
    "    on=\"DEP_DELAY_GROUP\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Feature processing complete!\")\n",
    "print(f\"Final dataset shape: {df_processed.count()} rows\")\n",
    "\n",
    "# Show vector dimensions\n",
    "categorical_size = df_processed.select(\"categorical_features_vector\").first()[0].size\n",
    "numerical_size = df_processed.select(\"scaled_numerical_features\").first()[0].size\n",
    "\n",
    "print(f\"Categorical features vector size: {categorical_size}\")\n",
    "print(f\"Numerical features vector size: {numerical_size}\")\n",
    "print(f\"Total features: {categorical_size + numerical_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f1b1da6-c98f-4ea6-afde-676ba35d2825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL FEATURE VECTOR ASSEMBLY\n",
    "print(\"=== Creating Final Feature Vector ===\")\n",
    "\n",
    "# Combine categorical and numerical feature vectors\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"categorical_features_vector\", \"scaled_numerical_features\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_final = final_assembler.transform(df_processed)\n",
    "\n",
    "# Select only what we need for modeling\n",
    "df_modeling = df_final.select(\"features\", \"DEP_DELAY_GROUP\")\n",
    "\n",
    "print(\"Final modeling dataset ready!\")\n",
    "print(f\"Shape: {df_modeling.count()} rows\")\n",
    "print(f\"Feature vector size: {df_modeling.select('features').first()[0].size}\")\n",
    "print(f\"Target classes: {df_modeling.select('DEP_DELAY_GROUP').distinct().count()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of final dataset:\")\n",
    "df_modeling.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f2522d-ccc4-4f2e-9151-518a838dd2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b9dfac-5cd4-4928-a7c6-c3751f32d408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.3: Handle ALL Categorical Features\n",
    "print(\"\\n=== HANDLING ALL CATEGORICAL FEATURES ===\")\n",
    "\n",
    "# Identify all string columns as categorical features\n",
    "categorical_features = [field.name for field in df_features.schema.fields\n",
    "                       if isinstance(field.dataType, StringType)]\n",
    "\n",
    "print(f\"Found {len(categorical_features)} categorical features to encode\")\n",
    "\n",
    "# Process all categorical features\n",
    "for col_name in categorical_features:\n",
    "    # Calculate cardinality\n",
    "    distinct_count = df_features.select(col_name).distinct().count()\n",
    "    print(f\"Encoding {col_name} with {distinct_count} distinct values\")\n",
    "    \n",
    "    # Get value frequencies\n",
    "    value_counts = df_features.groupBy(col_name).count().orderBy(\"count\", ascending=False)\n",
    "    \n",
    "    # Create indicators for top N values\n",
    "    top_n = 10 if distinct_count > 10 else distinct_count # Take up to 10 most common values\n",
    "    top_values = [row[col_name] for row in value_counts.limit(top_n).collect()]\n",
    "    \n",
    "    for value in top_values:\n",
    "        if value is not None:  # Skip None values\n",
    "            safe_value = str(value).replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\").lower()\n",
    "            new_col = f\"{col_name}_{safe_value}\"\n",
    "            df_features = df_features.withColumn(\n",
    "                new_col,\n",
    "                when(col(col_name) == value, 1).otherwise(0)\n",
    "            )\n",
    "    print(f\"Created {len(top_values)} indicator variables for top values of {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd42890b-0710-432c-85be-b8db3c5adee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.4: Scale ALL Numeric Features\n",
    "print(\"\\n=== SCALING ALL NUMERIC FEATURES ===\")\n",
    "\n",
    "# Identify all numeric columns\n",
    "numeric_features = [field.name for field in df_features.schema.fields \n",
    "                   if any(isinstance(field.dataType, t) for t in [DoubleType, IntegerType, FloatType])]\n",
    "\n",
    "# Exclude binary indicator columns and target variables\n",
    "binary_indicators = [col_name for col_name in df_features.columns \n",
    "                    if col_name.startswith(\"is_\") \n",
    "                    or col_name.endswith(\"_short\") \n",
    "                    or col_name.endswith(\"_medium\")\n",
    "                    or col_name.endswith(\"_long\")\n",
    "                    or col_name.endswith(\"_very_long\")\n",
    "                    or col_name.endswith(\"_morning\")\n",
    "                    or col_name.endswith(\"_afternoon\")\n",
    "                    or col_name.endswith(\"_evening\")\n",
    "                    or col_name.endswith(\"_night\")\n",
    "                    or any(col_name.startswith(f\"{c}_\") for c in categorical_features)]\n",
    "\n",
    "target_vars = [\"DEP_DELAY\", \"DEP_DEL15\"]\n",
    "\n",
    "# Filter numeric features to exclude binary indicators and target variables\n",
    "numeric_to_scale = [col_name for col_name in numeric_features \n",
    "                   if col_name not in binary_indicators\n",
    "                   and col_name not in target_vars\n",
    "                   and not col_name.endswith(\"_scaled\")]  # Avoid scaling already scaled features\n",
    "\n",
    "print(f\"Found {len(numeric_to_scale)} numeric features to scale\")\n",
    "\n",
    "# Apply z-score scaling to all numeric features\n",
    "for col_name in numeric_to_scale:\n",
    "    # Calculate statistics\n",
    "    stats = df_features.select(\n",
    "        F.mean(col(col_name)).alias(\"mean\"),\n",
    "        F.stddev(col(col_name)).alias(\"stddev\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    mean_val = stats[\"mean\"]\n",
    "    stddev_val = stats[\"stddev\"]\n",
    "    \n",
    "    if mean_val is not None and stddev_val is not None and stddev_val > 0:\n",
    "        # Create standardized version (z-score: (x - mean) / stddev)\n",
    "        df_features = df_features.withColumn(\n",
    "            col_name + \"_scaled\",\n",
    "            (col(col_name) - mean_val) / stddev_val\n",
    "        )\n",
    "        print(f\"Applied z-score scaling to {col_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping scaling for {col_name} (constant or missing data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e385b209-1848-483b-82ba-539c4754cbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4682015e-6b8d-493b-9f83-0af65a59c672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Drop Diverted Flights:\n",
    "\n",
    "print(\"=== Dropping Diverted Flights ===\")\n",
    "before_count = otpw_3m_imputed.count()\n",
    "otpw_3m_imputed = otpw_3m_imputed.filter((col(\"DIVERTED\") != 1) | col(\"DIVERTED\").isNull())\n",
    "after_count = otpw_3m_imputed.count()\n",
    "diverted_dropped = before_count - after_count\n",
    "print(f\"Dropped {diverted_dropped:,} diverted flights ({diverted_dropped/before_count*100:.2f}%)\")\n",
    "print(f\"Remaining flights: {after_count:,}\")\n",
    "\n",
    "%md\n",
    "#### Verify No Null Categorical Features After Cleaning:\n",
    "\n",
    "print(\"\\n=== Verifying Categorical Features Have No Nulls ===\")\n",
    "categorical_cols = [\n",
    "    \"ORIGIN\", \"DEST\", \"OP_UNIQUE_CARRIER\", \"TAIL_NUM\", \n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\", \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"ORIGIN_CITY_MARKET_ID\", \"DEST_CITY_MARKET_ID\",\n",
    "    \"ORIGIN_CITY_NAME\", \"DEST_CITY_NAME\", \"ORIGIN_STATE_ABR\", \"DEST_STATE_ABR\",\n",
    "    \"HourlyPresentWeatherType\", \"CANCELLATION_CODE\", \"DAY_OF_WEEK\"\n",
    "]\n",
    "\n",
    "categorical_null_check = []\n",
    "for col_name in categorical_cols:\n",
    "    if col_name in otpw_3m_imputed.columns:\n",
    "        null_count = otpw_3m_imputed.filter(col(col_name).isNull()).count()\n",
    "        categorical_null_check.append((col_name, null_count))\n",
    "        \n",
    "cat_null_df = pd.DataFrame(categorical_null_check, columns=[\"Column\", \"Null Count\"])\n",
    "print(\"\\nCategorical Columns Null Count:\")\n",
    "display(cat_null_df)\n",
    "\n",
    "if cat_null_df[\"Null Count\"].sum() == 0:\n",
    "    print(\" All categorical features have been successfully cleaned (no nulls)\")\n",
    "else:\n",
    "    print(f\"  Warning: {cat_null_df[cat_null_df['Null Count'] > 0].shape[0]} categorical columns still have nulls\")\n",
    "\n",
    "%md\n",
    "#### Remove All Leakage Features:\n",
    "\n",
    "print(\"\\n=== Removing Data Leakage Features ===\")\n",
    "\n",
    "# Define all leakage features (features known only after flight completion)\n",
    "leakage_features = [\n",
    "    # Actual times (only known after flight)\n",
    "    \"DEP_TIME\", \"ARR_TIME\", \"WHEELS_OFF\", \"WHEELS_ON\",\n",
    "    \n",
    "    # Actual delays (target-related)\n",
    "    \"DEP_DELAY\", \"DEP_DELAY_NEW\", \"DEP_DELAY_GROUP\",\n",
    "    \"ARR_DELAY\", \"ARR_DELAY_NEW\", \"ARR_DELAY_GROUP\",\n",
    "    \n",
    "    # Taxi times (only known after flight)\n",
    "    \"TAXI_OUT\", \"TAXI_IN\",\n",
    "    \n",
    "    # Flight durations (only known after completion)\n",
    "    \"ACTUAL_ELAPSED_TIME\", \"AIR_TIME\",\n",
    "    \n",
    "    # Delay breakdowns (only known after delay occurs)\n",
    "    \"CARRIER_DELAY\", \"WEATHER_DELAY\", \"NAS_DELAY\", \n",
    "    \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\",\n",
    "    \n",
    "    # Other post-flight info\n",
    "    \"FIRST_DEP_TIME\", \"TOTAL_ADD_GTIME\", \"LONGEST_ADD_GTIME\",\n",
    "    \n",
    "    # Keep ARR_DEL15 if predicting departure delay, remove if predicting arrival\n",
    "    \"ARR_DEL15\"  # Remove this since we're predicting DEP_DEL15\n",
    "]\n",
    "\n",
    "# Count and remove leakage features\n",
    "existing_leakage = [col for col in leakage_features if col in otpw_3m_imputed.columns]\n",
    "print(f\"Found {len(existing_leakage)} leakage features to remove:\")\n",
    "for feat in existing_leakage:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "otpw_3m_clean_no_leakage = otpw_3m_imputed.drop(*existing_leakage)\n",
    "\n",
    "print(f\"\\nColumns before: {len(otpw_3m_imputed.columns)}\")\n",
    "print(f\"Columns after: {len(otpw_3m_clean_no_leakage.columns)}\")\n",
    "print(f\"Removed: {len(otpw_3m_imputed.columns) - len(otpw_3m_clean_no_leakage.columns)} columns\")\n",
    "\n",
    "%md\n",
    "#### Feature Selection: Correlation Analysis with Target:\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n=== Feature Correlation Analysis ===\")\n",
    "\n",
    "# Select numerical features for correlation analysis\n",
    "numerical_features = [\n",
    "    \"QUARTER\", \"MONTH\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\", \"DISTANCE_GROUP\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlyWindSpeed\", \"HourlyVisibility\",\n",
    "    \"HourlyPrecipitation\", \"HourlySeaLevelPressure\"\n",
    "]\n",
    "\n",
    "# Filter to existing numerical columns\n",
    "existing_numerical = [col for col in numerical_features if col in otpw_3m_clean_no_leakage.columns]\n",
    "\n",
    "# Create temporary dataset with no nulls for correlation\n",
    "temp_df = otpw_3m_clean_no_leakage.select([\"DEP_DEL15\"] + existing_numerical).na.drop()\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = []\n",
    "for feature in existing_numerical:\n",
    "    corr = temp_df.stat.corr(\"DEP_DEL15\", feature)\n",
    "    correlations.append((feature, abs(corr), corr))\n",
    "\n",
    "# Create correlation dataframe\n",
    "corr_df = pd.DataFrame(correlations, columns=[\"Feature\", \"Abs_Correlation\", \"Correlation\"])\n",
    "corr_df = corr_df.sort_values(\"Abs_Correlation\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Features by Absolute Correlation with DEP_DEL15:\")\n",
    "display(corr_df.head(15))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(corr_df.head(15)[\"Feature\"], corr_df.head(15)[\"Correlation\"])\n",
    "plt.xlabel(\"Correlation with DEP_DEL15\")\n",
    "plt.title(\"Top 15 Features Correlated with Departure Delay\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "%md\n",
    "#### Prepare Features for Modeling:\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "print(\"\\n=== Preparing Features for Modeling ===\")\n",
    "\n",
    "# Define feature categories\n",
    "categorical_features = [\n",
    "    \"OP_UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\", \n",
    "    \"ORIGIN_STATE_ABR\", \"DEST_STATE_ABR\",\n",
    "    \"DAY_OF_WEEK\", \"MONTH\"\n",
    "]\n",
    "\n",
    "numerical_features_for_model = [\n",
    "    \"QUARTER\", \"DAY_OF_MONTH\",\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlyWindSpeed\", \"HourlyVisibility\",\n",
    "    \"HourlyPrecipitation\", \"HourlySeaLevelPressure\"\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "categorical_features = [col for col in categorical_features if col in otpw_3m_clean_no_leakage.columns]\n",
    "numerical_features_for_model = [col for col in numerical_features_for_model if col in otpw_3m_clean_no_leakage.columns]\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features_for_model)}\")\n",
    "\n",
    "# Save clean dataset before splitting\n",
    "otpw_3m_clean_no_leakage.write.mode(\"overwrite\").parquet(f\"{folder_path}/otpw_3m_clean_no_leakage.parquet\")\n",
    "print(f\"\\n Saved clean dataset (no leakage): {folder_path}/otpw_3m_clean_no_leakage.parquet\")\n",
    "\n",
    "%md\n",
    "#### Train/Validation/Test Split (60/20/20):\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "print(\"\\n=== Creating Train/Validation/Test Split ===\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "train_data, val_data, test_data = otpw_3m_clean_no_leakage.randomSplit([0.6, 0.2, 0.2], seed=seed)\n",
    "\n",
    "# Cache for performance\n",
    "train_data.cache()\n",
    "val_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"Total rows: {otpw_3m_clean_no_leakage.count():,}\")\n",
    "print(f\"Training set: {train_data.count():,} ({train_data.count()/otpw_3m_clean_no_leakage.count()*100:.1f}%)\")\n",
    "print(f\"Validation set: {val_data.count():,} ({val_data.count()/otpw_3m_clean_no_leakage.count()*100:.1f}%)\")\n",
    "print(f\"Test set: {test_data.count():,} ({test_data.count()/otpw_3m_clean_no_leakage.count()*100:.1f}%)\")\n",
    "\n",
    "# Check target distribution in each set\n",
    "print(\"\\n=== Target Distribution ===\")\n",
    "for name, dataset in [(\"Train\", train_data), (\"Validation\", val_data), (\"Test\", test_data)]:\n",
    "    delayed = dataset.filter(col(\"DEP_DEL15\") == 1).count()\n",
    "    total = dataset.count()\n",
    "    print(f\"{name}: {delayed:,} delayed ({delayed/total*100:.2f}%) | {total-delayed:,} on-time ({(total-delayed)/total*100:.2f}%)\")\n",
    "\n",
    "# Save splits\n",
    "train_data.write.mode(\"overwrite\").parquet(f\"{folder_path}/train_data.parquet\")\n",
    "val_data.write.mode(\"overwrite\").parquet(f\"{folder_path}/val_data.parquet\")\n",
    "test_data.write.mode(\"overwrite\").parquet(f\"{folder_path}/test_data.parquet\")\n",
    "print(f\"\\n Saved data splits to {folder_path}\")\n",
    "\n",
    "%md\n",
    "#### Apply One-Hot Encoding and Standard Scaler (Training Data Only):\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "print(\"\\n=== Building Feature Engineering Pipeline ===\")\n",
    "\n",
    "# Stage 1: String Indexing for categorical features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "# Stage 2: One-Hot Encoding\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "# Stage 3: Assemble all features\n",
    "encoded_cols = [f\"{col}_encoded\" for col in categorical_features]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numerical_features_for_model + encoded_cols,\n",
    "    outputCol=\"features_unscaled\"\n",
    ")\n",
    "\n",
    "# Stage 4: Standard Scaler (fitted on training data only)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "print(\"Pipeline stages:\")\n",
    "print(f\"  - {len(indexers)} StringIndexers\")\n",
    "print(f\"  - {len(encoders)} OneHotEncoders\")\n",
    "print(f\"  - 1 VectorAssembler\")\n",
    "print(f\"  - 1 StandardScaler\")\n",
    "\n",
    "# Fit pipeline on TRAINING DATA ONLY\n",
    "print(\"\\n  IMPORTANT: Fitting pipeline on training data only to prevent data leakage\")\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# Transform all datasets using the fitted pipeline\n",
    "train_transformed = pipeline_model.transform(train_data)\n",
    "val_transformed = pipeline_model.transform(val_data)\n",
    "test_transformed = pipeline_model.transform(test_data)\n",
    "\n",
    "print(\"\\n Pipeline fitted on training data\")\n",
    "print(\" Transformations applied to all datasets\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample of transformed features:\")\n",
    "train_transformed.select(\"DEP_DEL15\", \"features\").show(5, truncate=False)\n",
    "\n",
    "# Save transformed datasets and pipeline\n",
    "train_transformed.write.mode(\"overwrite\").parquet(f\"{folder_path}/train_transformed.parquet\")\n",
    "val_transformed.write.mode(\"overwrite\").parquet(f\"{folder_path}/val_transformed.parquet\")\n",
    "test_transformed.write.mode(\"overwrite\").parquet(f\"{folder_path}/test_transformed.parquet\")\n",
    "pipeline_model.write().overwrite().save(f\"{folder_path}/feature_pipeline_model\")\n",
    "\n",
    "print(f\"\\n Saved transformed datasets and pipeline to {folder_path}\")\n",
    "print(\"\\nReady for model training! \")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Team_4_4_Baseline_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}