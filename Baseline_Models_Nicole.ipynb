{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e05b5bb-0ee3-4f28-bf8e-fa81c3b63445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53be288-2f6c-47fc-855e-789244579c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, log, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import skewness\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09bbc83-25bf-4321-ba85-3b9d68524f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa40088-0873-448f-a5c8-f726a366f5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7258941"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline = spark.read.parquet(\"dbfs:/student-groups/Group_4_4/joined_1Y_final_feature_clean_with_removed_features\")\n",
    "\n",
    "df_baseline = df_baseline.cache()\n",
    "df_baseline.count()  # force materialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f379c64b-9dd1-4e82-a3f2-41c1438a602c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 160\n"
     ]
    }
   ],
   "source": [
    "print(f\"Columns: {len(df_baseline.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2fc49a-a8a0-4510-85db-663397c6467e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['OP_UNIQUE_CARRIER',\n",
       " 'DEST',\n",
       " 'ORIGIN',\n",
       " 'FL_DATE',\n",
       " 'YEAR',\n",
       " 'QUARTER',\n",
       " 'DAY_OF_MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'OP_CARRIER_FL_NUM',\n",
       " 'CRS_DEP_TIME',\n",
       " 'CRS_ARR_TIME',\n",
       " 'ORIGIN_AIRPORT_ID',\n",
       " 'ORIGIN_STATE_ABR',\n",
       " 'DEST_AIRPORT_ID',\n",
       " 'DEST_STATE_ABR',\n",
       " 'departure_hour',\n",
       " 'prediction_utc',\n",
       " 'origin_obs_utc',\n",
       " 'asof_minutes',\n",
       " 'HourlyDewPointTemperature',\n",
       " 'HourlyPrecipitation',\n",
       " 'HourlyWindSpeed',\n",
       " 'HourlyWindDirection',\n",
       " 'HourlyWindGustSpeed',\n",
       " 'HourlyVisibility',\n",
       " 'HourlyRelativeHumidity',\n",
       " 'HourlyAltimeterSetting',\n",
       " 'origin_airport_lat',\n",
       " 'origin_airport_lon',\n",
       " 'dest_airport_lat',\n",
       " 'dest_airport_lon',\n",
       " 'origin_station_dis',\n",
       " 'dest_station_dis',\n",
       " 'origin_type',\n",
       " 'dest_type',\n",
       " 'DEP_DEL15',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_CODE',\n",
       " 'DIVERTED',\n",
       " 'departure_month',\n",
       " 'departure_dayofweek',\n",
       " 'is_weekend',\n",
       " 'season',\n",
       " 'is_peak_hour',\n",
       " 'is_peak_month',\n",
       " 'time_of_day_early_morning',\n",
       " 'time_of_day_morning',\n",
       " 'time_of_day_afternoon',\n",
       " 'time_of_day_evening',\n",
       " 'time_of_day_night',\n",
       " 'rolling_origin_num_delays_24h',\n",
       " 'dep_delay15_24h_rolling_avg_by_origin',\n",
       " 'dep_delay15_24h_rolling_avg_by_origin_carrier',\n",
       " 'dep_delay15_24h_rolling_avg_by_origin_dayofweek',\n",
       " 'is_holiday_window',\n",
       " 'weather_severity_index',\n",
       " 'distance_medium',\n",
       " 'distance_long',\n",
       " 'distance_very_long',\n",
       " 'weather_condition_category',\n",
       " 'airport_traffic_density',\n",
       " 'carrier_flight_count',\n",
       " 'weather_obs_lag_hours',\n",
       " 'log_distance',\n",
       " 'is_rainy',\n",
       " 'prev_flight_dep_del15',\n",
       " 'prev_flight_crs_elapsed_time',\n",
       " 'hours_since_prev_flight',\n",
       " 'is_first_flight_of_aircraft',\n",
       " 'turnaround_category',\n",
       " 'is_business_hours',\n",
       " 'is_holiday_month',\n",
       " 'num_airport_wide_delays',\n",
       " 'temp_dest_count',\n",
       " 'oncoming_flights',\n",
       " 'prior_day_delay_rate',\n",
       " 'time_based_congestion_ratio',\n",
       " 'sky_condition_parsed',\n",
       " 'rapid_weather_change',\n",
       " 'temp_anomaly',\n",
       " 'dep_time_sin',\n",
       " 'dep_time_cos',\n",
       " 'arr_time_sin',\n",
       " 'arr_time_cos',\n",
       " 'day_of_week_sin',\n",
       " 'day_of_week_cos',\n",
       " 'month_sin',\n",
       " 'month_cos',\n",
       " 'wind_direction_sin',\n",
       " 'wind_direction_cos',\n",
       " 'extreme_precipitation',\n",
       " 'extreme_wind',\n",
       " 'extreme_temperature',\n",
       " 'low_visibility',\n",
       " 'extreme_weather_score',\n",
       " 'origin_degree_centrality',\n",
       " 'dest_degree_centrality',\n",
       " 'carrier_delay_stddev',\n",
       " 'pagerank',\n",
       " 'days_since_epoch',\n",
       " 'origin_1yr_delay_rate',\n",
       " 'dest_1yr_delay_rate',\n",
       " 'rolling_30day_volume',\n",
       " 'route_1yr_volume',\n",
       " 'ARR_DEL15_removed',\n",
       " 'DEP_TIME_removed',\n",
       " 'ARR_TIME_removed',\n",
       " 'WHEELS_OFF_removed',\n",
       " 'WHEELS_ON_removed',\n",
       " 'DEP_DELAY_removed',\n",
       " 'ARR_DELAY_removed',\n",
       " 'TAXI_OUT_removed',\n",
       " 'TAXI_IN_removed',\n",
       " 'ACTUAL_ELAPSED_TIME_removed',\n",
       " 'AIR_TIME_removed',\n",
       " 'CARRIER_DELAY_removed',\n",
       " 'WEATHER_DELAY_removed',\n",
       " 'NAS_DELAY_removed',\n",
       " 'SECURITY_DELAY_removed',\n",
       " 'LATE_AIRCRAFT_DELAY_removed',\n",
       " 'origin_station_lat_removed',\n",
       " 'origin_station_lon_removed',\n",
       " 'origin_airport_lat_removed',\n",
       " 'origin_airport_lon_removed',\n",
       " 'dest_station_lat_removed',\n",
       " 'dest_station_lon_removed',\n",
       " 'dest_airport_lat_removed',\n",
       " 'dest_airport_lon_removed',\n",
       " 'origin_station_dis_removed',\n",
       " 'dest_station_dis_removed',\n",
       " 'ORIGIN_CITY_NAME_removed',\n",
       " 'DEST_CITY_NAME_removed',\n",
       " 'origin_station_id_removed',\n",
       " 'dest_station_id_removed',\n",
       " 'DISTANCE_removed',\n",
       " 'DISTANCE_GROUP_removed',\n",
       " 'distance_short_removed',\n",
       " 'HourlyDryBulbTemperature_removed',\n",
       " 'HourlyWetBulbTemperature_removed',\n",
       " 'HourlyStationPressure_removed',\n",
       " 'HourlySeaLevelPressure_removed',\n",
       " 'MONTH_removed',\n",
       " 'CRS_DEP_HOUR_removed',\n",
       " 'rolling_origin_num_flights_24h_removed',\n",
       " 'rolling_origin_delay_ratio_24h_removed',\n",
       " 'rolling_origin_stddev_dep_delay_24h_removed',\n",
       " 'total_flights_per_origin_day_removed',\n",
       " 'prior_flights_today_removed',\n",
       " 'prior_delays_today_removed',\n",
       " 'same_day_prior_delay_percentage_removed',\n",
       " 'prev_flight_distance_removed',\n",
       " 'CRS_ELAPSED_TIME_removed',\n",
       " 'temp_humidity_interaction_removed',\n",
       " 'precip_anomaly_removed',\n",
       " 'extreme_precipitation_removed',\n",
       " 'extreme_weather_score_removed',\n",
       " 'num_airport_wide_cancellations_removed',\n",
       " 'flight_id_removed',\n",
       " 'HourlySkyConditions_removed',\n",
       " 'HourlyPresentWeatherType_removed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc262ea-f7b9-4fa8-9f8c-91778d292bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Festure Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e074b6-1782-4ef4-a92d-943d293a0ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_id_removed                         7466932\nHourlySkyConditions_removed               127407\nHourlyPresentWeatherType_removed          751\norigin_station_id_removed                 357\ndest_station_id_removed                   357\nDEST                                      353\nORIGIN                                    353\nDEST_CITY_NAME_removed                    351\nORIGIN_CITY_NAME_removed                  349\nORIGIN_STATE_ABR                          54\nDEST_STATE_ABR                            54\nOP_UNIQUE_CARRIER                         16\nsky_condition_parsed                      6\nseason                                    4\nturnaround_category                       4\norigin_type                               3\ndest_type                                 3\nweather_condition_category                3\nCANCELLATION_CODE                         1\n"
     ]
    }
   ],
   "source": [
    "label_col = \"DEP_DEL15\"\n",
    "\n",
    "# Identify string columns (excluding the label)\n",
    "string_cols = [c for c, t in df_baseline.dtypes if t == \"string\" and c != label_col]\n",
    "\n",
    "# Compute cardinality for each categorical column\n",
    "cardinality_exprs = [\n",
    "    approx_count_distinct(c).alias(c)\n",
    "    for c in string_cols\n",
    "]\n",
    "cardinality_row = df_baseline.select(cardinality_exprs).first()\n",
    "cardinality = {c: cardinality_row[c] for c in string_cols}\n",
    "\n",
    "# Sort by cardinality (high → low)\n",
    "sorted_cardinality = sorted(cardinality.items(), key=lambda x: x[1], reverse=True)\n",
    "for col, cnt in sorted_cardinality:\n",
    "    print(f\"{col:40}  {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9129e1f0-491c-496d-a0b0-e39c23eb735e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Feature removal ---\n",
    "label_col = \"DEP_DEL15\"\n",
    "\n",
    "# Drop leakage-related columns (contain post-departure info)\n",
    "leakage_cols = [\n",
    "    \"CANCELLED\",\n",
    "    \"CANCELLATION_CODE\",\n",
    "    \"DIVERTED\",\n",
    "\n",
    "    \"ARR_DEL15_removed\",\n",
    "    \"DEP_TIME_removed\",\n",
    "    \"ARR_TIME_removed\",\n",
    "    \"WHEELS_OFF_removed\",\n",
    "    \"WHEELS_ON_removed\",\n",
    "    \"DEP_DELAY_removed\",\n",
    "    \"ARR_DELAY_removed\",\n",
    "    \"TAXI_OUT_removed\",\n",
    "    \"TAXI_IN_removed\",\n",
    "    \"ACTUAL_ELAPSED_TIME_removed\",\n",
    "    \"AIR_TIME_removed\",\n",
    "    \"CARRIER_DELAY_removed\",\n",
    "    \"WEATHER_DELAY_removed\",\n",
    "    \"NAS_DELAY_removed\",\n",
    "    \"SECURITY_DELAY_removed\",\n",
    "    \"LATE_AIRCRAFT_DELAY_removed\",\n",
    "    \"num_airport_wide_cancellations_removed\",\n",
    "    \"CRS_ARR_TIME_removed\",\n",
    "    \"CRS_ELAPSED_TIME_removed\",\n",
    "    \"DEP_DELAY_removed\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "]\n",
    "\n",
    "high_card_cols = [\n",
    "    \"flight_id_removed\",\n",
    "    \"HourlySkyConditions_removed\",\n",
    "    \"HourlyPresentWeatherType_removed\",\n",
    "]\n",
    "\n",
    "# 1. Drop leakage + bad ID-like high-card columns\n",
    "df_baseline = (\n",
    "    df_baseline\n",
    "      .drop(*leakage_cols)\n",
    "      .drop(*high_card_cols)\n",
    "      .drop(\"FL_DATE\", \"prediction_utc\", \"origin_obs_utc\")\n",
    ")\n",
    "\n",
    "# 2. NOW recompute string_cols based on the cleaned df_baseline\n",
    "string_cols = [c for c, t in df_baseline.dtypes if t == \"string\" and c != label_col]\n",
    "\n",
    "# 3. Compute cardinality on the cleaned set of string columns\n",
    "cardinality_exprs = [\n",
    "    approx_count_distinct(c).alias(c)\n",
    "    for c in string_cols\n",
    "]\n",
    "cardinality_row = df_baseline.select(cardinality_exprs).first()\n",
    "cardinality = {c: cardinality_row[c] for c in string_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a667f59-9c05-4508-8fc9-1aba027ddb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspicious columns (verify not leakage): []\n"
     ]
    }
   ],
   "source": [
    "# Check for post-departure features\n",
    "\n",
    "post_departure_keywords = [\"ARR_\", \"WHEELS_\", \"TAXI_\", \"ACTUAL_ELAPSED\", \"AIR_TIME\"]\n",
    "suspicious_cols_baseline = [\n",
    "    c for c in df_baseline.columns\n",
    "    if any(kw in c for kw in post_departure_keywords)\n",
    "]\n",
    "print(f\"Suspicious columns (verify not leakage): {suspicious_cols_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460fb180-b96a-4961-a080-d9eda5fe29c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e447ba-9791-490a-a0a3-99b0b8ca6a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9869dd5f-128b-4dd7-bb74-061d0fefba19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 5486\nTest rows : 1819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_baseline = (\n",
    "    df_baseline\n",
    "    .filter(col(\"QUARTER\") < 4)\n",
    "    .cache()      \n",
    ")\n",
    "test_df_baseline = (\n",
    "    df_baseline\n",
    "    .filter(col(\"QUARTER\") == 4)\n",
    "    .cache()      \n",
    ")\n",
    "\n",
    "print(\"Train rows:\", train_df_baseline.count())\n",
    "print(\"Test rows :\", test_df_baseline.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a567edc1-3e76-4081-9840-3af72efadecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify No Temporal Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4eb939f-009c-457a-9fae-6a69715ae791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col, min as Fmin, max as Fmax\n",
    "\n",
    "def show_time_range(df, name):\n",
    "    print(name)\n",
    "    df.select(\n",
    "        Fmin(\"YEAR\").alias(\"min_year\"),\n",
    "        Fmax(\"YEAR\").alias(\"max_year\"),\n",
    "        Fmin(\"QUARTER\").alias(\"min_quarter\"),\n",
    "        Fmax(\"QUARTER\").alias(\"max_quarter\"),\n",
    "    ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6210e98b-36ba-42a2-a4a3-5c8f5fcb4b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8040210782231497>, line 12\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(name)\n",
       "\u001B[1;32m      5\u001B[0m     df\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m      6\u001B[0m         Fmin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_year\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      7\u001B[0m         Fmax(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_year\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      8\u001B[0m         Fmin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQUARTER\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_quarter\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      9\u001B[0m         Fmax(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQUARTER\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_quarter\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     10\u001B[0m     )\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[0;32m---> 12\u001B[0m show_time_range(train_df_baseline, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain time range\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     13\u001B[0m show_time_range(test_df_baseline,  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest time range\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'train_df_baseline' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'train_df_baseline' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'train_df_baseline' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-8040210782231497>, line 12\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(name)\n\u001B[1;32m      5\u001B[0m     df\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m      6\u001B[0m         Fmin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_year\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      7\u001B[0m         Fmax(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_year\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      8\u001B[0m         Fmin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQUARTER\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_quarter\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      9\u001B[0m         Fmax(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQUARTER\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_quarter\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     10\u001B[0m     )\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 12\u001B[0m show_time_range(train_df_baseline, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain time range\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m show_time_range(test_df_baseline,  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest time range\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'train_df_baseline' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%skip\n",
    "show_time_range(train_df_baseline, \"Train time range\")\n",
    "show_time_range(test_df_baseline,  \"Test time range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f511e77f-9bfc-4fed-bfb5-579119b012cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enconding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "404efd05-619f-4711-a342-d7beaac61fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffde1b3-2bde-41b0-bce9-e381d8e902f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nFEATURE CATEGORIZATION\n======================================================================\nTarget encoding:    8 columns\nOne-hot encoding:   7 columns\nBinary string:      0 columns\n\nTarget encoding columns:\n  DEST                                     (card=353) [REQUIRED]\n  DEST_CITY_NAME_removed                   (card=351)\n  DEST_STATE_ABR                           (card= 54) [REQUIRED]\n  ORIGIN                                   (card=353) [REQUIRED]\n  ORIGIN_CITY_NAME_removed                 (card=349)\n  ORIGIN_STATE_ABR                         (card= 54) [REQUIRED]\n  dest_station_id_removed                  (card=357)\n  origin_station_id_removed                (card=357)\n\nOne-hot encoding columns:\n  OP_UNIQUE_CARRIER                        (card= 16) [REQUIRED]\n  dest_type                                (card=  3) [REQUIRED]\n  origin_type                              (card=  3) [REQUIRED]\n  season                                   (card=  4) [REQUIRED]\n  sky_condition_parsed                     (card=  6) [REQUIRED]\n  turnaround_category                      (card=  4) [REQUIRED]\n  weather_condition_category               (card=  3) [REQUIRED]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Encoding assignment rules ---\n",
    "# High-cardinality categorical features → target encoding\n",
    "# Low-cardinality categorical features → one-hot encoding\n",
    "# Binary categorical features → treat as numeric (0/1)\n",
    "\n",
    "# 4. Decide which remaining string columns go to target vs one-hot\n",
    "\n",
    "# Features that MUST use one-hot encoding (override cardinality)\n",
    "required_onehot = [\n",
    "    \"OP_UNIQUE_CARRIER\",\n",
    "    \"sky_condition_parsed\",\n",
    "    \"season\",\n",
    "    \"turnaround_category\",\n",
    "    \"origin_type\",\n",
    "    \"dest_type\",\n",
    "    \"weather_condition_category\",\n",
    "]\n",
    "\n",
    "# Features that MUST use target encoding (override cardinality)\n",
    "required_target = [\n",
    "    \"DEST\",\n",
    "    \"ORIGIN\",\n",
    "    \"DEST_STATE_ABR\",\n",
    "    \"ORIGIN_STATE_ABR\"\n",
    "]\n",
    "high_card_threshold = 30   # >30 categories → target encoding\n",
    "low_card_min = 3           # 3–30 categories → one-hot encoding\n",
    "\n",
    "target_cols = []\n",
    "onehot_cols = []\n",
    "binary_string_cols = []\n",
    "\n",
    "for col in string_cols:\n",
    "    card = cardinality.get(col, 0)\n",
    "    \n",
    "    # Priority 1: Required lists (highest priority)\n",
    "    if col in required_target:\n",
    "        target_cols.append(col)\n",
    "    elif col in required_onehot:\n",
    "        onehot_cols.append(col)\n",
    "    \n",
    "    # Priority 2: Binary columns\n",
    "    elif card == 2:\n",
    "        binary_string_cols.append(col)\n",
    "    \n",
    "    # Priority 3: Cardinality-based rules\n",
    "    elif card > high_card_threshold:\n",
    "        target_cols.append(col)\n",
    "    elif low_card_min <= card <= high_card_threshold:\n",
    "        onehot_cols.append(col)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE CATEGORIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target encoding:  {len(target_cols):3d} columns\")\n",
    "print(f\"One-hot encoding: {len(onehot_cols):3d} columns\")\n",
    "print(f\"Binary string:    {len(binary_string_cols):3d} columns\")\n",
    "\n",
    "print(\"\\nTarget encoding columns:\")\n",
    "for col in sorted(target_cols):\n",
    "    card = cardinality.get(col, 0)\n",
    "    required = \" [REQUIRED]\" if col in required_target else \"\"\n",
    "    print(f\"  {col:40s} (card={card:3d}){required}\")\n",
    "\n",
    "print(\"\\nOne-hot encoding columns:\")\n",
    "for col in sorted(onehot_cols):\n",
    "    card = cardinality.get(col, 0)\n",
    "    required = \" [REQUIRED]\" if col in required_onehot else \"\"\n",
    "    print(f\"  {col:40s} (card={card:3d}){required}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8aa0e0-4ad7-4d82-8e14-36c829aab1e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nVALIDATION\n======================================================================\nAll 7 required one-hot features present\nAll 4 required target features present\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check required one-hot\n",
    "missing_onehot = set(required_onehot) - set(onehot_cols)\n",
    "if missing_onehot:\n",
    "    print(f\"Missing required one-hot: {missing_onehot}\")\n",
    "else:\n",
    "    print(f\"All {len(required_onehot)} required one-hot features present\")\n",
    "\n",
    "# Check required target\n",
    "missing_target = set(required_target) - set(target_cols)\n",
    "if missing_target:\n",
    "    print(f\"Missing required target: {missing_target}\")\n",
    "else:\n",
    "    print(f\"All {len(required_target)} required target features present\")\n",
    "\n",
    "# Check if required columns exist in data\n",
    "all_required = set(required_onehot) | set(required_target)\n",
    "not_in_data = all_required - set(string_cols)\n",
    "if not_in_data:\n",
    "    print(f\"\\nWARNING: {len(not_in_data)} required columns NOT in dataset:\")\n",
    "    for col in sorted(not_in_data):\n",
    "        print(f\"    - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739e42fc-9b02-44db-a394-06f8f41fd139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nBASELINE FEATURE VALIDATION\n======================================================================\n\n=== Check 1: Required One-Hot Features ===\n Missing expected one-hot features: {'ORIGIN_STATE_ABR', 'DEST_STATE_ABR'}\n   → Check cardinality - they might be in target_cols instead\n\nActual one-hot features (7):\n  - OP_UNIQUE_CARRIER                    (cardinality: 16)\n  - dest_type                            (cardinality: 3)\n  - origin_type                          (cardinality: 3)\n  - season                               (cardinality: 4)\n  - sky_condition_parsed                 (cardinality: 6)\n  - turnaround_category                  (cardinality: 4)\n  - weather_condition_category           (cardinality: 3)\n\nAdditional one-hot features found: 6\n  + dest_type                            (cardinality: 3)\n  + origin_type                          (cardinality: 3)\n  + season                               (cardinality: 4)\n  + sky_condition_parsed                 (cardinality: 6)\n  + turnaround_category                  (cardinality: 4)\n  + weather_condition_category           (cardinality: 3)\n\n=== Check 2: Required Target-Encoded Features ===\n✓ All core target-encoded features present\nMissing optional features: {'TAIL_NUM'}\n   → These are optional but could improve performance\n\nActual target-encoded features (8):\n  - DEST                                 (cardinality: 353)\n  - DEST_CITY_NAME_removed               (cardinality: 351)\n  - DEST_STATE_ABR                       (cardinality: 54)\n  - ORIGIN                               (cardinality: 353)\n  - ORIGIN_CITY_NAME_removed             (cardinality: 349)\n  - ORIGIN_STATE_ABR                     (cardinality: 54)\n  - dest_station_id_removed              (cardinality: 357)\n  - origin_station_id_removed            (cardinality: 357)\n\n=== Check 3: Binary String Features ===\nBinary string features found (0):\n  (none)\n  → This is OK - binary features can be treated as numeric later\n\n=== Check 4: Cardinality Distribution ===\n\nEncoding rules:\n  Binary (=2):      0 features\n  One-hot (3-50):   7 features\n  Target (>50):     8 features\n  Total string:     15 features\n\n✓ All string features assigned to an encoding strategy\n\n=== Check 5: Baseline vs Engineered Feature Comparison ===\nCommon required features:\n  ✓ DEST                            [target-encoded]\n  ✓ ORIGIN                          [target-encoded]\n  ✓ OP_UNIQUE_CARRIER               [one-hot]\n\n======================================================================\nBASELINE FEATURE VALIDATION SUMMARY\n======================================================================\nOne-Hot Features:     7 total\nTarget Features:      8 total, 0 missing\nBinary Features:      0 total\nUnassigned Features:  0\n\nOverall Status: PASS ✓\n======================================================================\n\n✓ Baseline feature setup looks good!\n  Ready to proceed with model training\n"
     ]
    }
   ],
   "source": [
    "# BASELINE: Feature Validation Checks\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE FEATURE VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Check 1: Verify Required One-Hot Encoded Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== Check 1: Required One-Hot Features ===\")\n",
    "\n",
    "# Expected one-hot features for baseline model\n",
    "required_onehot_baseline = [\n",
    "    \"OP_UNIQUE_CARRIER\",      \n",
    "    \"ORIGIN_STATE_ABR\",       \n",
    "    \"DEST_STATE_ABR\",         \n",
    "]\n",
    "\n",
    "# Note: These may vary based on your actual cardinality thresholds\n",
    "# Adjust based on what features actually fall in the 3-50 range\n",
    "\n",
    "missing_onehot = set(required_onehot_baseline) - set(onehot_cols)\n",
    "extra_onehot = set(onehot_cols) - set(required_onehot_baseline)\n",
    "\n",
    "if missing_onehot:\n",
    "    print(f\" Missing expected one-hot features: {missing_onehot}\")\n",
    "    print(\"   → Check cardinality - they might be in target_cols instead\")\n",
    "else:\n",
    "    print(f\"✓ All expected one-hot features present\")\n",
    "\n",
    "print(f\"\\nActual one-hot features ({len(onehot_cols)}):\")\n",
    "for col in sorted(onehot_cols):\n",
    "    card = cardinality.get(col, \"?\")\n",
    "    print(f\"  - {col:35s}  (cardinality: {card})\")\n",
    "\n",
    "if extra_onehot:\n",
    "    print(f\"\\nAdditional one-hot features found: {len(extra_onehot)}\")\n",
    "    for col in sorted(extra_onehot):\n",
    "        card = cardinality.get(col, \"?\")\n",
    "        print(f\"  + {col:35s}  (cardinality: {card})\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Check 2: Verify Required Target Encoded Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== Check 2: Required Target-Encoded Features ===\")\n",
    "\n",
    "# Expected high-cardinality features for target encoding\n",
    "required_target_baseline = [\n",
    "    \"DEST\",                # Destination airport (high cardinality)\n",
    "    \"ORIGIN\",              # Origin airport (high cardinality)\n",
    "]\n",
    "\n",
    "# Optional: These depend on your data\n",
    "optional_target_baseline = [\n",
    "    \"TAIL_NUM\",            # Aircraft tail number (if present)\n",
    "]\n",
    "\n",
    "missing_target = set(required_target_baseline) - set(target_cols)\n",
    "missing_optional = set(optional_target_baseline) - set(target_cols)\n",
    "\n",
    "if missing_target:\n",
    "    print(f\"Missing CORE target-encoded features: {missing_target}\")\n",
    "    print(\"   → These are critical for the model!\")\n",
    "else:\n",
    "    print(f\"✓ All core target-encoded features present\")\n",
    "\n",
    "if missing_optional:\n",
    "    print(f\"Missing optional features: {missing_optional}\")\n",
    "    print(\"   → These are optional but could improve performance\")\n",
    "\n",
    "print(f\"\\nActual target-encoded features ({len(target_cols)}):\")\n",
    "for col in sorted(target_cols):\n",
    "    card = cardinality.get(col, \"?\")\n",
    "    print(f\"  - {col:35s}  (cardinality: {card})\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Check 3: Verify Binary String Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== Check 3: Binary String Features ===\")\n",
    "\n",
    "# Expected binary features (cardinality = 2)\n",
    "expected_binary = [\n",
    "    # These depend on your data - examples:\n",
    "    # \"is_holiday\", \"is_weekend\", etc.\n",
    "]\n",
    "\n",
    "print(f\"Binary string features found ({len(binary_string_cols)}):\")\n",
    "if len(binary_string_cols) > 0:\n",
    "    for col in sorted(binary_string_cols):\n",
    "        # Show sample values\n",
    "        sample_values = df_baseline.select(col).distinct().limit(2).collect()\n",
    "        values = [row[col] for row in sample_values]\n",
    "        print(f\"  - {col:35s}  values: {values}\")\n",
    "else:\n",
    "    print(\"  (none)\")\n",
    "    print(\"  → This is OK - binary features can be treated as numeric later\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Check 4: Verify Cardinality Thresholds\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== Check 4: Cardinality Distribution ===\")\n",
    "\n",
    "print(f\"\\nEncoding rules:\")\n",
    "print(f\"  Binary (=2):      {len(binary_string_cols)} features\")\n",
    "print(f\"  One-hot (3-50):   {len(onehot_cols)} features\")\n",
    "print(f\"  Target (>50):     {len(target_cols)} features\")\n",
    "print(f\"  Total string:     {len(string_cols)} features\")\n",
    "\n",
    "# Check if any features fell through the cracks\n",
    "assigned = set(binary_string_cols) | set(onehot_cols) | set(target_cols)\n",
    "unassigned = set(string_cols) - assigned\n",
    "\n",
    "if unassigned:\n",
    "    print(f\"\\n WARNING: {len(unassigned)} features not assigned to any encoding:\")\n",
    "    for col in sorted(unassigned):\n",
    "        card = cardinality.get(col, \"?\")\n",
    "        print(f\"  - {col:35s}  (cardinality: {card})\")\n",
    "    print(\"  → Check your threshold logic!\")\n",
    "else:\n",
    "    print(f\"\\n✓ All string features assigned to an encoding strategy\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Check 5: Compare with Engineered Model Requirements\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== Check 5: Baseline vs Engineered Feature Comparison ===\")\n",
    "\n",
    "# Features that should exist in both baseline and engineered\n",
    "common_required = [\n",
    "    \"DEST\",\n",
    "    \"ORIGIN\", \n",
    "    \"OP_UNIQUE_CARRIER\",\n",
    "]\n",
    "\n",
    "baseline_has = [col for col in common_required if col in string_cols]\n",
    "baseline_missing = [col for col in common_required if col not in string_cols]\n",
    "\n",
    "print(f\"Common required features:\")\n",
    "for col in common_required:\n",
    "    status = \"✓\" if col in string_cols else \"✗\"\n",
    "    encoding = \"unknown\"\n",
    "    if col in target_cols:\n",
    "        encoding = \"target-encoded\"\n",
    "    elif col in onehot_cols:\n",
    "        encoding = \"one-hot\"\n",
    "    elif col in binary_string_cols:\n",
    "        encoding = \"binary\"\n",
    "    \n",
    "    print(f\"  {status} {col:30s}  [{encoding}]\")\n",
    "\n",
    "if baseline_missing:\n",
    "    print(f\"\\nWARNING: Missing common features: {baseline_missing}\")\n",
    "    print(\"   → These should exist in your baseline data!\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE FEATURE VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_core_present = (len(missing_target) == 0)\n",
    "status = \"PASS ✓\" if all_core_present else \"NEEDS ATTENTION\"\n",
    "\n",
    "print(f\"One-Hot Features:     {len(onehot_cols)} total\")\n",
    "print(f\"Target Features:      {len(target_cols)} total, {len(missing_target)} missing\")\n",
    "print(f\"Binary Features:      {len(binary_string_cols)} total\")\n",
    "print(f\"Unassigned Features:  {len(unassigned) if 'unassigned' in locals() else 0}\")\n",
    "print(f\"\\nOverall Status: {status}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Recommendation\n",
    "if not all_core_present:\n",
    "    print(\"\\nACTION REQUIRED:\")\n",
    "    print(\"   Review missing core features and verify data pipeline\")\n",
    "elif len(unassigned) > 0:\n",
    "    print(\"\\nREVIEW NEEDED:\")\n",
    "    print(\"   Some features were not assigned to any encoding strategy\")\n",
    "else:\n",
    "    print(\"\\n✓ Baseline feature setup looks good!\")\n",
    "    print(\"  Ready to proceed with model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c71626-14e9-4047-be6e-cae49edc2b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99cdf2c-983a-4dd2-9158-98be0d594276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Target encoding helper ---------------------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def add_target_encoding_for_fold(\n",
    "    train_df,\n",
    "    valid_df,\n",
    "    target_cols,\n",
    "    label_col,\n",
    "    k=100.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute smoothed target encoding for each column in target_cols\n",
    "    based only on the current training fold, and apply it to both\n",
    "    train and validation dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Global positive rate in the current training fold\n",
    "    global_mean = train_df.agg(F.mean(label_col)).first()[0]\n",
    "\n",
    "    for c in target_cols:\n",
    "        # Compute category-level stats on the training fold\n",
    "        stats = (\n",
    "            train_df\n",
    "            .groupBy(c)\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"n\"),\n",
    "                F.mean(label_col).alias(\"cat_mean\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                f\"{c}_te\",\n",
    "                (F.col(\"cat_mean\") * F.col(\"n\") + F.lit(global_mean) * F.lit(k))\n",
    "                / (F.col(\"n\") + F.lit(k))\n",
    "            )\n",
    "            .select(c, f\"{c}_te\")\n",
    "        )\n",
    "\n",
    "        # Join encoded values back to train and validation\n",
    "        train_df = (\n",
    "            train_df\n",
    "            .join(stats, on=c, how=\"left\")\n",
    "            .fillna({f\"{c}_te\": global_mean})\n",
    "        )\n",
    "\n",
    "        valid_df = (\n",
    "            valid_df\n",
    "            .join(stats, on=c, how=\"left\")\n",
    "            .fillna({f\"{c}_te\": global_mean})\n",
    "        )\n",
    "\n",
    "        # Optionally drop the original high-cardinality string column\n",
    "        train_df = train_df.drop(c)\n",
    "        valid_df = valid_df.drop(c)\n",
    "\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f269cdef-d259-483c-91ef-c3d82e481693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec4370b-19f6-45fb-989f-ed023c9cb5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Categorical preprocessing (index + one-hot) ------------------------------\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# StringIndexer for one-hot and binary categorical features\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=f\"{c}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in onehot_cols + binary_string_cols\n",
    "]\n",
    "\n",
    "# One-hot encoder for low-cardinality features\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in onehot_cols],\n",
    "    outputCols=[f\"{c}_ohe\" for c in onehot_cols],\n",
    "    handleInvalid=\"keep\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06e1d4fe-2488-4c13-9f60-de65043f4544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Baseline for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8d1fb5-c577-4a5d-a458-39cbd9faff43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# --- Evaluator ----------------------------------------------------------------\n",
    "# We use AUC-PR because the data is highly imbalanced\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "# --- One fold of LR with TE + one-hot + assembler ----------------------------\n",
    "def run_lr_on_fold(train_df_raw, valid_df_raw, reg_param, elastic_net_param):\n",
    "    \"\"\"\n",
    "    For a given time-based fold, first apply target encoding using\n",
    "    ONLY the training part of the fold, then run LR pipeline and\n",
    "    return AUC-PR on the validation part.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Fold-specific target encoding\n",
    "    train_df, valid_df = add_target_encoding_for_fold(\n",
    "        train_df=train_df_raw,\n",
    "        valid_df=valid_df_raw,\n",
    "        target_cols=target_cols,\n",
    "        label_col=label_col,\n",
    "        k=100.0\n",
    "    )\n",
    "\n",
    "    # 2) Recompute numeric feature columns AFTER target encoding\n",
    "    numeric_cols = [\n",
    "        c for c, t in train_df.dtypes\n",
    "        if t in (\"double\", \"int\", \"bigint\", \"float\") and c != label_col\n",
    "    ]\n",
    "\n",
    "    # 3) Replace NaN / null in numeric columns (avoid VectorAssembler NaN/Inf)\n",
    "    num_fill = {c: 0.0 for c in numeric_cols}\n",
    "    train_df = train_df.fillna(num_fill)\n",
    "    valid_df = valid_df.fillna(num_fill)\n",
    "\n",
    "    # 4) Binary string features will use their indexed version as numeric (0/1)\n",
    "    binary_idx_cols = [f\"{c}_idx\" for c in binary_string_cols]\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[f\"{c}_ohe\" for c in onehot_cols] +\n",
    "                  numeric_cols +\n",
    "                  binary_idx_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=label_col,\n",
    "        regParam=reg_param,\n",
    "        elasticNetParam=elastic_net_param,\n",
    "        maxIter=20\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + [encoder, assembler, lr])\n",
    "\n",
    "    model = pipeline.fit(train_df)\n",
    "    preds = model.transform(valid_df)\n",
    "    auc_pr = evaluator.evaluate(preds)\n",
    "\n",
    "    return auc_pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ca6f68-42f4-46de-97d0-e886f6ae7548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c29a0f-bbce-485e-a34e-2b089eae189a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Time-based folds (use raw df_baseline before any TE) ---------------------\n",
    "\n",
    "# Rolling time-series folds:\n",
    "# Fold 1: train on Q1,      validate on Q2\n",
    "# Fold 2: train on Q1–Q2,   validate on Q3\n",
    "\n",
    "USE_SMALL_LR = True\n",
    "SAMPLE_FRACTION_LR = 0.001\n",
    "\n",
    "def maybe_sample_baseline(df, quarter_filter):\n",
    "    base = df.filter(quarter_filter)\n",
    "    return base.sample(False, SAMPLE_FRACTION_LR, seed=42) if USE_SMALL_LR else base\n",
    "\n",
    "# sample + cache once per quarter\n",
    "df_q1 = maybe_sample_baseline(df_baseline, col(\"QUARTER\") == 1).cache()\n",
    "df_q2 = maybe_sample_baseline(df_baseline, col(\"QUARTER\") == 2).cache()\n",
    "df_q3 = maybe_sample_baseline(df_baseline, col(\"QUARTER\") == 3).cache()\n",
    "\n",
    "# force caching\n",
    "df_q1.count()\n",
    "df_q2.count()\n",
    "df_q3.count()\n",
    "\n",
    "folds = [\n",
    "    (\"Fold1\", df_q1, df_q2),\n",
    "    (\"Fold2\", df_q1.union(df_q2), df_q3),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14f66a6-98dd-4210-925c-b32e737dd496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2639ef-da4f-44d3-b3a1-e24798a232d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold1] regParam=0.0, elasticNetParam=0.0, AUC-PR=0.5377\n[Fold2] regParam=0.0, elasticNetParam=0.0, AUC-PR=0.4776\n--> Mean AUC-PR: 0.5077\n\n[Fold1] regParam=0.01, elasticNetParam=0.0, AUC-PR=0.5528\n[Fold2] regParam=0.01, elasticNetParam=0.0, AUC-PR=0.4891\n--> Mean AUC-PR: 0.5210\n\n[Fold1] regParam=0.1, elasticNetParam=0.0, AUC-PR=0.5516\n[Fold2] regParam=0.1, elasticNetParam=0.0, AUC-PR=0.4963\n--> Mean AUC-PR: 0.5240\n\n[Fold1] regParam=0.01, elasticNetParam=0.5, AUC-PR=0.5763\n[Fold2] regParam=0.01, elasticNetParam=0.5, AUC-PR=0.5164\n--> Mean AUC-PR: 0.5463\n\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\"regParam\": 0.0,  \"elasticNetParam\": 0.0},\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.0},\n",
    "    {\"regParam\": 0.1,  \"elasticNetParam\": 0.0},\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.5},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for params in param_grid:\n",
    "    reg = params[\"regParam\"]\n",
    "    en  = params[\"elasticNetParam\"]\n",
    "    fold_scores = []\n",
    "    for fold_name, fold_train, fold_valid in folds:\n",
    "        auc_pr = run_lr_on_fold(fold_train, fold_valid, reg, en)\n",
    "        print(f\"[{fold_name}] regParam={reg}, elasticNetParam={en}, AUC-PR={auc_pr:.4f}\")\n",
    "        fold_scores.append(auc_pr)\n",
    "    mean_auc = sum(fold_scores) / len(fold_scores)\n",
    "    results.append({\"regParam\": reg, \"elasticNetParam\": en, \"mean_auc_pr\": mean_auc})\n",
    "    print(f\"--> Mean AUC-PR: {mean_auc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2b4154-2801-4a10-8230-f98e17ed943b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold1] regParam=0.01, elasticNetParam=0.5, AUC-PR=0.5763\n[Fold2] regParam=0.01, elasticNetParam=0.5, AUC-PR=0.5164\n--> Mean AUC-PR: 0.5420\n\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.5},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for params in param_grid:\n",
    "    reg = params[\"regParam\"]\n",
    "    en  = params[\"elasticNetParam\"]\n",
    "    for fold_name, fold_train, fold_valid in folds:\n",
    "        auc_pr = run_lr_on_fold(fold_train, fold_valid, reg, en)\n",
    "        print(f\"[{fold_name}] regParam={reg}, elasticNetParam={en}, AUC-PR={auc_pr:.4f}\")\n",
    "        fold_scores.append(auc_pr)\n",
    "    mean_auc = sum(fold_scores) / len(fold_scores)\n",
    "    results.append({\"regParam\": reg, \"elasticNetParam\": en, \"mean_auc_pr\": mean_auc})\n",
    "    print(f\"--> Mean AUC-PR: {mean_auc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf07f62a-f79d-489a-b7b4-70d4cb207444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b343ccfd-3876-4162-9cc3-192a4ceee404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Find Best Paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656a566f-f2f2-437e-bdfe-d61ae1cc6659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pick best hyperparameters from CV results\n",
    "\n",
    "best_result = max(results, key=lambda r: r[\"mean_auc_pr\"])\n",
    "best_reg = best_result[\"regParam\"]\n",
    "best_en  = best_result[\"elasticNetParam\"]\n",
    "\n",
    "print(\"Best hyperparameters from Baseline CV:\")\n",
    "print(f\"  regParam={best_reg}, elasticNetParam={best_en}, mean AUC-PR={best_result['mean_auc_pr']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fdd9f88-f3af-4501-968a-47893af1eed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3edad1f-c9c8-4ad1-b4ab-8f8dd91c6474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array \n",
    "\n",
    "def train_baseline_lr_and_eval(train_df_raw, test_df_raw, reg_param, elastic_net_param,\n",
    "                               threshold=0.5, beta=0.5):\n",
    "    \"\"\"\n",
    "    Train final Logistic Regression model on full training window (Q1–Q3)\n",
    "    with target encoding, and evaluate on Q4.\n",
    "\n",
    "    Additionally:\n",
    "    - Compute and print F-beta score (default F0.5) using probability thresholding.\n",
    "    \"\"\"\n",
    "\n",
    "    # 2.1 Target encoding using ONLY training set statistics\n",
    "    #     (avoid leakage: compute TE from train_df and apply to both train/test)\n",
    "    train_df, test_df = add_target_encoding_for_fold(\n",
    "        train_df=train_df_raw,\n",
    "        valid_df=test_df_raw,\n",
    "        target_cols=target_cols,\n",
    "        label_col=label_col,\n",
    "        k=100.0\n",
    "    )\n",
    "\n",
    "    # 2.2 Recompute numeric columns AFTER target encoding is added\n",
    "    numeric_cols = [\n",
    "        c for c, t in train_df.dtypes\n",
    "        if t in (\"double\", \"int\", \"bigint\", \"float\") and c != label_col\n",
    "    ]\n",
    "\n",
    "    # 2.3 Fill numeric nulls to avoid NaN/Inf when assembling features\n",
    "    num_fill = {c: 0.0 for c in numeric_cols}\n",
    "    train_df = train_df.fillna(num_fill)\n",
    "    test_df  = test_df.fillna(num_fill)\n",
    "\n",
    "    # 2.4 Binary string features: use their indexed numeric version\n",
    "    #     (StringIndexer already applied earlier -> _idx columns)\n",
    "    binary_idx_cols = [f\"{c}_idx\" for c in binary_string_cols]\n",
    "\n",
    "    # Assemble all features (numeric + OHE + binary index)\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[f\"{c}_ohe\" for c in onehot_cols] +\n",
    "                  numeric_cols +\n",
    "                  binary_idx_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    # Logistic Regression model\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=label_col,\n",
    "        regParam=reg_param,\n",
    "        elasticNetParam=elastic_net_param,\n",
    "        maxIter=20\n",
    "    )\n",
    "\n",
    "    # Full pipeline\n",
    "    pipeline = Pipeline(stages=indexers + [encoder, assembler, lr])\n",
    "\n",
    "    # 2.5 Fit final model using Q1–Q3\n",
    "    final_model = pipeline.fit(train_df)\n",
    "\n",
    "    # 2.6 Predict on Q4\n",
    "    test_preds = final_model.transform(test_df)\n",
    "\n",
    "    # Evaluate AUC-PR (Spark built-in metric)\n",
    "    auc_pr = evaluator.evaluate(test_preds)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # NEW SECTION: Compute F-beta (default F0.5)\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    beta2 = beta ** 2\n",
    "\n",
    "    # Step 1: convert probability (VectorUDT) -> array<double>\n",
    "    #         then take the positive-class probability (index 1)\n",
    "    test_preds_with_prob = test_preds.withColumn(\n",
    "        \"prob_pos\",\n",
    "        vector_to_array(col(\"probability\")).getItem(1)\n",
    "    )\n",
    "\n",
    "    # Step 2: threshold on prob_pos to get binary predictions\n",
    "    preds_with_label = test_preds_with_prob.withColumn(\n",
    "        \"pred_label\",\n",
    "        (col(\"prob_pos\") >= threshold).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # Step 3: compute TP, FP, FN\n",
    "    stats = (\n",
    "        preds_with_label\n",
    "        .select(\n",
    "            ((col(\"pred_label\") == 1) & (col(label_col) == 1)).cast(\"int\").alias(\"tp\"),\n",
    "            ((col(\"pred_label\") == 1) & (col(label_col) == 0)).cast(\"int\").alias(\"fp\"),\n",
    "            ((col(\"pred_label\") == 0) & (col(label_col) == 1)).cast(\"int\").alias(\"fn\"),\n",
    "        )\n",
    "        .groupBy()\n",
    "        .sum()\n",
    "        .collect()[0]\n",
    "    )\n",
    "\n",
    "    tp = stats[\"sum(tp)\"]\n",
    "    fp = stats[\"sum(fp)\"]\n",
    "    fn = stats[\"sum(fn)\"]\n",
    "\n",
    "    # Precision / recall / F-beta (F0.5 by default)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        f_beta = 0.0\n",
    "    else:\n",
    "        f_beta = (1 + beta2) * precision * recall / (beta2 * precision + recall)\n",
    "    \n",
    "    return final_model, test_preds, auc_pr, f_beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afd4c98-e1cb-44ec-afd8-995128a74a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec9c451-7a17-4cd6-8cbb-7082a569aa54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df_baseline: QUARTER < 4\n",
    "# test_df_baseline : QUARTER == 4\n",
    "baseline_model, baseline_test_preds, baseline_auc_pr, baseline_f05 = train_baseline_lr_and_eval(\n",
    "    train_df_baseline,\n",
    "    test_df_baseline,\n",
    "    best_reg,\n",
    "    best_en\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2208a4b-9733-428b-bac8-c17018e27368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Logistic Regression Results ===\nAUC-PR: 0.5149\nF0.5  : 0.5476\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Baseline Logistic Regression Results ===\")\n",
    "print(f\"AUC-PR: {baseline_auc_pr:.4f}\")\n",
    "print(f\"F0.5  : {baseline_f05:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc4d418d-529c-4542-921c-0376c9939c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6584e1d6-1150-4e5c-bda9-5286cf5f7d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5,704,114\nColumns: 108\n"
     ]
    }
   ],
   "source": [
    "#df_engineered_raw_old = spark.read.parquet(\"dbfs:/student-groups/Group_4_4/joined_1Y_final_feature_clean.parquet\")\n",
    "\n",
    "df_engineered_raw = spark.read.parquet(\"dbfs:/student-groups/Group_4_4/checkpoint_5_final_clean_2015.parquet\")\n",
    "\n",
    "print(f\"Rows: {df_engineered_raw.count():,}\")\n",
    "print(f\"Columns: {len(df_engineered_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ee4d69-39d7-45e6-b1e4-a78479269e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('OP_UNIQUE_CARRIER', 'string'),\n",
       " ('DEST', 'string'),\n",
       " ('ORIGIN', 'string'),\n",
       " ('FL_DATE', 'date'),\n",
       " ('departure_hour', 'int'),\n",
       " ('prediction_utc', 'timestamp'),\n",
       " ('origin_obs_utc', 'timestamp'),\n",
       " ('asof_minutes', 'bigint'),\n",
       " ('YEAR', 'int'),\n",
       " ('QUARTER', 'int'),\n",
       " ('DAY_OF_MONTH', 'int'),\n",
       " ('DAY_OF_WEEK', 'int'),\n",
       " ('OP_CARRIER_FL_NUM', 'int'),\n",
       " ('CRS_DEP_TIME', 'int'),\n",
       " ('CRS_ARR_TIME', 'int'),\n",
       " ('ORIGIN_AIRPORT_ID', 'int'),\n",
       " ('ORIGIN_STATE_ABR', 'string'),\n",
       " ('DEST_AIRPORT_ID', 'int'),\n",
       " ('DEST_STATE_ABR', 'string'),\n",
       " ('HourlyDewPointTemperature', 'double'),\n",
       " ('HourlyPrecipitation', 'double'),\n",
       " ('HourlyWindSpeed', 'double'),\n",
       " ('HourlyWindDirection', 'double'),\n",
       " ('HourlyWindGustSpeed', 'double'),\n",
       " ('HourlyVisibility', 'double'),\n",
       " ('HourlyRelativeHumidity', 'double'),\n",
       " ('HourlyAltimeterSetting', 'double'),\n",
       " ('origin_airport_lat', 'double'),\n",
       " ('origin_airport_lon', 'double'),\n",
       " ('dest_airport_lat', 'double'),\n",
       " ('dest_airport_lon', 'double'),\n",
       " ('origin_station_dis', 'double'),\n",
       " ('dest_station_dis', 'double'),\n",
       " ('origin_type', 'string'),\n",
       " ('dest_type', 'string'),\n",
       " ('DEP_DEL15', 'int'),\n",
       " ('CANCELLED', 'int'),\n",
       " ('CANCELLATION_CODE', 'string'),\n",
       " ('DIVERTED', 'int'),\n",
       " ('departure_month', 'int'),\n",
       " ('departure_dayofweek', 'int'),\n",
       " ('is_weekend', 'int'),\n",
       " ('season', 'string'),\n",
       " ('is_peak_hour', 'int'),\n",
       " ('is_peak_month', 'int'),\n",
       " ('time_of_day_early_morning', 'int'),\n",
       " ('time_of_day_morning', 'int'),\n",
       " ('time_of_day_afternoon', 'int'),\n",
       " ('time_of_day_evening', 'int'),\n",
       " ('time_of_day_night', 'int'),\n",
       " ('rolling_origin_num_delays_24h', 'bigint'),\n",
       " ('dep_delay15_24h_rolling_avg_by_origin', 'double'),\n",
       " ('dep_delay15_24h_rolling_avg_by_origin_carrier', 'double'),\n",
       " ('dep_delay15_24h_rolling_avg_by_origin_dayofweek', 'double'),\n",
       " ('is_holiday_window', 'int'),\n",
       " ('weather_severity_index', 'double'),\n",
       " ('distance_medium', 'int'),\n",
       " ('distance_long', 'int'),\n",
       " ('distance_very_long', 'int'),\n",
       " ('weather_condition_category', 'string'),\n",
       " ('airport_traffic_density', 'double'),\n",
       " ('carrier_flight_count', 'bigint'),\n",
       " ('weather_obs_lag_hours', 'double'),\n",
       " ('log_distance', 'double'),\n",
       " ('is_rainy', 'int'),\n",
       " ('prev_flight_dep_del15', 'double'),\n",
       " ('prev_flight_crs_elapsed_time', 'double'),\n",
       " ('hours_since_prev_flight', 'double'),\n",
       " ('is_first_flight_of_aircraft', 'int'),\n",
       " ('turnaround_category', 'string'),\n",
       " ('is_business_hours', 'int'),\n",
       " ('is_holiday_month', 'int'),\n",
       " ('num_airport_wide_delays', 'bigint'),\n",
       " ('temp_dest_count', 'bigint'),\n",
       " ('oncoming_flights', 'bigint'),\n",
       " ('prior_day_delay_rate', 'double'),\n",
       " ('time_based_congestion_ratio', 'double'),\n",
       " ('sky_condition_parsed', 'string'),\n",
       " ('rapid_weather_change', 'int'),\n",
       " ('temp_anomaly', 'double'),\n",
       " ('dep_time_sin', 'double'),\n",
       " ('dep_time_cos', 'double'),\n",
       " ('arr_time_sin', 'double'),\n",
       " ('arr_time_cos', 'double'),\n",
       " ('day_of_week_sin', 'double'),\n",
       " ('day_of_week_cos', 'double'),\n",
       " ('month_sin', 'double'),\n",
       " ('month_cos', 'double'),\n",
       " ('wind_direction_sin', 'double'),\n",
       " ('wind_direction_cos', 'double'),\n",
       " ('extreme_precipitation', 'int'),\n",
       " ('extreme_wind', 'int'),\n",
       " ('extreme_temperature', 'int'),\n",
       " ('low_visibility', 'int'),\n",
       " ('extreme_weather_score', 'double'),\n",
       " ('origin_degree_centrality', 'double'),\n",
       " ('dest_degree_centrality', 'double'),\n",
       " ('carrier_delay_stddev', 'double'),\n",
       " ('pagerank', 'double'),\n",
       " ('days_since_epoch', 'int'),\n",
       " ('origin_1yr_delay_rate', 'double'),\n",
       " ('dest_1yr_delay_rate', 'double'),\n",
       " ('rolling_30day_volume', 'bigint'),\n",
       " ('route_1yr_volume', 'bigint')]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_engineered_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a138fcd0-23c7-45f9-a706-b31af86a48cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b4a489-22c2-439e-9124-ff698482f13c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of *_removed columns: 0\nColumns before drop: 108\nColumns after  drop: 97\n"
     ]
    }
   ],
   "source": [
    "label_col = \"DEP_DEL15\"\n",
    "\n",
    "# 1. Drop obvious leakage columns (same logic as baseline)\n",
    "leakage_cols_eng = [\n",
    "# --- Original leakage flags ---\n",
    "    \"CANCELLED\",\n",
    "    \"CANCELLATION_CODE\",\n",
    "    \"DIVERTED\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "\n",
    "    # --- Actual Times (known only after flight) ---\n",
    "    \"DEP_TIME\",\n",
    "    \"ARR_TIME\",\n",
    "    \"WHEELS_OFF\",\n",
    "    \"WHEELS_ON\",\n",
    "\n",
    "    # --- Actual Delays (target-related) ---\n",
    "    \"DEP_DELAY\",\n",
    "    \"ARR_DELAY\",\n",
    "    \"ARR_DEL15\",\n",
    "\n",
    "    # --- Taxi Times (known only after departure) ---\n",
    "    \"TAXI_OUT\",\n",
    "    \"TAXI_IN\",\n",
    "\n",
    "    # --- Flight Durations (known only after completion) ---\n",
    "    \"ACTUAL_ELAPSED_TIME\",\n",
    "    \"AIR_TIME\",\n",
    "\n",
    "    # --- Delay Breakdowns (only known after delay cause assigned) ---\n",
    "    \"CARRIER_DELAY\",\n",
    "    \"WEATHER_DELAY\",\n",
    "    \"NAS_DELAY\",\n",
    "    \"SECURITY_DELAY\",\n",
    "    \"LATE_AIRCRAFT_DELAY\",\n",
    "\n",
    "    # --- Engineered leakage (future or aggregate outcome) ---\n",
    "    \"same_day_prior_delay_percentage\",\n",
    "    \"prior_day_delay_rate\",\n",
    "    \"rolling_origin_num_delays_24h\",\n",
    "    \"dep_delay15_24h_rolling_avg_by_origin\",\n",
    "    \"dep_delay15_24h_rolling_avg_by_origin_carrier\",\n",
    "    \"dep_delay15_24h_rolling_avg_by_origin_dayofweek\",\n",
    "    \"origin_1yr_delay_rate\",\n",
    "    \"dest_1yr_delay_rate\",\n",
    "    \"rolling_30day_volume\",\n",
    "    \"route_1yr_volume\",\n",
    "]\n",
    "\n",
    "# 2. Drop all *_removed columns (these were marked as removed features)\n",
    "removed_cols_eng = [c for c in df_engineered_raw.columns if c.endswith(\"_removed\")]\n",
    "\n",
    "print(f\"Number of *_removed columns: {len(removed_cols_eng)}\")\n",
    "\n",
    "df_engineered = (\n",
    "    df_engineered_raw\n",
    "      .drop(*leakage_cols_eng)\n",
    "      .drop(*removed_cols_eng)\n",
    ")\n",
    "\n",
    "df_engineered = df_engineered.cache()\n",
    "df_engineered.count()\n",
    "\n",
    "print(\"Columns before drop:\", len(df_engineered_raw.columns))\n",
    "print(\"Columns after  drop:\", len(df_engineered.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c901edbd-3637-4fff-bf02-dd531d28662e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No '_removed' columns found (good!)\n"
     ]
    }
   ],
   "source": [
    "# Check no *_removed columns remain\n",
    "\n",
    "remaining_removed = [c for c in df_engineered.columns if \"_removed\" in c]\n",
    "\n",
    "if len(remaining_removed) == 0:\n",
    "    print(\"✓ No '_removed' columns found (good!)\")\n",
    "else:\n",
    "    print(f\"LEAKAGE WARNING: Found {len(remaining_removed)} '_removed' columns:\")\n",
    "    for col in remaining_removed:\n",
    "        print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5946d5c5-4312-4690-85a1-387a8e96c81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspicious columns (verify not leakage): []\n"
     ]
    }
   ],
   "source": [
    "# Check for post-departure features\n",
    "\n",
    "post_departure_keywords = [\"ARR_\", \"WHEELS_\", \"TAXI_\", \"ACTUAL_ELAPSED\", \"AIR_TIME\"]\n",
    "suspicious_cols = [\n",
    "    c for c in df_engineered.columns\n",
    "    if any(kw in c for kw in post_departure_keywords)\n",
    "]\n",
    "print(f\"Suspicious columns (verify not leakage): {suspicious_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6427cf05-f67e-4464-8d2d-67f8cd2c286a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f155d14a-0daa-4a6f-87e2-aab329a227fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== Skewness ranking (high → low) ===\norigin_station_dis                        110.662\ndest_station_dis                          110.382\nhours_since_prev_flight                   95.118\nis_first_flight_of_aircraft               34.089\nHourlyPresentWeatherType_indexed          24.859\nHourlyPrecipitation                       21.744\nrf_prob_delay                             8.958\nweather_x_airport_delays                  8.364\nrapid_weather_change                      6.280\ntime_of_day_early_morning                 4.827\nweekend_x_route_volume                    4.564\nextreme_precipitation                     3.979\nextreme_wind                              3.723\ndistance_very_long                        3.536\nnum_airport_wide_delays                   3.424\nairport_traffic_density                   3.411\nDEST_indexed                              2.705\nORIGIN_indexed                            2.699\nroute_delays_30d                          2.373\nextreme_temperature                       2.363\nweather_condition_category_indexed        2.301\ntime_based_congestion_ratio               2.287\nis_rainy                                  2.242\nextreme_weather_score                     2.225\ncarrier_delays_at_origin_30d              2.223\ntime_of_day_night                         2.223\ntemp_x_holiday                            2.023\norigin_type_indexed                       1.962\ntime_of_day_morning                       1.875\nprior_flights_today                       1.829\nroute_delay_rate_x_peak_hour              1.755\noncoming_flights                          1.744\nprev_flight_dep_del15                     1.633\ndep_delay15_24h_rolling_avg_by_origin_weighted  1.494\ndep_delay15_24h_rolling_avg_by_origin_carrier_weighted  1.470\nHourlyWindGustSpeed                       1.419\nORIGIN_STATE_ABR_indexed                  1.395\nDEST_STATE_ABR_indexed                    1.391\ndistance_long                             1.356\nprev_flight_crs_elapsed_time              1.342\nroute_delay_rate_30d                      1.219\nis_holiday_month                          1.198\nis_weekend                                1.089\nasof_minutes                              1.064\nweather_obs_lag_hours                     1.064\nOP_UNIQUE_CARRIER_indexed                 0.994\nOP_CARRIER_FL_NUM                         0.869\ntime_of_day_evening                       0.846\ndistance_x_weather_severity               0.810\ndep_time_cos                              0.706\nis_peak_month                             0.645\narr_time_sin                              0.626\ndistance_medium                           0.617\nsky_condition_parsed_indexed              0.558\ntime_of_day_afternoon                     0.537\ndest_degree_centrality                    0.474\norigin_degree_centrality                  0.472\nday_hour_interaction_indexed              0.391\ndep_time_sin                              0.266\nturnaround_category_indexed               0.234\ncarrier_flight_count                      0.163\nHourlyAltimeterSetting                    0.140\nwind_direction_sin                        0.116\nseason_indexed                            0.061\nDAY_OF_WEEK                               0.054\nmonth_cos                                 0.052\nDEST_AIRPORT_ID                           0.051\nwind_direction_cos                        0.051\nORIGIN_AIRPORT_ID                         0.050\nDAY_OF_MONTH                              0.010\nmonth_sin                                 0.005\nday_of_week_cos                           0.003\ndeparture_dayofweek                       0.002\nQUARTER                                   -0.020\ntemp_anomaly                              -0.046\nday_of_week_sin                           -0.050\norigin_airport_lat                        -0.050\ndest_airport_lat                          -0.059\nHourlyWindDirection                       -0.108\nlog_distance                              -0.265\nHourlyRelativeHumidity                    -0.338\nHourlyDryBulbTemperature                  -0.661\ndeparture_hour_weighted                   -0.715\ndest_airport_lon                          -0.799\norigin_airport_lon                        -0.801\ncarrier_delay_stddev                      -0.878\ndays_since_carrier_last_delay_at_origin   -1.632\ndays_since_last_delay_route               -1.633\nrf_prob_delay_binned                      -2.313\nHourlyStationPressure                     -2.442\nHourlyVisibility                          -3.080\n"
     ]
    }
   ],
   "source": [
    "# Log transform skewed numeric columns\n",
    "\n",
    "# Compute skewness for numeric columns\n",
    "numeric_cols_raw = [\n",
    "    c for c,t in df_engineered.dtypes\n",
    "    if t in (\"double\",\"int\",\"bigint\",\"float\") and c != label_col\n",
    "]\n",
    "\n",
    "skew_df = df_engineered.select([\n",
    "    F.skewness(c).alias(c) for c in numeric_cols_raw\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Convert to list and sort by skew descending\n",
    "skew_sorted = sorted(\n",
    "    [(c, v) for c, v in skew_df.items() if v is not None],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Skewness ranking (high → low) ===\")\n",
    "for col, skew in skew_sorted:\n",
    "    print(f\"{col:40s}  {skew:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fdb44f-9247-4446-8edf-53162eb46a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nCOLUMNS SELECTED FOR LOG TRANSFORM\n======================================================================\nTotal: 17 columns\n\nColumns:\n  - HourlyPrecipitation                      (skewness: 21.744)\n  - origin_station_dis                       (skewness: 110.662)\n  - dest_station_dis                         (skewness: 110.382)\n  - airport_traffic_density                  (skewness: 3.411)\n  - hours_since_prev_flight                  (skewness: 95.118)\n  - num_airport_wide_delays                  (skewness: 3.424)\n  - time_based_congestion_ratio              (skewness: 2.287)\n  - extreme_weather_score                    (skewness: 2.225)\n  - route_delays_30d                         (skewness: 2.373)\n  - carrier_delays_at_origin_30d             (skewness: 2.223)\n  - weekend_x_route_volume                   (skewness: 4.564)\n  - weather_x_airport_delays                 (skewness: 8.364)\n  - rf_prob_delay                            (skewness: 8.958)\n  - DEST_indexed                             (skewness: 2.705)\n  - ORIGIN_indexed                           (skewness: 2.699)\n  - HourlyPresentWeatherType_indexed         (skewness: 24.859)\n  - weather_condition_category_indexed       (skewness: 2.301)\n"
     ]
    }
   ],
   "source": [
    "# Identify Log Transform Candidates\n",
    "\n",
    "# Log-transformations were automatically applied to numeric features with severe right-skew (skewness > 2), non-negative values, and more than two distinct levels. \n",
    "stats = df_engineered.select(\n",
    "    *[F.min(c).alias(f\"{c}_min\") for c in numeric_cols_raw],\n",
    "    *[approx_count_distinct(c).alias(f\"{c}_dc\") for c in numeric_cols_raw]\n",
    ").collect()[0]\n",
    "\n",
    "log_candidates = []\n",
    "\n",
    "for c in numeric_cols_raw:\n",
    "    skew = skew_df.get(c)\n",
    "    min_val = stats[f\"{c}_min\"]\n",
    "    dc = stats[f\"{c}_dc\"]\n",
    "\n",
    "    if skew is None:\n",
    "        continue\n",
    "\n",
    "    # Criteria for log transform:\n",
    "    # 1. Severe right-skew (skewness > 2)\n",
    "    # 2. Non-negative values (min >= 0)\n",
    "    # 3. More than 2 distinct values (dc > 2)\n",
    "    # 4. Not already log-transformed (no \"log\" in column name)\n",
    "    if (skew > 2 and \n",
    "        min_val is not None and \n",
    "        min_val >= 0 and \n",
    "        dc > 2 and \n",
    "        \"log\" not in c.lower()):\n",
    "        log_candidates.append(c)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLUMNS SELECTED FOR LOG TRANSFORM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total: {len(log_candidates)} columns\")\n",
    "print(\"\\nColumns:\")\n",
    "for c in log_candidates:\n",
    "    print(f\"  - {c:40s} (skewness: {skew_df[c]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed300ed-db9f-4329-9d2e-7163f7c512d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLog transform applied!\n   Created 17 new columns with '_log' suffix\n"
     ]
    }
   ],
   "source": [
    "# Apply log1p transform to create new columns with \"_log\" suffix\n",
    "for c in log_candidates:\n",
    "    df_engineered = df_engineered.withColumn(\n",
    "        f\"{c}_log\",\n",
    "        F.log1p(F.col(c))\n",
    "    )\n",
    "    \n",
    "# df_engineered = df_engineered.drop(*log_candidates)\n",
    "\n",
    "print(\"\\nLog transform applied!\")\n",
    "print(f\"   Created {len(log_candidates)} new columns with '_log' suffix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbaee613-2945-4887-a0e0-fefc69f73d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_cols_df_eng = [c for c, t in df_engineered.dtypes if t == \"string\" and c != label_col]\n",
    "string_cols_df_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ec3638-d6e0-4516-ac40-b167d0b0a9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0881fabd-c66f-40cc-b871-45b14d4c7d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows (engineered): 2145680\nTest rows  (engineered): 705151\n"
     ]
    }
   ],
   "source": [
    "# Time-based split\n",
    "\n",
    "USE_SAMPLE_IMPROVED = False\n",
    "SAMPLE_FRACTION_IMPROVED = 0.5\n",
    "\n",
    "def maybe_sample_improved(df, fraction=None):\n",
    "    \"\"\"\n",
    "    Returns sampled dataframe if sampling is enabled,\n",
    "    otherwise returns full dataframe.\n",
    "    \"\"\"\n",
    "    if USE_SAMPLE_IMPROVED:\n",
    "        return df.sample(False, fraction or SAMPLE_FRACTION_IMPROVED, seed=42)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Apply sampling once\n",
    "df_eng_base = maybe_sample_improved(df_engineered).cache()\n",
    "\n",
    "# Time-based split\n",
    "train_df_eng = df_eng_base.filter(col(\"QUARTER\") < 4).cache()\n",
    "test_df_eng  = df_eng_base.filter(col(\"QUARTER\") == 4).cache()\n",
    "\n",
    "print(\"Train rows (engineered):\", train_df_eng.count())\n",
    "print(\"Test rows  (engineered):\", test_df_eng.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14cf8c66-4125-43f2-83f9-a62d5cfa37b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify No Temporal Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d377a7e-52e4-4d63-ad96-59a40564f25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time range\n+--------+--------+-----------+-----------+\n|min_year|max_year|min_quarter|max_quarter|\n+--------+--------+-----------+-----------+\n|2015    |2015    |1          |3          |\n+--------+--------+-----------+-----------+\n\nTest time range\n+--------+--------+-----------+-----------+\n|min_year|max_year|min_quarter|max_quarter|\n+--------+--------+-----------+-----------+\n|2015    |2015    |4          |4          |\n+--------+--------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# check temporal ranges (no overlap)\n",
    "\n",
    "show_time_range(train_df_eng, \"Train time range\")\n",
    "show_time_range(test_df_eng,  \"Test time range\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f4ecfb-6539-4d69-b77d-50354c7d6d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476fed5d-8e2a-4d36-9ed6-314da80df6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nString columns (engineered df):\n[]\n\n=== Column cardinality on engineered df (high → low) ===\n"
     ]
    }
   ],
   "source": [
    "# 3. Identify string (categorical) columns on engineered df (excluding label)\n",
    "string_cols_eng = [\n",
    "    c for c, t in df_engineered.dtypes\n",
    "    if t == \"string\" and c != label_col\n",
    "]\n",
    "\n",
    "print(\"\\nString columns (engineered df):\")\n",
    "print(string_cols_eng)\n",
    "\n",
    "# 4. Compute cardinality for each string column\n",
    "cardinality_exprs_eng = [\n",
    "    approx_count_distinct(c).alias(c)\n",
    "    for c in string_cols_eng\n",
    "]\n",
    "\n",
    "cardinality_row_eng = df_engineered.select(cardinality_exprs_eng).first()\n",
    "cardinality_eng = {c: cardinality_row_eng[c] for c in string_cols_eng}\n",
    "\n",
    "# 5. Show cardinalities sorted (high → low)\n",
    "sorted_cardinality_eng = sorted(\n",
    "    cardinality_eng.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Column cardinality on engineered df (high → low) ===\")\n",
    "for col_name, cnt in sorted_cardinality_eng:\n",
    "    print(f\"{col_name:35s}  {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e211f68d-172c-44a6-ab13-8ab23494b90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== Encoding assignment on engineered df ===\nTarget encoding: []\nOne-hot encoding: []\nBinary string : []\n"
     ]
    }
   ],
   "source": [
    "# 6. Assign encoding types based on cardinality\n",
    "high_card_threshold = 30   # > 30 → target encoding\n",
    "low_card_min = 3           # 3–30 → one-hot\n",
    "\n",
    "target_cols_eng = [\n",
    "    c for c in string_cols_eng\n",
    "    if cardinality_eng[c] > high_card_threshold\n",
    "]\n",
    "\n",
    "onehot_cols_eng = [\n",
    "    c for c in string_cols_eng\n",
    "    if low_card_min <= cardinality_eng[c] <= high_card_threshold\n",
    "]\n",
    "\n",
    "binary_string_cols_eng = [\n",
    "    c for c in string_cols_eng\n",
    "    if cardinality_eng[c] == 2\n",
    "]\n",
    "\n",
    "onehot_cols_eng = sorted(set(onehot_cols_eng) )\n",
    "target_cols_eng = [c for c in target_cols_eng ]\n",
    "\n",
    "print(\"\\n=== Encoding assignment on engineered df ===\")\n",
    "print(\"Target encoding:\", target_cols_eng)\n",
    "print(\"One-hot encoding:\", onehot_cols_eng)\n",
    "print(\"Binary string :\", binary_string_cols_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e98ed29-f6ca-4474-9ba6-094e85f31f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1297364f-4949-4e8b-a3b6-bb378ca49543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== VALIDATION #8: Checking Required One-Hot Features ===\n WARNING: Missing required one-hot features: {'weather_condition_category', 'sky_condition_parsed', 'season', 'origin_type', 'OP_UNIQUE_CARRIER', 'turnaround_category', 'dest_type'}\n\nActual one-hot features (0):\n\n=== VALIDATION #9: Checking Required Target-Encoded Features ===\nWARNING: Missing required target-encoded features: {'ORIGIN_STATE_ABR', 'DEST_STATE_ABR', 'DEST', 'ORIGIN'}\n\nActual target-encoded features (0):\n\n======================================================================\nFEATURE VALIDATION SUMMARY\n======================================================================\nOne-Hot Features:   0 total, 7 missing\nTarget Features:    0 total, 4 missing\nBinary Features:    0 total\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Add Feature Validation Checks \n",
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Verify Required One-Hot Encoded Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== VALIDATION #8: Checking Required One-Hot Features ===\")\n",
    "\n",
    "required_onehot = [\n",
    "    \"OP_UNIQUE_CARRIER\",\n",
    "    \"sky_condition_parsed\",\n",
    "    \"season\",\n",
    "    \"turnaround_category\",\n",
    "    \"origin_type\",\n",
    "    \"dest_type\",\n",
    "    \"weather_condition_category\",\n",
    "    # \"CANCELLATION_CODE\"\n",
    "]\n",
    "\n",
    "missing_onehot = set(required_onehot) - set(onehot_cols_eng)\n",
    "if missing_onehot:\n",
    "    print(f\" WARNING: Missing required one-hot features: {missing_onehot}\")\n",
    "else:\n",
    "    print(f\"✓ All required one-hot features present: {len(required_onehot)}/{len(required_onehot)}\")\n",
    "\n",
    "# Display actual one-hot features present\n",
    "print(f\"\\nActual one-hot features ({len(onehot_cols_eng)}):\")\n",
    "for col in sorted(onehot_cols_eng):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Verify Required Target Encoded Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n=== VALIDATION #9: Checking Required Target-Encoded Features ===\")\n",
    "\n",
    "required_target = [\n",
    "    # \"HourlyPresentWeatherType\",\n",
    "    \"DEST\",\n",
    "    \"ORIGIN\",\n",
    "    # \"day_hour_interaction\",\n",
    "    \"DEST_STATE_ABR\",\n",
    "    \"ORIGIN_STATE_ABR\"\n",
    "]\n",
    "\n",
    "missing_target = set(required_target) - set(target_cols_eng)\n",
    "if missing_target:\n",
    "    print(f\"WARNING: Missing required target-encoded features: {missing_target}\")\n",
    "else:\n",
    "    print(f\"✓ All required target-encoded features present: {len(required_target)}/{len(required_target)}\")\n",
    "\n",
    "# Display actual target-encoded features present\n",
    "print(f\"\\nActual target-encoded features ({len(target_cols_eng)}):\")\n",
    "for col in sorted(target_cols_eng):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"One-Hot Features:   {len(onehot_cols_eng)} total, {len(missing_onehot)} missing\")\n",
    "print(f\"Target Features:    {len(target_cols_eng)} total, {len(missing_target)} missing\")\n",
    "print(f\"Binary Features:    {len(binary_string_cols_eng)} total\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70cc47c8-8096-4946-b9f0-1cbcbf77a6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a8bf20-c0a4-48a0-b022-8cdf47ac5f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reuse add_target_encoding_for_fold() from Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "654b5283-8f39-48c9-82b4-1f60a3f3f426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d793d4-08e7-48de-98bd-0d0567217ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexers_eng = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=f\"{c}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in onehot_cols_eng\n",
    "]\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder  \n",
    "\n",
    "encoder_eng = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in onehot_cols_eng],\n",
    "    outputCols=[f\"{c}_ohe\" for c in onehot_cols_eng],\n",
    "    handleInvalid=\"keep\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814a3ae5-4ab8-41ce-ae79-66d9f8544e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a2d8fab-2231-46a4-b2dc-841c705b890e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4f85db-c2f6-442f-9660-21ed93deb301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nSCALING CONFIGURATION DEFINED\n======================================================================\nDatetime columns (no scaling):  4\nRobustScaler candidates:        10\nMinMaxScaler candidates:        3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOW define the scaling configuration.\n",
    "\n",
    "Key point: For columns that got log-transformed, we should:\n",
    "1. Include the NEW \"_log\" column in our features\n",
    "2. EXCLUDE the ORIGINAL column from RobustScaler (it's redundant)\n",
    "\n",
    "The categorize_numeric_features() function will handle this automatically\n",
    "by checking if a \"_log\" version exists.\n",
    "\"\"\"\n",
    "# Datetime columns (will NOT be scaled)\n",
    "DATETIME_COLS = [\n",
    "    \"YEAR\",\n",
    "    \"QUARTER\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\"\n",
    "]\n",
    "\n",
    "# RobustScaler candidates (for columns with outliers)\n",
    "# Note: If a column has a \"_log\" version, the original will be automatically\n",
    "# excluded by filter_log_transformed_cols()\n",
    "ROBUST_SCALER_COLS_BASE = [\n",
    "    # Ultra-high skewness (> 70) - but some may have been log-transformed\n",
    "    \"HourlyWindSpeed\",              \n",
    "    \"hours_since_prev_flight\",      \n",
    "    \"weather_severity_index\",       \n",
    "    \"origin_station_dis\",           \n",
    "    \"dest_station_dis\",             \n",
    "    \n",
    "    # High skewness (20-70)\n",
    "    \"HourlyPrecipitation\",          \n",
    "    \n",
    "    # Medium skewness (5-20) - these may NOT have been log-transformed\n",
    "    \"oncoming_flights\",             \n",
    "    \"rapid_weather_change\",         \n",
    "    \n",
    "    # Lower skewness but still have outliers\n",
    "    \"num_airport_wide_delays\",      \n",
    "    \"extreme_weather_score\",        \n",
    "]\n",
    "\n",
    "# MinMaxScaler candidates (for ratio/probability features)\n",
    "MINMAX_SCALER_COLS_BASE = [\n",
    "    \"dest_1yr_delay_rate\",          # Ratio feature (0 to ~1)\n",
    "    \"origin_1yr_delay_rate\",        # Ratio feature (0 to ~1)\n",
    "    \"prior_day_delay_rate\",         # Ratio feature (0 to ~1)\n",
    "    # \"HourlyRelativeHumidity\",     # Usually 0-100, but may not exist\n",
    "]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALING CONFIGURATION DEFINED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Datetime columns (no scaling):  {len(DATETIME_COLS)}\")\n",
    "print(f\"RobustScaler candidates:        {len(ROBUST_SCALER_COLS_BASE)}\")\n",
    "print(f\"MinMaxScaler candidates:        {len(MINMAX_SCALER_COLS_BASE)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170f61f8-e938-4c8d-8e1f-0a22a28265d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNumeric columns considered for skew: 105\n\n======================================================================\nSCALING CONFIGURATION DEFINED (AUTO BY SKEW)\n======================================================================\nDatetime columns (no scaling):  4\nRobustScaler candidates:        11\n  - Ultra-high skew   (>70):    3\n  - High skew      (20–70]:     3\n  - Medium skew     (5–20]:     5\n  - Low skew        (<=5):      94 (not in ROBUST list)\nMinMaxScaler candidates:        7\n======================================================================\n\nDATETIME_COLS:\n  - YEAR\n  - QUARTER\n  - DAY_OF_MONTH\n  - DAY_OF_WEEK\n\nROBUST_SCALER_COLS_BASE:\n  - HourlyPrecipitation\n  - HourlyPrecipitation_log\n  - HourlyPresentWeatherType_indexed\n  - dest_station_dis\n  - hours_since_prev_flight\n  - is_first_flight_of_aircraft\n  - origin_station_dis\n  - rapid_weather_change\n  - rf_prob_delay\n  - rf_prob_delay_log\n  - weather_x_airport_delays\n\nMINMAX_SCALER_COLS_BASE:\n  - rf_prob_delay\n  - rf_prob_delay_binned\n  - rf_prob_delay_log\n  - route_delay_rate_30d\n  - route_delay_rate_x_peak_hour\n  - time_based_congestion_ratio\n  - time_based_congestion_ratio_log\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"\"\"\n",
    "Auto-define the scaling configuration based on skewness.\n",
    "\n",
    "Logic:\n",
    "- DATETIME_COLS: keep the same 4 time index columns (no scaling).\n",
    "- For numeric columns (excluding label, datetime, leakage):\n",
    "    * Compute skewness.\n",
    "    * Ultra-high skewness:  |skew| > 70\n",
    "    * High skewness:        20 < |skew| <= 70\n",
    "    * Medium skewness:      5  < |skew| <= 20\n",
    "    -> ROBUST_SCALER_COLS_BASE = union of (ultra + high + medium)\n",
    "\n",
    "- MINMAX_SCALER_COLS_BASE:\n",
    "    * Numeric columns whose names look like ratios/probabilities:\n",
    "      contain 'rate', 'ratio', 'prob', 'probability', 'share', 'fraction'\n",
    "\"\"\"\n",
    "\n",
    "# 1) Datetime columns (no scaling) - keep exactly as before\n",
    "DATETIME_COLS = [\n",
    "    \"YEAR\",\n",
    "    \"QUARTER\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "]\n",
    "\n",
    "# 2) Figure out which numeric columns to consider for skew\n",
    "#    (exclude label, datetime, known leakage cols if你已经有 leakage_cols_eng)\n",
    "all_dtypes = dict(df_engineered.dtypes)\n",
    "\n",
    "numeric_cols = [\n",
    "    c for c, t in df_engineered.dtypes\n",
    "    if t in (\"double\", \"float\", \"int\", \"bigint\")\n",
    "    and c != label_col\n",
    "    and c not in DATETIME_COLS\n",
    "    and (c not in leakage_cols_eng)  #\n",
    "]\n",
    "\n",
    "print(f\"\\nNumeric columns considered for skew: {len(numeric_cols)}\")\n",
    "\n",
    "# 3) Compute skewness for all numeric cols in a single pass\n",
    "skew_exprs = [F.skewness(F.col(c)).alias(c) for c in numeric_cols]\n",
    "skew_row = df_engineered.select(*skew_exprs).collect()[0]\n",
    "\n",
    "skew_dict = {c: skew_row[c] for c in numeric_cols}\n",
    "\n",
    "# 4) Bucketize by skew magnitude (matching your comments)\n",
    "ultra_high_skew = []\n",
    "high_skew = []\n",
    "medium_skew = []\n",
    "low_skew = []\n",
    "\n",
    "for c, v in skew_dict.items():\n",
    "    if v is None:\n",
    "        continue\n",
    "    s = abs(float(v))\n",
    "    if s > 70:\n",
    "        ultra_high_skew.append(c)\n",
    "    elif s > 20:\n",
    "        high_skew.append(c)\n",
    "    elif s > 5:\n",
    "        medium_skew.append(c)\n",
    "    else:\n",
    "        low_skew.append(c)\n",
    "\n",
    "# 5) Define ROBUST_SCALER_COLS_BASE from skew buckets\n",
    "ROBUST_SCALER_COLS_BASE = sorted(\n",
    "    set(ultra_high_skew + high_skew + medium_skew)\n",
    ")\n",
    "\n",
    "# 6) Define MINMAX_SCALER_COLS_BASE based on name patterns\n",
    "minmax_name_patterns = [\"rate\", \"ratio\", \"prob\", \"probability\", \"share\", \"fraction\"]\n",
    "\n",
    "MINMAX_SCALER_COLS_BASE = sorted([\n",
    "    c for c in numeric_cols\n",
    "    if any(p in c.lower() for p in minmax_name_patterns)\n",
    "])\n",
    "\n",
    "# 7) ）\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALING CONFIGURATION DEFINED (AUTO BY SKEW)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Datetime columns (no scaling):  {len(DATETIME_COLS)}\")\n",
    "print(f\"RobustScaler candidates:        {len(ROBUST_SCALER_COLS_BASE)}\")\n",
    "print(f\"  - Ultra-high skew   (>70):    {len(ultra_high_skew)}\")\n",
    "print(f\"  - High skew      (20–70]:     {len(high_skew)}\")\n",
    "print(f\"  - Medium skew     (5–20]:     {len(medium_skew)}\")\n",
    "print(f\"  - Low skew        (<=5):      {len(low_skew)} (not in ROBUST list)\")\n",
    "print(f\"MinMaxScaler candidates:        {len(MINMAX_SCALER_COLS_BASE)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDATETIME_COLS:\")\n",
    "for c in DATETIME_COLS:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "print(\"\\nROBUST_SCALER_COLS_BASE:\")\n",
    "for c in ROBUST_SCALER_COLS_BASE:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "print(\"\\nMINMAX_SCALER_COLS_BASE:\")\n",
    "for c in MINMAX_SCALER_COLS_BASE:\n",
    "    print(\"  -\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4886610-35f6-41ef-b8d5-9043b3f7f3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def get_actual_columns(candidate_cols, available_cols):\n",
    "    \"\"\"\n",
    "    Filter candidate columns to only those that actually exist in the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_cols : list\n",
    "        List of candidate column names\n",
    "    available_cols : list\n",
    "        List of columns available in the dataframe\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Columns that exist in both lists\n",
    "    \"\"\"\n",
    "    actual_cols = [c for c in candidate_cols if c in available_cols]\n",
    "    \n",
    "    missing_cols = [c for c in candidate_cols if c not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Following columns don't exist, skipped: {missing_cols}\")\n",
    "    \n",
    "    return actual_cols\n",
    "\n",
    "def filter_log_transformed_cols(robust_cols, all_numeric_cols):\n",
    "    \"\"\"\n",
    "    Exclude original columns that have been log-transformed.\n",
    "    \n",
    "    If a column \"col\" has a corresponding \"col_log\" version, we should NOT\n",
    "    apply RobustScaler to the original \"col\" (it's redundant).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    robust_cols : list\n",
    "        Candidate columns for RobustScaler\n",
    "    all_numeric_cols : list\n",
    "        All numeric columns (including \"_log\" columns)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Filtered columns (excluding originals that have \"_log\" versions)\n",
    "    \"\"\"\n",
    "    # Find all base column names that have been log-transformed\n",
    "    # e.g., if \"HourlyWindSpeed_log\" exists, then \"HourlyWindSpeed\" is log-transformed\n",
    "    log_transformed_base_cols = [\n",
    "        c.replace(\"_log\", \"\") \n",
    "        for c in all_numeric_cols \n",
    "        if \"_log\" in c\n",
    "    ]\n",
    "    \n",
    "    # Exclude original columns if they have \"_log\" versions\n",
    "    filtered_cols = [\n",
    "        c for c in robust_cols \n",
    "        if c not in log_transformed_base_cols\n",
    "    ]\n",
    "    \n",
    "    excluded = [c for c in robust_cols if c in log_transformed_base_cols]\n",
    "    if excluded:\n",
    "        print(f\"  Following columns have log versions, excluding originals from RobustScaler:\")\n",
    "        for c in excluded:\n",
    "            print(f\"    - {c} (use {c}_log instead)\")\n",
    "    \n",
    "    return filtered_cols\n",
    "\n",
    "\n",
    "def categorize_numeric_features(df, label_col):\n",
    "    \"\"\"\n",
    "    Categorize numeric features into different scaling groups.\n",
    "    \n",
    "    Groups:\n",
    "    - datetime: Date/time columns that should NOT be scaled\n",
    "    - robust: Features with outliers → use RobustScaler\n",
    "    - minmax: Ratio/probability features → use MinMaxScaler (0-1 normalization)\n",
    "    - standard: Other continuous features → use StandardScaler\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        PySpark DataFrame with all features\n",
    "    label_col : str\n",
    "        Name of the label column\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys: 'datetime', 'robust', 'minmax', 'standard'\n",
    "    \"\"\"\n",
    "    # 1. Get all numeric columns (excluding label)\n",
    "    all_numeric_cols = [\n",
    "        c for c, t in df.dtypes\n",
    "        if t in (\"double\", \"int\", \"bigint\", \"float\") and c != label_col\n",
    "    ]\n",
    "    \n",
    "    # 2. Get actual datetime columns that exist in df\n",
    "    datetime_cols = get_actual_columns(DATETIME_COLS, df.columns)\n",
    "    \n",
    "    # 3. Continuous numeric columns (excluding datetime)\n",
    "    continuous_numeric_cols = [\n",
    "        c for c in all_numeric_cols\n",
    "        if c not in datetime_cols\n",
    "    ]\n",
    "    \n",
    "    # 4. Get RobustScaler columns\n",
    "    # - First, filter to columns that actually exist\n",
    "    # - Then, exclude original columns that have been log-transformed\n",
    "    robust_cols = get_actual_columns(ROBUST_SCALER_COLS_BASE, continuous_numeric_cols)\n",
    "    robust_cols = filter_log_transformed_cols(robust_cols, all_numeric_cols)\n",
    "    \n",
    "    # 5. Get MinMaxScaler columns\n",
    "    minmax_cols = get_actual_columns(MINMAX_SCALER_COLS_BASE, continuous_numeric_cols)\n",
    "    \n",
    "    # 6. Remaining columns use StandardScaler\n",
    "    standard_cols = [\n",
    "        c for c in continuous_numeric_cols\n",
    "        if c not in robust_cols and c not in minmax_cols\n",
    "    ]\n",
    "    \n",
    "    result = {\n",
    "        'datetime': datetime_cols,\n",
    "        'robust': robust_cols,\n",
    "        'minmax': minmax_cols,\n",
    "        'standard': standard_cols\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE CATEGORIZATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Datetime columns (no scaling):        {len(datetime_cols):3d}\")\n",
    "    print(f\"RobustScaler columns (w/ outliers):   {len(robust_cols):3d}\")\n",
    "    print(f\"MinMaxScaler columns (ratios):        {len(minmax_cols):3d}\")\n",
    "    print(f\"StandardScaler columns (others):      {len(standard_cols):3d}\")\n",
    "    print(f\"{'Total continuous features:':<40} {len(continuous_numeric_cols):3d}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Optional: Print detailed lists\n",
    "    if robust_cols:\n",
    "        print(\"\\nRobustScaler columns:\")\n",
    "        for c in robust_cols:\n",
    "            print(f\"  - {c}\")\n",
    "    \n",
    "    if minmax_cols:\n",
    "        print(\"\\nMinMaxScaler columns:\")\n",
    "        for c in minmax_cols:\n",
    "            print(f\"  - {c}\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36e454d-3ce9-4be1-b237-938347ef19ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Following columns have log versions, excluding originals from RobustScaler:\n    - HourlyPrecipitation (use HourlyPrecipitation_log instead)\n    - HourlyPresentWeatherType_indexed (use HourlyPresentWeatherType_indexed_log instead)\n    - dest_station_dis (use dest_station_dis_log instead)\n    - hours_since_prev_flight (use hours_since_prev_flight_log instead)\n    - origin_station_dis (use origin_station_dis_log instead)\n    - rf_prob_delay (use rf_prob_delay_log instead)\n    - weather_x_airport_delays (use weather_x_airport_delays_log instead)\n\n======================================================================\nFEATURE CATEGORIZATION SUMMARY\n======================================================================\nDatetime columns (no scaling):          4\nRobustScaler columns (w/ outliers):     4\nMinMaxScaler columns (ratios):          7\nStandardScaler columns (others):       95\nTotal continuous features:               105\n======================================================================\n\nRobustScaler columns:\n  - HourlyPrecipitation_log\n  - is_first_flight_of_aircraft\n  - rapid_weather_change\n  - rf_prob_delay_log\n\nMinMaxScaler columns:\n  - rf_prob_delay\n  - rf_prob_delay_binned\n  - rf_prob_delay_log\n  - route_delay_rate_30d\n  - route_delay_rate_x_peak_hour\n  - time_based_congestion_ratio\n  - time_based_congestion_ratio_log\n"
     ]
    }
   ],
   "source": [
    "# Run the categorization\n",
    "FEATURE_CATEGORIES = categorize_numeric_features(\n",
    "    df=df_engineered,\n",
    "    label_col=label_col  # e.g., \"DEP_DEL15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "858017c8-2b3d-4f53-a6ae-5b61cfde19c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de7b69c-e6a9-4b64-828b-317114524004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def undersample_train(df, label_col, target_pos_ratio=0.5, seed=42):\n",
    "    \"\"\"\n",
    "    Undersample the majority class (label=0) while keeping all positives (label=1).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Input DataFrame\n",
    "    label_col : str\n",
    "        Name of the label column\n",
    "    target_pos_ratio : float\n",
    "        Desired share of positives after resampling (e.g., 0.4 = 40%)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Undersampled DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count positives / negatives\n",
    "    counts = (\n",
    "        df.groupBy(label_col)\n",
    "          .count()\n",
    "          .collect()\n",
    "    )\n",
    "    counts_dict = {row[label_col]: row[\"count\"] for row in counts}\n",
    "\n",
    "    n_pos = counts_dict.get(1, 0)\n",
    "    n_neg = counts_dict.get(0, 0)\n",
    "\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        # Degenerate case\n",
    "        return df\n",
    "\n",
    "    # Calculate how many negatives to keep\n",
    "    # target_pos_ratio = n_pos / (n_pos + neg_keep)\n",
    "    # => neg_keep = n_pos * (1 - r) / r\n",
    "    neg_keep = n_pos * (1 - target_pos_ratio) / target_pos_ratio\n",
    "\n",
    "    # If we already have fewer negatives than desired, don't downsample\n",
    "    if neg_keep >= n_neg:\n",
    "        return df\n",
    "\n",
    "    neg_frac = float(neg_keep) / float(n_neg)\n",
    "\n",
    "    # Split and sample\n",
    "    df_pos = df.filter(col(label_col) == 1)\n",
    "    df_neg = df.filter(col(label_col) == 0).sample(False, neg_frac, seed=seed)\n",
    "\n",
    "    # ========== FIX: Handle string columns BEFORE union ==========\n",
    "    \n",
    "    # Find string columns (excluding label)\n",
    "    string_cols = [c for c, t in df.dtypes if t == \"string\" and c != label_col]\n",
    "    \n",
    "    if string_cols:\n",
    "        # Create fillna dict\n",
    "        string_fill = {c: \"MISSING\" for c in string_cols}\n",
    "        \n",
    "        # Apply fillna to BOTH DataFrames BEFORE union\n",
    "        df_pos = df_pos.fillna(string_fill)\n",
    "        df_neg = df_neg.fillna(string_fill)\n",
    "    \n",
    "    # Now union (both have same schema, no nulls in string columns)\n",
    "    df_balanced = df_pos.unionByName(df_neg)\n",
    "    \n",
    "    # ========== End of fix ==========\n",
    "\n",
    "    print(f\"Undersampling: pos={n_pos}, neg={n_neg} -> neg_keep≈{int(neg_keep)}, frac={neg_frac:.3f}\")\n",
    "    print(f\"After undersampling: {df_balanced.count()} rows\")\n",
    "\n",
    "    return df_balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5fc62a-fea3-4d7b-9071-0ea053491f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e928920-7ad3-42ec-9cbc-b2e8f2e7cd8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9927f1b8-4fc3-451d-8eb0-b23278252c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE_SMALL_LR = True\n",
    "SAMPLE_FRACTION_LR = 0.01\n",
    "\n",
    "def maybe_sample_lr(df):\n",
    "    return df.sample(False, SAMPLE_FRACTION_LR, seed=42) if USE_SMALL_LR else df\n",
    "\n",
    "# Apply sampling ONCE\n",
    "df_eng_base = maybe_sample_lr(df_engineered).cache()\n",
    "df_eng_base.count()  # force materialization\n",
    "\n",
    "# sample + cache once per quarter\n",
    "df_eng_q1 = df_eng_base.filter(col(\"QUARTER\") == 1).cache()\n",
    "df_eng_q2 = df_eng_base.filter(col(\"QUARTER\") == 2).cache()\n",
    "df_eng_q3 = df_eng_base.filter(col(\"QUARTER\") == 3).cache()\n",
    "\n",
    "# force caching\n",
    "df_eng_q1.count()\n",
    "df_eng_q2.count()\n",
    "df_eng_q3.count()\n",
    "\n",
    "folds_eng = [\n",
    "    (\"Fold1\", df_eng_q1, df_eng_q2),\n",
    "    (\"Fold2\", df_eng_q1.union(df_eng_q2), df_eng_q3),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57838f63-2867-478b-9f71-b3e594242433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== Quarter-level sampled row counts ===\nQ1 sampled: 13,528\nQ2 sampled: 14,530\nQ3 sampled: 14,913\n\n=== Fold-level row counts ===\nFold1 Train rows: 13,528\nFold1 Valid rows: 14,530\nFold2 Train rows: 28,058\nFold2 Valid rows: 14,913\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Quarter-level sampled row counts ===\")\n",
    "q1 = df_eng_q1.count()\n",
    "q2 = df_eng_q2.count()\n",
    "q3 = df_eng_q3.count()\n",
    "\n",
    "print(f\"Q1 sampled: {q1:,}\")\n",
    "print(f\"Q2 sampled: {q2:,}\")\n",
    "print(f\"Q3 sampled: {q3:,}\")\n",
    "\n",
    "print(\"\\n=== Fold-level row counts ===\")\n",
    "fold1_train = q1\n",
    "fold1_valid = q2\n",
    "\n",
    "fold2_train = q1 + q2\n",
    "fold2_valid = q3\n",
    "\n",
    "print(f\"Fold1 Train rows: {fold1_train:,}\")\n",
    "print(f\"Fold1 Valid rows: {fold1_valid:,}\")\n",
    "\n",
    "print(f\"Fold2 Train rows: {fold2_train:,}\")\n",
    "print(f\"Fold2 Valid rows: {fold2_valid:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fcdadab-f07f-48d1-8c38-ba6c54e6301c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improved LR for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abbf998-d93c-4a62-805d-ad326f00534e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def run_improved_lr_on_fold(\n",
    "    train_df_raw,\n",
    "    valid_df_raw,\n",
    "    reg_param,\n",
    "    elastic_net_param,\n",
    "    feature_categories, \n",
    "    use_undersample=False,\n",
    "    use_class_weight=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train LR with improved scaling strategy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df_raw : DataFrame\n",
    "        Raw training data\n",
    "    valid_df_raw : DataFrame\n",
    "        Raw validation data\n",
    "    reg_param : float\n",
    "        Regularization parameter\n",
    "    elastic_net_param : float\n",
    "        ElasticNet parameter\n",
    "    feature_categories : dict\n",
    "        Feature categorization with keys: 'datetime', 'robust', 'minmax', 'standard'\n",
    "        Obtained from categorize_numeric_features()\n",
    "    use_undersample : bool\n",
    "        Whether to undersample majority class\n",
    "    use_class_weight : bool\n",
    "        Whether to use class weights\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        AUC-PR score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract feature categories from config\n",
    "    datetime_cols = feature_categories['datetime']\n",
    "    robust_cols = feature_categories['robust']\n",
    "    minmax_cols = feature_categories['minmax']\n",
    "    standard_cols = feature_categories['standard']\n",
    "\n",
    "    log_base_cols = [\n",
    "        c.replace(\"_log\", \"\")\n",
    "        for c in train_df_raw.columns\n",
    "        if c.endswith(\"_log\")\n",
    "    ]\n",
    "\n",
    "    if log_base_cols:\n",
    "        print(\"Detected log-transformed columns, dropping originals from numeric feature groups:\")\n",
    "        for c in log_base_cols:\n",
    "            print(f\"  - drop original: {c}, keep: {c}_log\")\n",
    "\n",
    "    robust_cols   = [c for c in robust_cols   if c not in log_base_cols]\n",
    "    minmax_cols   = [c for c in minmax_cols   if c not in log_base_cols]\n",
    "    standard_cols = [c for c in standard_cols if c not in log_base_cols]\n",
    "\n",
    "    # ========== 0) Optional Undersampling ==========\n",
    "    if use_undersample:\n",
    "        train_df_raw = undersample_train(\n",
    "            train_df_raw,\n",
    "            label_col,\n",
    "            target_pos_ratio=0.4,\n",
    "            seed=42\n",
    "        )\n",
    "        for c in onehot_cols_eng:\n",
    "            train_df_raw = (\n",
    "                train_df_raw\n",
    "                .withColumn(c, F.col(c).cast(\"string\"))\n",
    "                .fillna({c: \"MISSING\"}) \n",
    "            )\n",
    "\n",
    "    # ========== 1) Target Encoding ==========\n",
    "    train_df, valid_df = add_target_encoding_for_fold(\n",
    "        train_df=train_df_raw,\n",
    "        valid_df=valid_df_raw,\n",
    "        target_cols=target_cols_eng,\n",
    "        label_col=label_col,\n",
    "        k=100.0\n",
    "    )\n",
    "    \n",
    "    # Ensure categorical columns are strings\n",
    "    for c in onehot_cols_eng:\n",
    "        train_df = (\n",
    "            train_df\n",
    "            .withColumn(c, F.col(c).cast(\"string\"))\n",
    "            .fillna({c: \"MISSING\"})\n",
    "        )\n",
    "        valid_df = (\n",
    "            valid_df\n",
    "            .withColumn(c, F.col(c).cast(\"string\"))\n",
    "            .fillna({c: \"MISSING\"})\n",
    "        )\n",
    "\n",
    "    # ========== 2) Fill Numeric NaNs ==========\n",
    "    # Get all numeric columns\n",
    "    all_numeric_cols = datetime_cols + robust_cols + minmax_cols + standard_cols\n",
    "    num_fill = {c: 0.0 for c in all_numeric_cols}\n",
    "    train_df = train_df.fillna(num_fill)\n",
    "    valid_df = valid_df.fillna(num_fill)\n",
    "\n",
    "    # ========== 3) Optional Class Weights ==========\n",
    "    if use_class_weight:\n",
    "        counts = (\n",
    "            train_df.groupBy(label_col)\n",
    "                     .count()\n",
    "                     .collect()\n",
    "        )\n",
    "        counts_dict = {row[label_col]: row[\"count\"] for row in counts}\n",
    "        n_pos = counts_dict.get(1, 0)\n",
    "        n_neg = counts_dict.get(0, 0)\n",
    "\n",
    "        total = n_pos + n_neg\n",
    "        w0 = total / (2.0 * n_neg)\n",
    "        w1 = total / (2.0 * n_pos)\n",
    "\n",
    "        train_df = train_df.withColumn(\n",
    "            \"class_weight\",\n",
    "            F.when(F.col(label_col) == 1, F.lit(w1)).otherwise(F.lit(w0))\n",
    "        )\n",
    "        weight_col_name = \"class_weight\"\n",
    "    else:\n",
    "        weight_col_name = None\n",
    "\n",
    "    # ========== 4) Build Pipeline with Multiple Scalers ==========\n",
    "    \n",
    "    pipeline_stages = []\n",
    "    \n",
    "    # Stage 1: One-hot encoding\n",
    "    pipeline_stages.extend(indexers_eng)\n",
    "    pipeline_stages.append(encoder_eng)\n",
    "    \n",
    "    # Stage 2a: RobustScaler (if applicable)\n",
    "    if robust_cols:\n",
    "        robust_assembler = VectorAssembler(\n",
    "            inputCols=robust_cols,\n",
    "            outputCol=\"robust_features_unscaled\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        robust_scaler = RobustScaler(\n",
    "            inputCol=\"robust_features_unscaled\",\n",
    "            outputCol=\"robust_features_scaled\",\n",
    "            withScaling=True,\n",
    "            withCentering=False  # Keep sparse\n",
    "        )\n",
    "        \n",
    "        pipeline_stages.extend([robust_assembler, robust_scaler])\n",
    "    \n",
    "    # Stage 2b: MinMaxScaler (if applicable)\n",
    "    if minmax_cols:\n",
    "        minmax_assembler = VectorAssembler(\n",
    "            inputCols=minmax_cols,\n",
    "            outputCol=\"minmax_features_unscaled\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        minmax_scaler = MinMaxScaler(\n",
    "            inputCol=\"minmax_features_unscaled\",\n",
    "            outputCol=\"minmax_features_scaled\",\n",
    "            min=0.0,\n",
    "            max=1.0\n",
    "        )\n",
    "        \n",
    "        pipeline_stages.extend([minmax_assembler, minmax_scaler])\n",
    "    \n",
    "    # Stage 2c: StandardScaler (if applicable)\n",
    "    if standard_cols:\n",
    "        standard_assembler = VectorAssembler(\n",
    "            inputCols=standard_cols,\n",
    "            outputCol=\"standard_features_unscaled\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        \n",
    "        standard_scaler = StandardScaler(\n",
    "            inputCol=\"standard_features_unscaled\",\n",
    "            outputCol=\"standard_features_scaled\",\n",
    "            withStd=True,\n",
    "            withMean=False  # Keep sparse\n",
    "        )\n",
    "        \n",
    "        pipeline_stages.extend([standard_assembler, standard_scaler])\n",
    "    \n",
    "    # Stage 3: Combine all features\n",
    "    # Build list of feature columns to combine\n",
    "    final_feature_cols = [f\"{c}_ohe\" for c in onehot_cols_eng] + datetime_cols\n",
    "    \n",
    "    if robust_cols:\n",
    "        final_feature_cols.append(\"robust_features_scaled\")\n",
    "    if minmax_cols:\n",
    "        final_feature_cols.append(\"minmax_features_scaled\")\n",
    "    if standard_cols:\n",
    "        final_feature_cols.append(\"standard_features_scaled\")\n",
    "    \n",
    "    final_assembler = VectorAssembler(\n",
    "        inputCols=final_feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    pipeline_stages.append(final_assembler)\n",
    "    \n",
    "    # Stage 4: Logistic Regression\n",
    "    lr_params = {\n",
    "                    \"featuresCol\": \"features\",\n",
    "                    \"labelCol\": label_col,\n",
    "                    \"regParam\": reg_param,\n",
    "                    \"elasticNetParam\": elastic_net_param,\n",
    "                    \"maxIter\": 30,\n",
    "                    }\n",
    "    # Only add weightCol if it's not None or empty\n",
    "    if weight_col_name is not None and weight_col_name != \"\":\n",
    "        lr_params[\"weightCol\"] = weight_col_name\n",
    "\n",
    "    lr = LogisticRegression(**lr_params)\n",
    "\n",
    "    pipeline_stages.append(lr)\n",
    "    \n",
    "    # ========== 5) Train and Evaluate ==========\n",
    "    pipeline = Pipeline(stages=pipeline_stages)\n",
    "    \n",
    "    model = pipeline.fit(train_df)\n",
    "    preds = model.transform(valid_df)\n",
    "    auc_pr = evaluator.evaluate(preds)\n",
    "    \n",
    "    return auc_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa13d605-6ca0-46fe-b4ae-da2ad2fe8067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Grid Search LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd5b90b-7f11-49a4-94a8-62652ab53fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== Strategy: undersample_only ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6427977222241002>, line 29\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m fold_scores \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold_name, fold_train, fold_valid \u001B[38;5;129;01min\u001B[39;00m folds_eng:\n",
       "\u001B[1;32m     24\u001B[0m     auc_pr \u001B[38;5;241m=\u001B[39m run_improved_lr_on_fold(\n",
       "\u001B[1;32m     25\u001B[0m         fold_train,\n",
       "\u001B[1;32m     26\u001B[0m         fold_valid,\n",
       "\u001B[1;32m     27\u001B[0m         reg_param\u001B[38;5;241m=\u001B[39mreg,\n",
       "\u001B[1;32m     28\u001B[0m         elastic_net_param\u001B[38;5;241m=\u001B[39men,\n",
       "\u001B[0;32m---> 29\u001B[0m         feature_categories\u001B[38;5;241m=\u001B[39mFEATURE_CATEGORIES,  \n",
       "\u001B[1;32m     30\u001B[0m         use_undersample\u001B[38;5;241m=\u001B[39muse_us,\n",
       "\u001B[1;32m     31\u001B[0m         use_class_weight\u001B[38;5;241m=\u001B[39muse_cw,\n",
       "\u001B[1;32m     32\u001B[0m     )\n",
       "\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstrat_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] reg=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, en=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00men\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, AUC-PR=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mauc_pr\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     34\u001B[0m     fold_scores\u001B[38;5;241m.\u001B[39mappend(auc_pr)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'FEATURE_CATEGORIES' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'FEATURE_CATEGORIES' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'FEATURE_CATEGORIES' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-6427977222241002>, line 29\u001B[0m\n\u001B[1;32m     21\u001B[0m fold_scores \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold_name, fold_train, fold_valid \u001B[38;5;129;01min\u001B[39;00m folds_eng:\n\u001B[1;32m     24\u001B[0m     auc_pr \u001B[38;5;241m=\u001B[39m run_improved_lr_on_fold(\n\u001B[1;32m     25\u001B[0m         fold_train,\n\u001B[1;32m     26\u001B[0m         fold_valid,\n\u001B[1;32m     27\u001B[0m         reg_param\u001B[38;5;241m=\u001B[39mreg,\n\u001B[1;32m     28\u001B[0m         elastic_net_param\u001B[38;5;241m=\u001B[39men,\n\u001B[0;32m---> 29\u001B[0m         feature_categories\u001B[38;5;241m=\u001B[39mFEATURE_CATEGORIES,  \n\u001B[1;32m     30\u001B[0m         use_undersample\u001B[38;5;241m=\u001B[39muse_us,\n\u001B[1;32m     31\u001B[0m         use_class_weight\u001B[38;5;241m=\u001B[39muse_cw,\n\u001B[1;32m     32\u001B[0m     )\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstrat_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] reg=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, en=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00men\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, AUC-PR=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mauc_pr\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m     fold_scores\u001B[38;5;241m.\u001B[39mappend(auc_pr)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'FEATURE_CATEGORIES' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\"regParam\": 0.0,  \"elasticNetParam\": 0.0},\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.0},\n",
    "    {\"regParam\": 0.1,  \"elasticNetParam\": 0.0},\n",
    "    {\"regParam\": 0.01, \"elasticNetParam\": 0.5},\n",
    "    {\"regParam\": 0.1,  \"elasticNetParam\": 0.5},\n",
    "]\n",
    "\n",
    "strategies = [\n",
    "    (\"undersample_only\",       True,  False),\n",
    "    (\"class_weight_only\",      False, True),\n",
    "]\n",
    "\n",
    "results_improved = []\n",
    "\n",
    "for strat_name, use_us, use_cw in strategies:\n",
    "    print(f\"\\n=== Strategy: {strat_name} ===\")\n",
    "    for params in param_grid:\n",
    "        reg = params[\"regParam\"]\n",
    "        en  = params[\"elasticNetParam\"]\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold_name, fold_train, fold_valid in folds_eng:\n",
    "            auc_pr = run_improved_lr_on_fold(\n",
    "                fold_train,\n",
    "                fold_valid,\n",
    "                reg_param=reg,\n",
    "                elastic_net_param=en,\n",
    "                feature_categories=FEATURE_CATEGORIES,  \n",
    "                use_undersample=use_us,\n",
    "                use_class_weight=use_cw,\n",
    "            )\n",
    "            print(f\"[{strat_name}-{fold_name}] reg={reg}, en={en}, AUC-PR={auc_pr:.4f}\")\n",
    "            fold_scores.append(auc_pr)\n",
    "\n",
    "        mean_auc = sum(fold_scores) / len(fold_scores)\n",
    "        results_improved.append({\n",
    "            \"strategy\": strat_name,\n",
    "            \"regParam\": reg,\n",
    "            \"elasticNetParam\": en,\n",
    "            \"mean_auc_pr\": mean_auc\n",
    "        })\n",
    "        print(f\"--> {strat_name} Mean AUC-PR: {mean_auc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592c10c9-eb5c-4509-8037-1cf457bb1de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'strategy': 'undersample_only',\n",
       "  'regParam': 0.0,\n",
       "  'elasticNetParam': 0.0,\n",
       "  'mean_auc_pr': 0.594465012861737},\n",
       " {'strategy': 'undersample_only',\n",
       "  'regParam': 0.01,\n",
       "  'elasticNetParam': 0.0,\n",
       "  'mean_auc_pr': 0.5932147872480613},\n",
       " {'strategy': 'class_weight_only',\n",
       "  'regParam': 0.0,\n",
       "  'elasticNetParam': 0.0,\n",
       "  'mean_auc_pr': 0.5917132199098694},\n",
       " {'strategy': 'class_weight_only',\n",
       "  'regParam': 0.01,\n",
       "  'elasticNetParam': 0.0,\n",
       "  'mean_auc_pr': 0.5910690577940545}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aad713b-96e2-4c46-a10f-6602692684e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Final Improved LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31e923c2-ac52-4b0c-9b54-494a2cf2f382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Find Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad3cbbc-fdf6-494d-acd7-ffca95f450fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nGRID SEARCH CV RESULTS\n======================================================================\nEmpty DataFrame\nColumns: []\nIndex: []\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'mean_auc_pr'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6427977222241006>, line 16\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(results_improved\u001B[38;5;241m.\u001B[39mto_string(index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Find best configuration\u001B[39;00m\n",
       "\u001B[0;32m---> 16\u001B[0m best_idx \u001B[38;5;241m=\u001B[39m results_improved[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_auc_pr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39midxmax()\n",
       "\u001B[1;32m     17\u001B[0m best_result \u001B[38;5;241m=\u001B[39m results_improved\u001B[38;5;241m.\u001B[39miloc[best_idx]\n",
       "\u001B[1;32m     19\u001B[0m best_strategy \u001B[38;5;241m=\u001B[39m best_result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrategy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n",
       "\u001B[0;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n",
       "\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
       "\u001B[1;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
       "\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
       "\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n",
       "\u001B[1;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n",
       "\u001B[1;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
       "\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'mean_auc_pr'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "KeyError",
        "evalue": "'mean_auc_pr'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'mean_auc_pr'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
        "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
        "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
        "\u001B[0;31mKeyError\u001B[0m: 'mean_auc_pr'",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
        "File \u001B[0;32m<command-6427977222241006>, line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(results_improved\u001B[38;5;241m.\u001B[39mto_string(index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Find best configuration\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m best_idx \u001B[38;5;241m=\u001B[39m results_improved[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_auc_pr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39midxmax()\n\u001B[1;32m     17\u001B[0m best_result \u001B[38;5;241m=\u001B[39m results_improved\u001B[38;5;241m.\u001B[39miloc[best_idx]\n\u001B[1;32m     19\u001B[0m best_strategy \u001B[38;5;241m=\u001B[39m best_result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrategy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
        "\u001B[0;31mKeyError\u001B[0m: 'mean_auc_pr'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Find Best Hyperparameters\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "results_improved = pd.DataFrame(results_improved)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GRID SEARCH CV RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(results_improved.to_string(index=False))\n",
    "\n",
    "# Find best configuration\n",
    "best_idx = results_improved['mean_auc_pr'].idxmax()\n",
    "best_result = results_improved.iloc[best_idx]\n",
    "\n",
    "best_strategy = best_result['strategy']\n",
    "best_reg = best_result['regParam']\n",
    "best_en = best_result['elasticNetParam']\n",
    "best_cv_auc = best_result['mean_auc_pr']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST CONFIGURATION FROM CV\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Strategy:           {best_strategy}\")\n",
    "print(f\"Regularization:     {best_reg}\")\n",
    "print(f\"ElasticNet:         {best_en}\")\n",
    "print(f\"CV Mean AUC-PR:     {best_cv_auc:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Decode strategy\n",
    "use_undersample_final = \"undersample\" in best_strategy.lower()\n",
    "use_class_weight_final = \"class_weight\" in best_strategy.lower()\n",
    "\n",
    "print(f\"\\nFinal model will use:\")\n",
    "print(f\"  Undersampling:   {use_undersample_final}\")\n",
    "print(f\"  Class weights:   {use_class_weight_final}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64edd728-1f4a-42f7-8687-7e2bcbd813e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ad9a80-27cf-41f8-8c34-60ad3a20c4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "def train_final_improved_lr_and_eval(\n",
    "    train_df_raw,\n",
    "    test_df_raw,\n",
    "    reg_param,\n",
    "    elastic_net_param,\n",
    "    feature_categories,\n",
    "    use_undersample=False,\n",
    "    use_class_weight=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train final improved LR model and evaluate on test.\n",
    "    Additionally returns:\n",
    "      - test_auc_pr\n",
    "      - test_f05\n",
    "      - final_model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING FINAL IMPROVED MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extract feature categories\n",
    "    datetime_cols = feature_categories['datetime']\n",
    "    robust_cols = feature_categories['robust']\n",
    "    minmax_cols = feature_categories['minmax']\n",
    "    standard_cols = feature_categories['standard']\n",
    "    \n",
    "    # ========== Optional Undersampling ==========\n",
    "    if use_undersample:\n",
    "        print(\" Applying undersampling to training data...\")\n",
    "        train_df_raw = undersample_train(\n",
    "            train_df_raw,\n",
    "            label_col,\n",
    "            target_pos_ratio=0.4,\n",
    "            seed=42\n",
    "        )\n",
    "        for c in onehot_cols_eng:\n",
    "            train_df_raw = (\n",
    "                train_df_raw\n",
    "                .withColumn(c, F.col(c).cast(\"string\"))\n",
    "                .fillna({c: \"MISSING\"})\n",
    "            )\n",
    "        print(f\" After undersampling: {train_df_raw.count():,} rows\")\n",
    "    \n",
    "    # ========== Target Encoding ==========\n",
    "    print(\" Applying target encoding...\")\n",
    "    train_df, test_df = add_target_encoding_for_fold(\n",
    "        train_df=train_df_raw,\n",
    "        valid_df=test_df_raw,\n",
    "        target_cols=target_cols_eng,\n",
    "        label_col=label_col,\n",
    "        k=100.0\n",
    "    )\n",
    "    print(\" Target encoding complete\")\n",
    "    \n",
    "    # ========== String Columns ==========\n",
    "    print(\" Processing string columns...\")\n",
    "    for c in onehot_cols_eng:\n",
    "        train_df = train_df.withColumn(c, F.col(c).cast(\"string\")).fillna({c: \"MISSING\"})\n",
    "        test_df = test_df.withColumn(c, F.col(c).cast(\"string\")).fillna({c: \"MISSING\"})\n",
    "    \n",
    "    # ========== Fill Numeric NaNs ==========\n",
    "    print(\" Filling numeric NaNs...\")\n",
    "    all_numeric_cols = datetime_cols + robust_cols + minmax_cols + standard_cols\n",
    "    num_fill = {c: 0.0 for c in all_numeric_cols}\n",
    "    train_df = train_df.fillna(num_fill)\n",
    "    test_df = test_df.fillna(num_fill)\n",
    "    \n",
    "    # ========== Class Weights ==========\n",
    "    weight_col_name = None\n",
    "    if use_class_weight:\n",
    "        print(\" Computing class weights...\")\n",
    "        counts = train_df.groupBy(label_col).count().collect()\n",
    "        counts_dict = {row[label_col]: row[\"count\"] for row in counts}\n",
    "        n_pos = counts_dict.get(1, 0)\n",
    "        n_neg = counts_dict.get(0, 0)\n",
    "        total = n_pos + n_neg\n",
    "        w0 = total / (2.0 * n_neg)\n",
    "        w1 = total / (2.0 * n_pos)\n",
    "        train_df = train_df.withColumn(\n",
    "            \"class_weight\",\n",
    "            F.when(F.col(label_col) == 1, F.lit(w1)).otherwise(F.lit(w0))\n",
    "        )\n",
    "        weight_col_name = \"class_weight\"\n",
    "        print(f\" Class weights: w0={w0:.3f}, w1={w1:.3f}\")\n",
    "    \n",
    "    # ========== Build Pipeline ==========\n",
    "    print(\" Building pipeline...\")\n",
    "    pipeline_stages = []\n",
    "    \n",
    "    # One-hot encoding\n",
    "    pipeline_stages.extend(indexers_eng)\n",
    "    pipeline_stages.append(encoder_eng)\n",
    "    \n",
    "    # RobustScaler\n",
    "    if robust_cols:\n",
    "        pipeline_stages.extend([\n",
    "            VectorAssembler(inputCols=robust_cols, outputCol=\"robust_features_unscaled\", handleInvalid=\"keep\"),\n",
    "            RobustScaler(inputCol=\"robust_features_unscaled\", outputCol=\"robust_features_scaled\", withScaling=True, withCentering=False)\n",
    "        ])\n",
    "    \n",
    "    # MinMaxScaler\n",
    "    if minmax_cols:\n",
    "        pipeline_stages.extend([\n",
    "            VectorAssembler(inputCols=minmax_cols, outputCol=\"minmax_features_unscaled\", handleInvalid=\"keep\"),\n",
    "            MinMaxScaler(inputCol=\"minmax_features_unscaled\", outputCol=\"minmax_features_scaled\", min=0.0, max=1.0)\n",
    "        ])\n",
    "    \n",
    "    # StandardScaler\n",
    "    if standard_cols:\n",
    "        pipeline_stages.extend([\n",
    "            VectorAssembler(inputCols=standard_cols, outputCol=\"standard_features_unscaled\", handleInvalid=\"keep\"),\n",
    "            StandardScaler(inputCol=\"standard_features_unscaled\", outputCol=\"standard_features_scaled\", withStd=True, withMean=False)\n",
    "        ])\n",
    "    \n",
    "    # Final assembler\n",
    "    final_feature_cols = [f\"{c}_ohe\" for c in onehot_cols_eng] + datetime_cols\n",
    "    if robust_cols:\n",
    "        final_feature_cols.append(\"robust_features_scaled\")\n",
    "    if minmax_cols:\n",
    "        final_feature_cols.append(\"minmax_features_scaled\")\n",
    "    if standard_cols:\n",
    "        final_feature_cols.append(\"standard_features_scaled\")\n",
    "    \n",
    "    pipeline_stages.append(\n",
    "        VectorAssembler(inputCols=final_feature_cols, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "    )\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_params = {\n",
    "        \"featuresCol\": \"features\",\n",
    "        \"labelCol\": label_col,\n",
    "        \"regParam\": reg_param,\n",
    "        \"elasticNetParam\": elastic_net_param,\n",
    "        \"maxIter\": 30,\n",
    "    }\n",
    "    if weight_col_name:\n",
    "        lr_params[\"weightCol\"] = weight_col_name\n",
    "    \n",
    "    pipeline_stages.append(LogisticRegression(**lr_params))\n",
    "    \n",
    "    print(f\" Pipeline built with {len(pipeline_stages)} stages\")\n",
    "    \n",
    "    # ========== Train ==========\n",
    "    print(\" Training model (this may take a few minutes)...\")\n",
    "    pipeline = Pipeline(stages=pipeline_stages)\n",
    "    final_model = pipeline.fit(train_df)\n",
    "    print(\" MODEL TRAINED!\")\n",
    "    \n",
    "    # ========== Evaluate ==========\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATING ON TEST SET\")\n",
    "    print(\"=\"*70)\n",
    "    print(\" Making predictions...\")\n",
    "    predictions = final_model.transform(test_df)\n",
    "    \n",
    "    print(\" Computing AUC-PR...\")\n",
    "    test_auc_pr = evaluator.evaluate(predictions)\n",
    "    print(f\" Test AUC-PR (Improved): {test_auc_pr:.4f}\")\n",
    "    \n",
    "\n",
    "    # ==========================================================\n",
    "    # NEW: Compute F0.5\n",
    "    # ==========================================================\n",
    "    print(\" Computing F0.5 (threshold = 0.5)...\")\n",
    "    threshold = 0.5\n",
    "    beta = 0.5\n",
    "    beta2 = beta ** 2\n",
    "\n",
    "    preds_with_prob = predictions.withColumn(\n",
    "        \"prob_pos\",\n",
    "        vector_to_array(col(\"probability\")).getItem(1)\n",
    "    )\n",
    "\n",
    "    preds_with_label = preds_with_prob.withColumn(\n",
    "        \"pred_label\",\n",
    "        (col(\"prob_pos\") >= threshold).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    stats = (\n",
    "        preds_with_label\n",
    "        .select(\n",
    "            ((col(\"pred_label\") == 1) & (col(label_col) == 1)).cast(\"int\").alias(\"tp\"),\n",
    "            ((col(\"pred_label\") == 1) & (col(label_col) == 0)).cast(\"int\").alias(\"fp\"),\n",
    "            ((col(\"pred_label\") == 0) & (col(label_col) == 1)).cast(\"int\").alias(\"fn\"),\n",
    "        )\n",
    "        .groupBy()\n",
    "        .sum()\n",
    "        .collect()[0]\n",
    "    )\n",
    "\n",
    "    tp = stats[\"sum(tp)\"]\n",
    "    fp = stats[\"sum(fp)\"]\n",
    "    fn = stats[\"sum(fn)\"]\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        test_f05 = 0.0\n",
    "    else:\n",
    "        test_f05 = (1 + beta2) * precision * recall / (beta2 * precision + recall)\n",
    "\n",
    "    print(f\" Test F0.5 (Improved): {test_f05:.4f}\")\n",
    "    print(f\"  precision={precision:.4f}, recall={recall:.4f}\")\n",
    "\n",
    "    return test_auc_pr, test_f05, final_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0654bc3f-f714-4109-b2b7-8ee87751bd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea24ef67-097f-477c-ba9b-657721ed2d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nTRAINING FINAL MODEL WITH BEST CONFIGURATION\n======================================================================\n\n======================================================================\nTRAINING FINAL IMPROVED MODEL\n======================================================================\n Applying target encoding...\n Target encoding complete\n Processing string columns...\n Filling numeric NaNs...\n Computing class weights...\n Class weights: w0=0.618, w1=2.613\n Building pipeline...\n Pipeline built with 9 stages\n Training model (this may take a few minutes)...\n MODEL TRAINED!\n\n======================================================================\nEVALUATING ON TEST SET\n======================================================================\n Making predictions...\n Computing AUC-PR...\n Test AUC-PR (Improved): 0.5662\n Computing F0.5 (threshold = 0.5)...\n Test F0.5 (Improved): 0.4827\n  precision=0.4541, recall=0.6450\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Train Final Model with Best Hyperparameters\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING FINAL MODEL WITH BEST CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_auc_pr_improved, test_f05_improved, final_model_improved = train_final_improved_lr_and_eval(\n",
    "    train_df_raw=train_df_eng,  # Q1+Q2+Q3\n",
    "    test_df_raw=test_df_eng,    # Q4\n",
    "    reg_param= best_reg,\n",
    "    elastic_net_param= best_en,\n",
    "    feature_categories=FEATURE_CATEGORIES,\n",
    "    use_undersample=use_undersample_final\n",
    "    use_class_weight=use_class_weight_final\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48eef248-e3ef-48f0-946c-43cf041e0cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426a5e2c-ee83-4275-94f8-9aed47c3750c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, lit, avg, count\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5b0d5c-2e98-4089-84b6-8914691efdb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_CATEGORIES_TREE = {\n",
    "    \"onehot_cols\": onehot_cols_eng,     \n",
    "    \"target_cols\": target_cols_eng,         \n",
    "    \"datetime_cols\": FEATURE_CATEGORIES[\"datetime\"],\n",
    "    \"robust_cols\":   FEATURE_CATEGORIES[\"robust\"],\n",
    "    \"minmax_cols\":   FEATURE_CATEGORIES[\"minmax\"],\n",
    "    \"standard_cols\": FEATURE_CATEGORIES[\"standard\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d87ccf-bc35-4d5c-aa76-9c6468028141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963af50f-bec0-4246-afa3-1f2a49b12bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6427977222241015>, line 9\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m indexer_model \u001B[38;5;241m=\u001B[39m indexer_pipeline\u001B[38;5;241m.\u001B[39mfit(df_engineered)\n",
       "\u001B[1;32m      8\u001B[0m df_indexed \u001B[38;5;241m=\u001B[39m indexer_model\u001B[38;5;241m.\u001B[39mtransform(df_engineered)\u001B[38;5;241m.\u001B[39mcache()\n",
       "\u001B[0;32m----> 9\u001B[0m df_indexed\u001B[38;5;241m.\u001B[39mcount()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1413\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n",
       "\u001B[1;32m   1391\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   1392\u001B[0m \n",
       "\u001B[1;32m   1393\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1411\u001B[0m \u001B[38;5;124;03m    3\u001B[39;00m\n",
       "\u001B[1;32m   1412\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1413\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mcount())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    271\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1679.count.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21689.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21689.0 (TID 1244771) (10.59.132.150 executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Command exited with code 52, oom\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o1679.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21689.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21689.0 (TID 1244771) (10.59.132.150 executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Command exited with code 52, oom\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o1679.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21689.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21689.0 (TID 1244771) (10.59.132.150 executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Command exited with code 52, oom\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-6427977222241015>, line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m indexer_model \u001B[38;5;241m=\u001B[39m indexer_pipeline\u001B[38;5;241m.\u001B[39mfit(df_engineered)\n\u001B[1;32m      8\u001B[0m df_indexed \u001B[38;5;241m=\u001B[39m indexer_model\u001B[38;5;241m.\u001B[39mtransform(df_engineered)\u001B[38;5;241m.\u001B[39mcache()\n\u001B[0;32m----> 9\u001B[0m df_indexed\u001B[38;5;241m.\u001B[39mcount()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1413\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   1392\u001B[0m \n\u001B[1;32m   1393\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1411\u001B[0m \u001B[38;5;124;03m    3\u001B[39;00m\n\u001B[1;32m   1412\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1413\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mcount())\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    271\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1679.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21689.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21689.0 (TID 1244771) (10.59.132.150 executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Command exited with code 52, oom\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexer_pipeline = Pipeline(stages=[\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in FEATURE_CATEGORIES_TREE[\"onehot_cols\"] + FEATURE_CATEGORIES_TREE[\"target_cols\"]\n",
    "])\n",
    "\n",
    "indexer_model = indexer_pipeline.fit(df_engineered)\n",
    "\n",
    "df_indexed = indexer_model.transform(df_engineered).cache()\n",
    "df_indexed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c51ddc41-e9e2-4a26-a98d-bbc4101910d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RF CV Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07700d6-a42c-4152-9ffe-24f053f45f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE_SMALL_RF = True\n",
    "SAMPLE_FRACTION_RF = 0.02\n",
    "\n",
    "def maybe_sample_rf(df, quarter_filter):\n",
    "    base = df.filter(quarter_filter)\n",
    "    return base.sample(False, SAMPLE_FRACTION_RF, seed=42) if USE_SMALL_RF else base\n",
    "\n",
    "df_rf_q1 = maybe_sample_rf(df_engineered, col(\"QUARTER\") == 1).cache()\n",
    "df_rf_q2 = maybe_sample_rf(df_engineered, col(\"QUARTER\") == 2).cache()\n",
    "df_rf_q3 = maybe_sample_rf(df_engineered, col(\"QUARTER\") == 3).cache()\n",
    "\n",
    "folds_rf = [\n",
    "    (\"Fold1\", df_rf_q1, df_rf_q2),\n",
    "    (\"Fold2\", df_rf_q1.union(df_rf_q2), df_rf_q3)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6f1b3b-6899-4ece-81ef-57cb3a6f8fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improved RF for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ef0f16-b491-4173-b776-c9096f6ca097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cv_rf(\n",
    "    folds,\n",
    "    feature_categories,\n",
    "    num_trees=50,\n",
    "    max_depth=8,\n",
    "    max_bins=32,\n",
    "    feature_subset_strategy=\"sqrt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Tree CV (Random Forest):\n",
    "    All features are already numeric in the input DataFrame.\n",
    "    No StringIndexer / no one-hot inside this function.\n",
    "    \"\"\"\n",
    "\n",
    "    onehot_cols   = feature_categories[\"onehot_cols\"]\n",
    "    target_cols   = feature_categories[\"target_cols\"]\n",
    "    datetime_cols = feature_categories[\"datetime_cols\"]\n",
    "    robust_cols   = feature_categories[\"robust_cols\"]\n",
    "    minmax_cols   = feature_categories[\"minmax_cols\"]\n",
    "    standard_cols = feature_categories[\"standard_cols\"]\n",
    "\n",
    "    # 现在把所有这些都当作 numeric features 使用\n",
    "    numeric_cols = (\n",
    "        datetime_cols\n",
    "        + robust_cols\n",
    "        + minmax_cols\n",
    "        + standard_cols\n",
    "        + onehot_cols\n",
    "        + target_cols\n",
    "    )\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=label_col,\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "\n",
    "    rf_cv_results = []\n",
    "\n",
    "    for fold_idx, (fold_name, train_df_raw, valid_df_raw) in enumerate(folds, 1):\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(f\"RF Fold {fold_idx}: {fold_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"RF Train rows: {train_df_raw.count():,}\")\n",
    "        print(f\"RF Valid rows: {valid_df_raw.count():,}\")\n",
    "\n",
    "        # Fill numeric\n",
    "        num_fill = {c: 0.0 for c in numeric_cols}\n",
    "        train_df = train_df_raw.fillna(num_fill)\n",
    "        valid_df = valid_df_raw.fillna(num_fill)\n",
    "\n",
    "        # Assemble features: 直接用 numeric_cols\n",
    "        final_feature_cols = numeric_cols\n",
    "\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=final_feature_cols,\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            bootstrap=False,\n",
    "            labelCol=label_col,\n",
    "            featuresCol=\"features\",\n",
    "            numTrees=num_trees,\n",
    "            maxDepth=max_depth,\n",
    "            maxBins=max_bins,\n",
    "            subsamplingRate=0.8,\n",
    "            featureSubsetStrategy=feature_subset_strategy\n",
    "        )\n",
    "\n",
    "        pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "        print(\"✓ RF Training...\")\n",
    "        model = pipeline.fit(train_df)\n",
    "\n",
    "        train_pred = model.transform(train_df)\n",
    "        valid_pred = model.transform(valid_df)\n",
    "\n",
    "        train_auc = evaluator.evaluate(train_pred)\n",
    "        valid_auc = evaluator.evaluate(valid_pred)\n",
    "\n",
    "        print(f\"✓ RF Train AUC-PR: {train_auc:.4f}\")\n",
    "        print(f\"✓ RF Valid AUC-PR: {valid_auc:.4f}\")\n",
    "\n",
    "        rf_cv_results.append((fold_name, train_auc, valid_auc))\n",
    "\n",
    "    # Compute averages\n",
    "    avg_train = sum(r[1] for r in rf_cv_results) / len(rf_cv_results)\n",
    "    avg_valid = sum(r[2] for r in rf_cv_results) / len(rf_cv_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RF CV SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    for name, tr, va in rf_cv_results:\n",
    "        print(f\"{name}: RF Train={tr:.4f}, RF Valid={va:.4f}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"RF Avg Train: {avg_train:.4f}\")\n",
    "    print(f\"RF Avg Valid: {avg_valid:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return rf_cv_results, avg_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68fe54df-9cd5-4b3b-b529-b7eba9e42048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae281cd0-45bd-4d99-965e-acc12e70f066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"num_trees\":  [10, 30],\n",
    "    \"max_depth\":  [5, 10],\n",
    "    \"feature_subset_strategy\": [\"x\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424dca8d-528c-417a-9d20-ac8b0d7b41fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def grid_search_rf(folds, feature_categories, param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    combinations = list(product(*param_grid.values()))\n",
    "    \n",
    "    rf_results = []\n",
    "    rf_best_auc = -1\n",
    "    rf_best_params = None\n",
    "    \n",
    "    for values in combinations:\n",
    "        params = dict(zip(keys, values))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Testing params:\", params)\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        rf_cv_results, avg_auc = cv_rf(\n",
    "            folds=folds,\n",
    "            feature_categories=feature_categories,\n",
    "            num_trees=params.get(\"num_trees\"),\n",
    "            max_depth=params.get(\"max_depth\"),\n",
    "            feature_subset_strategy=params.get(\"feature_subset_strategy\")\n",
    "        )\n",
    "\n",
    "        record = {\n",
    "            **params,\n",
    "            \"avg_auc\": avg_auc\n",
    "        }\n",
    "\n",
    "        rf_results.append(record)\n",
    "\n",
    "        if avg_auc > rf_best_auc:\n",
    "            rf_best_auc = avg_auc\n",
    "            rf_best_params = record\n",
    "\n",
    "    return rf_results, rf_best_params, rf_best_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d203070-8ffd-493c-a3dc-3975b0909e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nTesting params: {'num_trees': 10, 'max_depth': 5, 'feature_subset_strategy': 'sqrt'}\n================================================================================\n\n----------------------------------------------------------------------\nRF Fold 1: Fold1\n----------------------------------------------------------------------\nRF Train rows: 27,202\nRF Valid rows: 29,299\n✓ RF Training...\n✓ RF Train AUC-PR: 0.6180\n✓ RF Valid AUC-PR: 0.5901\n\n----------------------------------------------------------------------\nRF Fold 2: Fold2\n----------------------------------------------------------------------\nRF Train rows: 56,501\nRF Valid rows: 29,690\n✓ RF Training...\n✓ RF Train AUC-PR: 0.6011\n✓ RF Valid AUC-PR: 0.5791\n\n======================================================================\nRF CV SUMMARY\n======================================================================\nFold1: RF Train=0.6180, RF Valid=0.5901\nFold2: RF Train=0.6011, RF Valid=0.5791\n----------------------------------------------------------------------\nRF Avg Train: 0.6095\nRF Avg Valid: 0.5846\n======================================================================\n\n================================================================================\nTesting params: {'num_trees': 10, 'max_depth': 10, 'feature_subset_strategy': 'sqrt'}\n================================================================================\n\n----------------------------------------------------------------------\nRF Fold 1: Fold1\n----------------------------------------------------------------------\nRF Train rows: 27,202\nRF Valid rows: 29,299\n✓ RF Training...\n✓ RF Train AUC-PR: 0.8032\n✓ RF Valid AUC-PR: 0.6196\n\n----------------------------------------------------------------------\nRF Fold 2: Fold2\n----------------------------------------------------------------------\nRF Train rows: 56,501\nRF Valid rows: 29,690\n✓ RF Training...\n✓ RF Train AUC-PR: 0.7418\n✓ RF Valid AUC-PR: 0.6235\n\n======================================================================\nRF CV SUMMARY\n======================================================================\nFold1: RF Train=0.8032, RF Valid=0.6196\nFold2: RF Train=0.7418, RF Valid=0.6235\n----------------------------------------------------------------------\nRF Avg Train: 0.7725\nRF Avg Valid: 0.6215\n======================================================================\n\n================================================================================\nTesting params: {'num_trees': 30, 'max_depth': 5, 'feature_subset_strategy': 'sqrt'}\n================================================================================\n\n----------------------------------------------------------------------\nRF Fold 1: Fold1\n----------------------------------------------------------------------\nRF Train rows: 27,202\nRF Valid rows: 29,299\n✓ RF Training...\n✓ RF Train AUC-PR: 0.6353\n✓ RF Valid AUC-PR: 0.6078\n\n----------------------------------------------------------------------\nRF Fold 2: Fold2\n----------------------------------------------------------------------\nRF Train rows: 56,501\nRF Valid rows: 29,690\n✓ RF Training...\n✓ RF Train AUC-PR: 0.6227\n✓ RF Valid AUC-PR: 0.6007\n\n======================================================================\nRF CV SUMMARY\n======================================================================\nFold1: RF Train=0.6353, RF Valid=0.6078\nFold2: RF Train=0.6227, RF Valid=0.6007\n----------------------------------------------------------------------\nRF Avg Train: 0.6290\nRF Avg Valid: 0.6042\n======================================================================\n\n================================================================================\nTesting params: {'num_trees': 30, 'max_depth': 10, 'feature_subset_strategy': 'sqrt'}\n================================================================================\n\n----------------------------------------------------------------------\nRF Fold 1: Fold1\n----------------------------------------------------------------------\nRF Train rows: 27,202\nRF Valid rows: 29,299\n✓ RF Training...\n✓ RF Train AUC-PR: 0.8102\n✓ RF Valid AUC-PR: 0.6431\n\n----------------------------------------------------------------------\nRF Fold 2: Fold2\n----------------------------------------------------------------------\nRF Train rows: 56,501\nRF Valid rows: 29,690\n✓ RF Training...\n✓ RF Train AUC-PR: 0.7513\n✓ RF Valid AUC-PR: 0.6421\n\n======================================================================\nRF CV SUMMARY\n======================================================================\nFold1: RF Train=0.8102, RF Valid=0.6431\nFold2: RF Train=0.7513, RF Valid=0.6421\n----------------------------------------------------------------------\nRF Avg Train: 0.7807\nRF Avg Valid: 0.6426\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "rf_results, rf_best_params, rf_best_auc = grid_search_rf(\n",
    "    folds=folds_rf,\n",
    "    feature_categories=FEATURE_CATEGORIES_TREE,\n",
    "    param_grid=param_grid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6664cd-7e44-4d1f-8a32-972871b83e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params from CV: {'num_trees': 30, 'max_depth': 10, 'feature_subset_strategy': 'sqrt', 'avg_auc': 0.6426176070661658}\nBest RF CV AUC-PR: 0.6426176070661658\n"
     ]
    }
   ],
   "source": [
    "print(\"Best RF params from CV:\", rf_best_params)\n",
    "print(\"Best RF CV AUC-PR:\", rf_best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b1363f-e6d2-405b-8576-0227207f07f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f77840e-eff6-4da4-890a-88c595968dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RF train rows: 2145680\nFinal RF test rows: 705151\n"
     ]
    }
   ],
   "source": [
    "# 1) Split the full indexed dataset into train (Q1–Q3) and final test (Q4)\n",
    "\n",
    "USE_SAMPLE_RF_FINAL = False\n",
    "SAMPLE_FRACTION_RF_FINAL = 0.7\n",
    "\n",
    "def maybe_sample_rf_final(df):\n",
    "    if USE_SAMPLE_RF_FINAL:\n",
    "        return df.sample(False, SAMPLE_FRACTION_RF_FINAL, seed=42)\n",
    "    return df\n",
    "\n",
    "# Apply sampling BEFORE time split\n",
    "df_engineered_base = maybe_sample_rf_final(df_engineered).cache()\n",
    "\n",
    "# Time-based split for final RF evaluation\n",
    "rf_train_full = df_engineered_base.filter(col(\"QUARTER\") < 4).cache()\n",
    "rf_test_full  = df_engineered_base.filter(col(\"QUARTER\") == 4).cache()\n",
    "\n",
    "print(\"Final RF train rows:\", rf_train_full.count())\n",
    "print(\"Final RF test rows:\", rf_test_full.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d56355-5531-4d8a-a8f4-d20797a3c75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Final Improved RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ccd647-1aec-401b-9557-6142ef40baca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10eec8d-fc92-4216-a196-3ab72cfefdfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "def train_final_rf_full(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    feature_categories,\n",
    "    num_trees,\n",
    "    max_depth,\n",
    "    max_bins=32,\n",
    "    feature_subset_strategy=\"sqrt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train final Random Forest model and evaluate on Q4.\n",
    "    Now returns:\n",
    "        final_model, test_auc_pr, test_f05\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract feature groups\n",
    "    onehot_cols   = feature_categories[\"onehot_cols\"]\n",
    "    target_cols   = feature_categories[\"target_cols\"]\n",
    "    datetime_cols = feature_categories[\"datetime_cols\"]\n",
    "    robust_cols   = feature_categories[\"robust_cols\"]\n",
    "    minmax_cols   = feature_categories[\"minmax_cols\"]\n",
    "    standard_cols = feature_categories[\"standard_cols\"]\n",
    "\n",
    "    numeric_cols = (\n",
    "        datetime_cols\n",
    "        + robust_cols\n",
    "        + minmax_cols\n",
    "        + standard_cols\n",
    "        + onehot_cols\n",
    "        + target_cols\n",
    "    )\n",
    "\n",
    "    # --- Fill missing numeric values\n",
    "    num_fill = {c: 0.0 for c in numeric_cols}\n",
    "    train_df = train_df.fillna(num_fill)\n",
    "    test_df  = test_df.fillna(num_fill)\n",
    "\n",
    "    # --- Assemble features\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=numeric_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    # --- Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        bootstrap=False,\n",
    "        labelCol=label_col,\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=num_trees,\n",
    "        maxDepth=max_depth,\n",
    "        maxBins=max_bins,\n",
    "        subsamplingRate=0.8,\n",
    "        featureSubsetStrategy=feature_subset_strategy\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING FINAL RANDOM FOREST MODEL\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    final_model = pipeline.fit(train_df)\n",
    "\n",
    "    # --- Evaluate on Q4\n",
    "    print(\"\\nEvaluating on held-out Q4 test set...\")\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=label_col,\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "\n",
    "    test_pred = final_model.transform(test_df)\n",
    "    test_auc_pr = evaluator.evaluate(test_pred)\n",
    "\n",
    "    print(f\"\\nFINAL RF Test AUC-PR: {test_auc_pr:.4f}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # NEW: Compute F0.5 using probability column\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"Computing F0.5 (threshold = 0.5)...\")\n",
    "\n",
    "    threshold = 0.5\n",
    "    beta = 0.5\n",
    "    beta2 = beta ** 2\n",
    "\n",
    "    # Convert probability vector to array and take positive class prob\n",
    "    preds_with_prob = test_pred.withColumn(\n",
    "        \"prob_pos\",\n",
    "        vector_to_array(col(\"probability\")).getItem(1)\n",
    "    )\n",
    "\n",
    "    # Threshold -> predicted label\n",
    "    preds_with_label = preds_with_prob.withColumn(\n",
    "        \"pred_label\",\n",
    "        (col(\"prob_pos\") >= threshold).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # Compute TP, FP, FN\n",
    "    stats = (\n",
    "        preds_with_label\n",
    "        .select(\n",
    "            ((col(\"pred_label\") == 1) & (col(label_col) == 1)).cast(\"int\").alias(\"tp\"),\n",
    "            ((col(\"pred_label\") == 1) & (col(label_col) == 0)).cast(\"int\").alias(\"fp\"),\n",
    "            ((col(\"pred_label\") == 0) & (col(label_col) == 1)).cast(\"int\").alias(\"fn\"),\n",
    "        )\n",
    "        .groupBy()\n",
    "        .sum()\n",
    "        .collect()[0]\n",
    "    )\n",
    "\n",
    "    tp = stats[\"sum(tp)\"]\n",
    "    fp = stats[\"sum(fp)\"]\n",
    "    fn = stats[\"sum(fn)\"]\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        test_f05 = 0.0\n",
    "    else:\n",
    "        test_f05 = (1 + beta2) * precision * recall / (beta2 * precision + recall)\n",
    "\n",
    "    print(f\"FINAL RF Test F0.5: {test_f05:.4f}\")\n",
    "    print(f\"  precision={precision:.4f}, recall={recall:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return final_model, test_auc_pr, test_f05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7a8768-f1c6-4aa2-80a0-bf1c2558849d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679cf1f8-9ae6-4a72-9254-2dd2394cd23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nTRAINING FINAL RANDOM FOREST MODEL\n======================================================================\n\nEvaluating on held-out Q4 test set...\n\nFINAL RF Test AUC-PR: 0.6244\nComputing F0.5 (threshold = 0.5)...\nFINAL RF Test F0.5: 0.6338\n  precision=0.7814, recall=0.3611\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "final_rf_model, final_rf_auc_pr, final_rf_f05 = train_final_rf_full(\n",
    "    train_df=rf_train_full,\n",
    "    test_df=rf_test_full,\n",
    "    feature_categories=FEATURE_CATEGORIES_TREE,\n",
    "    num_trees= 30, #rf_best_params[\"num_trees\"],\n",
    "    max_depth= 10, #rf_best_params[\"max_depth\"],\n",
    "    feature_subset_strategy= \"sqrt\" #rf_best_params[\"feature_subset_strategy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872e891c-1636-41fd-be82-5cc3ba0a6312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nTRAINING FINAL RANDOM FOREST MODEL\n======================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6427977222241032>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m final_rf_model_manual, final_rf_auc_pr_manual \u001B[38;5;241m=\u001B[39m train_final_rf_full(\n",
       "\u001B[1;32m      2\u001B[0m     train_df\u001B[38;5;241m=\u001B[39mrf_train_full,\n",
       "\u001B[1;32m      3\u001B[0m     test_df\u001B[38;5;241m=\u001B[39mrf_test_full,\n",
       "\u001B[1;32m      4\u001B[0m     feature_categories\u001B[38;5;241m=\u001B[39mFEATURE_CATEGORIES_TREE,\n",
       "\u001B[1;32m      5\u001B[0m     num_trees\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m,\n",
       "\u001B[1;32m      6\u001B[0m     max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m,\n",
       "\u001B[1;32m      7\u001B[0m     feature_subset_strategy\u001B[38;5;241m=\u001B[39mrf_best_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_subset_strategy\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m      8\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m<command-6427977222241029>, line 68\u001B[0m, in \u001B[0;36mtrain_final_rf_full\u001B[0;34m(train_df, test_df, feature_categories, num_trees, max_depth, max_bins, feature_subset_strategy)\u001B[0m\n",
       "\u001B[1;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTRAINING FINAL RANDOM FOREST MODEL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     67\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m70\u001B[39m)\n",
       "\u001B[0;32m---> 68\u001B[0m final_model \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mfit(train_df)\n",
       "\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# --- Evaluate on Q4 test set\u001B[39;00m\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mEvaluating on held-out Q4 test set...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:511\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    509\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 511\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m call_original_fn_with_event_logging(original, args, kwargs)\n",
       "\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    513\u001B[0m     \u001B[38;5;66;03m# If original function succeeds, but `patch_function_exception` exists,\u001B[39;00m\n",
       "\u001B[1;32m    514\u001B[0m     \u001B[38;5;66;03m# it represent patching code unexpected failure, so we call\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    517\u001B[0m     \u001B[38;5;66;03m# even if `patch_function_exception` exists, because original function failure\u001B[39;00m\n",
       "\u001B[1;32m    518\u001B[0m     \u001B[38;5;66;03m# means there's some error in user code (e.g. user provide wrong arguments)\u001B[39;00m\n",
       "\u001B[1;32m    519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m patch_error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m failed_during_original:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:425\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    423\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_start(og_args, og_kwargs)\n",
       "\u001B[0;32m--> 425\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m original_fn(\u001B[38;5;241m*\u001B[39mog_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mog_kwargs)\n",
       "\u001B[1;32m    427\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_success(og_args, og_kwargs)\n",
       "\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:136\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    134\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n",
       "\u001B[0;32m--> 136\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n",
       "\u001B[1;32m    137\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n",
       "\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:402\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m    385\u001B[0m     active_session_failed\n",
       "\u001B[1;32m    386\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m autologging_is_disabled(autologging_integration)\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    396\u001B[0m     \u001B[38;5;66;03m# warning behavior during original function execution, since autologging is being\u001B[39;00m\n",
       "\u001B[1;32m    397\u001B[0m     \u001B[38;5;66;03m# skipped\u001B[39;00m\n",
       "\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n",
       "\u001B[1;32m    399\u001B[0m         disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    400\u001B[0m         reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    401\u001B[0m     ):\n",
       "\u001B[0;32m--> 402\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    404\u001B[0m \u001B[38;5;66;03m# Whether or not the original / underlying function has been called during the\u001B[39;00m\n",
       "\u001B[1;32m    405\u001B[0m \u001B[38;5;66;03m# execution of patched code\u001B[39;00m\n",
       "\u001B[1;32m    406\u001B[0m original_has_been_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:398\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n",
       "\u001B[0;32m--> 398\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_java(dataset)\n",
       "\u001B[1;32m    399\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n",
       "\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:395\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    394\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mfit(dataset\u001B[38;5;241m.\u001B[39m_jdf)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    271\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o21274.fit.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 65765.0 failed 4 times, most recent failure: Lost task 6.3 in stage 65765.0 (TID 2151130) (10.59.133.149 executor 279): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
       "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
       "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
       "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n",
       "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n",
       "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n",
       "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
       "\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
       "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
       "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n",
       "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n",
       "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n",
       "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n",
       "\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n",
       "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1593)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1577)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3260)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1111)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1109)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:741)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:680)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:212)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:306)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:169)\n",
       "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:323)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:323)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:140)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:48)\n",
       "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor1118.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
       "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
       "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
       "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n",
       "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n",
       "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n",
       "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
       "\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
       "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
       "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n",
       "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n",
       "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n",
       "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n",
       "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n",
       "\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n",
       "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n",
       "\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o21274.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 65765.0 failed 4 times, most recent failure: Lost task 6.3 in stage 65765.0 (TID 2151130) (10.59.133.149 executor 279): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1577)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3260)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1109)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:741)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:680)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:212)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:306)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:323)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:323)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:140)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:48)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat jdk.internal.reflect.GeneratedMethodAccessor1118.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o21274.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 65765.0 failed 4 times, most recent failure: Lost task 6.3 in stage 65765.0 (TID 2151130) (10.59.133.149 executor 279): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1577)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3260)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1109)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:741)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:680)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:212)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:306)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:323)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:323)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:140)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:48)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat jdk.internal.reflect.GeneratedMethodAccessor1118.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-6427977222241032>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m final_rf_model_manual, final_rf_auc_pr_manual \u001B[38;5;241m=\u001B[39m train_final_rf_full(\n\u001B[1;32m      2\u001B[0m     train_df\u001B[38;5;241m=\u001B[39mrf_train_full,\n\u001B[1;32m      3\u001B[0m     test_df\u001B[38;5;241m=\u001B[39mrf_test_full,\n\u001B[1;32m      4\u001B[0m     feature_categories\u001B[38;5;241m=\u001B[39mFEATURE_CATEGORIES_TREE,\n\u001B[1;32m      5\u001B[0m     num_trees\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m,\n\u001B[1;32m      6\u001B[0m     max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m,\n\u001B[1;32m      7\u001B[0m     feature_subset_strategy\u001B[38;5;241m=\u001B[39mrf_best_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_subset_strategy\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      8\u001B[0m )\n",
        "File \u001B[0;32m<command-6427977222241029>, line 68\u001B[0m, in \u001B[0;36mtrain_final_rf_full\u001B[0;34m(train_df, test_df, feature_categories, num_trees, max_depth, max_bins, feature_subset_strategy)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTRAINING FINAL RANDOM FOREST MODEL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m70\u001B[39m)\n\u001B[0;32m---> 68\u001B[0m final_model \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mfit(train_df)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# --- Evaluate on Q4 test set\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mEvaluating on held-out Q4 test set...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:511\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    509\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m call_original_fn_with_event_logging(original, args, kwargs)\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;66;03m# If original function succeeds, but `patch_function_exception` exists,\u001B[39;00m\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;66;03m# it represent patching code unexpected failure, so we call\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;66;03m# even if `patch_function_exception` exists, because original function failure\u001B[39;00m\n\u001B[1;32m    518\u001B[0m     \u001B[38;5;66;03m# means there's some error in user code (e.g. user provide wrong arguments)\u001B[39;00m\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m patch_error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m failed_during_original:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:425\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    423\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_start(og_args, og_kwargs)\n\u001B[0;32m--> 425\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m original_fn(\u001B[38;5;241m*\u001B[39mog_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mog_kwargs)\n\u001B[1;32m    427\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_success(og_args, og_kwargs)\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:136\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    134\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n\u001B[1;32m    137\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:402\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    385\u001B[0m     active_session_failed\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m autologging_is_disabled(autologging_integration)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    396\u001B[0m     \u001B[38;5;66;03m# warning behavior during original function execution, since autologging is being\u001B[39;00m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;66;03m# skipped\u001B[39;00m\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001B[1;32m    399\u001B[0m         disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    400\u001B[0m         reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    401\u001B[0m     ):\n\u001B[0;32m--> 402\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    404\u001B[0m \u001B[38;5;66;03m# Whether or not the original / underlying function has been called during the\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;66;03m# execution of patched code\u001B[39;00m\n\u001B[1;32m    406\u001B[0m original_has_been_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:398\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 398\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_java(dataset)\n\u001B[1;32m    399\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:395\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mfit(dataset\u001B[38;5;241m.\u001B[39m_jdf)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    271\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o21274.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 65765.0 failed 4 times, most recent failure: Lost task 6.3 in stage 65765.0 (TID 2151130) (10.59.133.149 executor 279): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1577)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3260)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1109)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:741)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:461)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:680)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:212)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:306)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:323)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:323)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:140)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:48)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat jdk.internal.reflect.GeneratedMethodAccessor1118.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:101)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:121)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$4737/0x00007fc8a84c7230.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$153/0x00007fc8a722c680.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:120)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:86)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:311)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:308)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:239)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:324)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1730)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2807/0x00007fc8a7b390f8.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1656)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1721)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1469)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:432)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:382)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:420)\n\tat org.apache.spark.rdd.RDD$$Lambda$3494/0x00007fc8a7eb7b30.apply(Unknown Source)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%skip\n",
    "final_rf_model_manual, final_rf_auc_pr_manual = train_final_rf_full(\n",
    "    train_df=rf_train_full,\n",
    "    test_df=rf_test_full,\n",
    "    feature_categories=FEATURE_CATEGORIES_TREE,\n",
    "    num_trees=30,\n",
    "    max_depth=15,\n",
    "    feature_subset_strategy=rf_best_params[\"feature_subset_strategy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a68e79-1db0-49ff-85e2-f282efd93947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Findings on Grid Search vs. Full-Dataset Results\n",
    "\n",
    "Because large-scale grid search was not computationally feasible (OOM when using full data), tuning was performed on a **small sampled subset** instead. This constraint leads to two effects:\n",
    "\n",
    "1. **Shallow trees appeared optimal during grid search**  \n",
    "   On a small sample, deeper trees quickly overfit, so the search consistently selected lower `maxDepth` values  \n",
    "   *e.g., `maxDepth = 5–8` performed best during tuning.*\n",
    "\n",
    "2. **This behavior does *not* generalize to the full dataset**  \n",
    "   When training on the **entire time-ordered dataset (Q1–Q3)**, a **deeper model**(e.g., depth 20-30) achieved **higher PR-AUC without overfitting**  \n",
    "   *e.g., increasing `maxDepth` beyond the grid-searched range improved performance on full data.*\n",
    "\n",
    "This is expected:\n",
    "\n",
    "- With limited data, deeper trees **overfit** and appear worse.\n",
    "- With much larger data volume, deeper trees can **use additional splits effectively** and improve generalization.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "* **Grid search was used only to narrow the parameter space under compute limits**, not to select the final hyperparameters.\n",
    "* **Final model settings were chosen based on full-data evaluation**, which reflects real deployment conditions rather than sample-size artifacts.\n",
    "* This confirms the trade-off:  \n",
    "  *“When data is small, deeper trees overfit; when data is large, deeper trees can safely increase complexity and improve performance.”*\n",
    "\n",
    "Additionally:\n",
    "\n",
    "Because increasing the sample size would trigger OOM failures, grid search **cannot directly optimize for the ideal full-data configuration**.  \n",
    "To address this, we combine:\n",
    "\n",
    "- **coarse tuning on sampled data**, and  \n",
    "- **final validation using the full dataset** (Q1–Q3 → Q4).\n",
    "\n",
    "This approach ensures the final model is both computationally feasible and aligned with real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c1129b-cf20-4b3e-af1d-6b391a37b2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_params[\"max_depth\"]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Baseline_Models_Nicole",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}